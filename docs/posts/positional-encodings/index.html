<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ramu Nalla">
<meta name="dcterms.date" content="2026-01-13">
<meta name="description" content="How does a Transformer treat ‘The cat ate the fish’ different than ‘The fish ate the cat’? We explore the mathematics of Positional Encoding to see how models learn order in parallel processing.">

<title>Positional Encoding in Transformers: The Geography of Language – Ramu Nalla</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../favicon.svg" rel="icon" type="image/svg+xml">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-fc8d3d93f0a84b7619e1f8ff6eb63d42.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-38d03fa8751a617e088234052a949a1c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="floating nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Ramu Nalla</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../projects.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../cv.html"> 
<span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/RamuNalla" target="_blank"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://linkedin.com/in/ramu-nalla-2959b8b3/" target="_blank"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Positional Encoding in Transformers: The Geography of Language</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Deep Learning</div>
    <div class="quarto-category">Transformers</div>
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">PyTorch</div>
  </div>
  </div>

<div>
  <div class="description">
    How does a Transformer treat ‘The cat ate the fish’ different than ‘The fish ate the cat’? We explore the mathematics of Positional Encoding to see how models learn order in parallel processing.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ramu Nalla </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 13, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<div class="blog-manual-meta">
Published by Ramu Nalla - January 13, 2026
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="image.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption>Conceptual Illustration of Language Chaos.</figcaption>
</figure>
</div>
<hr>
<p>Imagine you tossed the sentence “The cat ate the fish” into a blender. You know exactly which words are in the mix, but you have lost the story. “The fish ate the cat” would look identical in that blender.</p>
<p>This is the fundamental problem with the standard Transformer architecture.</p>
<p>Unlike Recurrent Neural Networks (RNNs), which process words sequentially (<span class="math inline">\(t_1\)</span>, then <span class="math inline">\(t_2\)</span>, then <span class="math inline">\(t_3\)</span>), Transformers process all words in a sentence simultaneously in parallel. This parallelism is what makes them so fast, but it comes at a steep cost: the model is inherently invariant to word order. It sees a “bag of words,” not a sentence. To fix this, we must inject a sense of order back into the data. We need to “tag” every word with its geographic location in the sentence before it enters the Transformer.</p>
<p>This technique is called <strong>Positional Encoding (PE)</strong>.</p>
<p>In this post, we are going to explore the two main strategies for doing this: the elegant, fixed mathematical approach (Sinusoidal) used in the original “Attention Is All You Need” paper, and the simpler, data-driven approach (Learnable).</p>
<section id="the-goal-a-unique-fingerprint" class="level2">
<h2 class="anchored" data-anchor-id="the-goal-a-unique-fingerprint">The Goal: A Unique Fingerprint</h2>
<p>What properties should a good positional tag have?</p>
<ol type="1">
<li><strong>Unique:</strong> Position 5 must look different from Position 6.</li>
<li><strong>Consistent distance:</strong> The “mathematical distance” between Position 5 and 6 should be the same as between Position 10 and 11.</li>
<li><strong>Extensible:</strong> Ideally, the model should be able to handle sentences longer than ones it saw during training.</li>
<li><strong>Deterministic:</strong> The tag for position 5 should always be the same.</li>
</ol>
<p>You might think, <em>“Why not just add the number 1 to the first word vector, 2 to the second, and so on?”</em></p>
<p>The problem is <strong>scale</strong>. For a 1000-word document, the 1000th word would have massive values added to it, completely drowning out the actual semantic meaning of the word embedding (which usually consists of small numbers between -1 and 1).</p>
<p>We need a system that stays within a bounded range. <strong>Enter trigonometry.</strong></p>
</section>
<section id="strategy-1-sinusoidal-positional-encoding-the-fixed-approach" class="level2">
<h2 class="anchored" data-anchor-id="strategy-1-sinusoidal-positional-encoding-the-fixed-approach">Strategy 1: Sinusoidal Positional Encoding (The Fixed Approach)</h2>
<p>The authors of the original Transformer paper proposed a brilliant, non-learnable solution based on sine and cosine waves of different frequencies.</p>
<p>Here is the intuition: <strong>Imagine a row of analog clocks.</strong></p>
<ul>
<li>The first clock has a hand that spins very fast (seconds).</li>
<li>The next spins slower (minutes).</li>
<li>The next spins even slower (hours).</li>
</ul>
<p>If you take a snapshot of all the clock hands at any given moment, that specific combination of hand positions is a unique timestamp.</p>
<p>Sinusoidal encoding works similarly. Every position gets a unique combination of sine and cosine values generated by waves oscillating at different speeds across the embedding dimension.</p>
<section id="the-formula" class="level3">
<h3 class="anchored" data-anchor-id="the-formula">The Formula</h3>
<p>This is the part that usually scares people away. Let’s look at it and then decode it.</p>
<p>Given a position <code>pos</code> in the sentence, and a specific dimension index <code>i</code> in the embedding vector, the encoding is computed as:</p>
<p><span class="math display">\[PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)\]</span></p>
<p><span class="math display">\[PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)\]</span></p>
<p>Don’t panic. Let’s break down the technical interpretation.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="sine_cosine.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption>Conceptual Illustration of Sine - Cosine Positional Encoding.</figcaption>
</figure>
</div>
</section>
<section id="technical-interpretation-position-vs.-dimension" class="level3">
<h3 class="anchored" data-anchor-id="technical-interpretation-position-vs.-dimension">Technical Interpretation: Position vs.&nbsp;Dimension</h3>
<p>The crucial thing to understand is that the <strong>frequency</strong> of the wave changes depending on where you are in the embedding vector.</p>
<p><strong>1. Interpreting Along the Dimensions (Fixing position <code>pos</code>)</strong></p>
<p>Imagine you are looking at the word at <strong>Position 10</strong>. Its positional encoding is a vector of size <span class="math inline">\(d_{model}\)</span> (e.g., 512).</p>
<ul>
<li>At the start of that vector (low <code>i</code> indices), the denominator is small. The frequency is <strong>high</strong>. The values change rapidly.</li>
<li>At the end of that vector (high <code>i</code> indices), the denominator is huge. The frequency is <strong>very low</strong>. The wave is stretched out incredibly long.</li>
</ul>
<p>This means early dimensions encode <strong>local order</strong>, while later dimensions encode <strong>global position</strong>.</p>
<p><strong>2. Interpreting Along the Positions (Fixing dimension <code>i</code>)</strong></p>
<p>Now, imagine you are looking at just <strong>Dimension 0</strong> across the entire sentence. You will see a perfect high-frequency sine wave as you read from word 1 to word 100. If you look at Dimension 256, you will see a much slower cosine wave.</p>
</section>
<section id="a-concrete-example-tiny-math-trace" class="level3">
<h3 class="anchored" data-anchor-id="a-concrete-example-tiny-math-trace">A Concrete Example (Tiny Math Trace)</h3>
<p>Let’s use tiny numbers to see it work.</p>
<ul>
<li><span class="math inline">\(d_{model} = 4\)</span></li>
<li>Sequence Length = 2 (Positions 0 and 1)</li>
</ul>
<p>We need to build a <span class="math inline">\((2 \times 4)\)</span> matrix.</p>
<p><strong>The Frequencies (The Denominator term):</strong></p>
<p>We have pairs of dimensions <span class="math inline">\(2i\)</span> and <span class="math inline">\(2i+1\)</span>. The indices <span class="math inline">\(i\)</span> are <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. The denominator is <span class="math inline">\(10000^{(2i/4)}\)</span>.</p>
<ul>
<li>For dimensions 0 and 1 (<span class="math inline">\(i=0\)</span>): Denominator is <span class="math inline">\(1\)</span>. Frequency term is <span class="math inline">\(1\)</span>.</li>
<li>For dimensions 2 and 3 (<span class="math inline">\(i=1\)</span>): Denominator is <span class="math inline">\(100\)</span>. Frequency term is <span class="math inline">\(0.01\)</span>.</li>
</ul>
<p><strong>Calculating Position 0:</strong></p>
<ul>
<li><p>Dim 0 (sin, freq 1): <span class="math inline">\(\sin(0) = 0\)</span></p></li>
<li><p>Dim 1 (cos, freq 1): <span class="math inline">\(\cos(0) = 1\)</span></p></li>
<li><p>Dim 2 (sin, freq 0.01): <span class="math inline">\(\sin(0) = 0\)</span></p></li>
<li><p>Dim 3 (cos, freq 0.01): <span class="math inline">\(\cos(0) = 1\)</span></p></li>
<li><p><strong>PE at Pos 0: [0, 1, 0, 1]</strong></p></li>
</ul>
<p><strong>Calculating Position 1:</strong></p>
<ul>
<li><p>Dim 0 (sin, freq 1): <span class="math inline">\(\sin(1) \approx 0.84\)</span></p></li>
<li><p>Dim 1 (cos, freq 1): <span class="math inline">\(\cos(1) \approx 0.54\)</span></p></li>
<li><p>Dim 2 (sin, freq 0.01): <span class="math inline">\(\sin(0.01) \approx 0.01\)</span></p></li>
<li><p>Dim 3 (cos, freq 0.01): <span class="math inline">\(\cos(0.01) \approx 0.99\)</span></p></li>
<li><p><strong>PE at Pos 1: [0.84, 0.54, 0.01, 0.99]</strong></p></li>
</ul>
<p>We now have unique vectors for position 0 and 1 that are bounded between -1 and 1. We literally add these to the semantic word embeddings.</p>
</section>
<section id="the-implementation-pytorch" class="level3">
<h3 class="anchored" data-anchor-id="the-implementation-pytorch">The Implementation (PyTorch)</h3>
<p>Implementing this efficiently requires some clever broadcasting and a numerical stability trick.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PositionalEncoding(nn.Module):</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Sinusoidal positional encoding implementation.</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model: <span class="bu">int</span>, max_len: <span class="bu">int</span> <span class="op">=</span> <span class="dv">5000</span>, dropout_p: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.1</span>):</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(PositionalEncoding, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(p<span class="op">=</span>dropout_p)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. Create the Matrix Buffer</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize a matrix (max_len x d_model) with zeros.</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        pe <span class="op">=</span> torch.zeros(max_len, d_model) </span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. Create Position Indices</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># A column vector [[0], [1], [2], ...]. Shape: (max_len, 1)</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        position <span class="op">=</span> torch.arange(<span class="dv">0</span>, max_len, dtype<span class="op">=</span>torch.<span class="bu">float</span>).unsqueeze(<span class="dv">1</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. Calculate the Frequencies (The "div_term")</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The formula uses 10000^(2i/d_model).</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># To avoid huge numbers, we use the log trick: a^b = exp(b * ln(a))</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This calculates the denominator term for the even indices (0, 2, 4...).</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Shape: (d_model / 2)</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        div_term <span class="op">=</span> torch.exp(</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>            torch.arange(<span class="dv">0</span>, d_model, <span class="dv">2</span>).<span class="bu">float</span>() <span class="op">*</span> (<span class="op">-</span>math.log(<span class="fl">10000.0</span>) <span class="op">/</span> d_model)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 4. Apply Sine and Cosine using Broadcasting</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Multiply position (col vector) by frequencies (row vector) to get the</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># arguments for sin/cos. Fill even cols with sin, odd with cos.</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> torch.sin(position <span class="op">*</span> div_term)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>        pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> torch.cos(position <span class="op">*</span> div_term)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 5. Add Batch Dimension and Register</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Shape becomes (1, max_len, d_model) for easy addition later.</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>        pe <span class="op">=</span> pe.unsqueeze(<span class="dv">0</span>)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># register_buffer means: "Save this tensor with the model weights, </span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># but do NOT update it during training with gradients."</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'pe'</span>, pe)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="co">            x: Input embeddings of shape (batch_size, actual_seq_len, d_model)</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. Slice the pre-computed matrix to match the actual sequence length.</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If input is 10 words long, take the first 10 rows of 'pe'.</span></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. ADD the encoding to the input embeddings.</span></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.pe[:, :x.size(<span class="dv">1</span>), :]</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply dropout for regularization</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.dropout(x)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>The beauty of this code is that <code>pe</code> is computed once when the model is initialized. During training or inference, it’s just a fast lookup and addition operation.</p>
</section>
</section>
<section id="strategy-2-learnable-positional-encoding-the-data-driven-approach" class="level2">
<h2 class="anchored" data-anchor-id="strategy-2-learnable-positional-encoding-the-data-driven-approach">Strategy 2: Learnable Positional Encoding (The Data-Driven Approach)</h2>
<p>After the original Transformer paper, researchers asked a pragmatic question: <em>“Why work so hard with trigonometry? Why are we forcing a human-designed pattern onto the model? Why not just let the neural network learn the best position tags itself?”</em></p>
<p>This is the <strong>Learnable</strong> approach. It is deceptively simple, yet it powers some of the most famous models in history, including BERT and the early GPT series.</p>
<section id="the-intuition-a-second-vocabulary" class="level3">
<h3 class="anchored" data-anchor-id="the-intuition-a-second-vocabulary">The Intuition: A Second Vocabulary</h3>
<p>In this approach, we treat “Position” exactly the same way we treat “Words.”</p>
<p>Just as the model has a lookup table (an embedding layer) for words—where ID 256 might map to the vector for “Apple”—it has a second lookup table for positions.</p>
<ul>
<li>Position ID 0 maps to a learned vector <span class="math inline">\(v_0\)</span>.</li>
<li>Position ID 1 maps to a learned vector <span class="math inline">\(v_1\)</span>.</li>
<li>…</li>
<li>Position ID 511 maps to a learned vector <span class="math inline">\(v_{511}\)</span>.</li>
</ul>
<p>These vectors are initialized with random noise. During the training process, while the model is learning to translate English to French, it is simultaneously adjusting these position vectors via backpropagation. It figures out exactly what numbers need to be in those vectors to help it distinguish <em>“The cat ate the mouse”</em> from <em>“The mouse ate the cat.”</em></p>
</section>
<section id="technical-interpretation-the-black-box-dimensions" class="level3">
<h3 class="anchored" data-anchor-id="technical-interpretation-the-black-box-dimensions">Technical Interpretation: The “Black Box” Dimensions</h3>
<p>Unlike the Sinusoidal approach, where we know exactly what Dimension 0 represents (a high-frequency wave) and what Dimension 511 represents (a low-frequency wave), Learnable Encodings are opaque.</p>
<p><strong>1. No Fixed Mathematical Meaning</strong></p>
<p>If you look at Dimension <span class="math inline">\(i\)</span> in a learnable position vector, it does not necessarily correspond to a frequency or a clear pattern.</p>
<ul>
<li>Dimension 0 might learn to be a simple counter.</li>
<li>Dimension 1 might learn to toggle on and off every 5 words.</li>
<li>Dimension 2 might look like random noise to us, but contain a specific signal the model finds useful for attention heads.</li>
</ul>
<p>We lose the interpretability of the sine waves, but we gain flexibility. The model is free to invent its own coordinate system.</p>
<p><strong>2. Neighbor Similarity</strong></p>
<p>Despite the lack of a fixed formula, a well-trained model usually converges on a logical structure. It tends to learn vectors such that <strong>Position 5</strong> and <strong>Position 6</strong> are very similar (they have a high cosine similarity).</p>
<p><strong>Why?</strong> Because words next to each other usually share context. If the positional tags for neighbors were wildly different (orthogonal), it would be much harder for the Attention mechanism to learn local relationships like adjective-noun pairings. The model naturally drifts toward making neighbors “look” similar in vector space to minimize its loss.</p>
</section>
<section id="how-it-works-overall-the-summation-step" class="level3">
<h3 class="anchored" data-anchor-id="how-it-works-overall-the-summation-step">How It Works Overall: The “Summation” Step</h3>
<p>A common point of confusion is where this positional information enters the network. Does it go into the Query? The Key?</p>
<p>In the standard architecture (like BERT or the original Transformer), the positional vector is added directly to the <strong>Input Embedding</strong> before the data ever touches the first Attention layer.</p>
<ol type="1">
<li><strong>Step 1: Word Lookup.</strong> We convert the word “Cat” into a 512-dimensional semantic vector.</li>
<li><strong>Step 2: Position Lookup.</strong> We look up the learned vector for “Position 5.”</li>
<li><strong>Step 3: Addition.</strong> We element-wise add them together.</li>
</ol>
<p><span class="math display">\[Input = Word\_Embedding + Position\_Embedding\]</span></p>
<p><strong>Wait, doesn’t adding two vectors corrupt the information?</strong> If “Cat” is <code>[0.5, 0.2]</code> and Position 5 is <code>[0.1, 0.9]</code>, the result <code>[0.6, 1.1]</code> is neither purely “Cat” nor purely “Position 5.”</p>
<p>This is true. However, in high-dimensional space (e.g., 512, 1024, or 4096 dimensions), the model learns to segregate the information. It might use the first few dozen dimensions primarily for positional signaling and the rest for semantic meaning, or interleave them in complex ways.</p>
<p>Once this “summed” vector enters the first Attention layer:</p>
<ul>
<li>It is projected into Queries (<span class="math inline">\(Q\)</span>), Keys (<span class="math inline">\(K\)</span>), and Values (<span class="math inline">\(V\)</span>).</li>
<li>Because the position information was baked into the input, <span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span>, and <span class="math inline">\(V\)</span> all contain traces of that positional signal.</li>
<li>When <span class="math inline">\(Q\)</span> and <span class="math inline">\(K\)</span> interact (dot product), the positional traces help determine the attention score. (e.g., <em>“I am a noun at Pos 5, looking for an adjective near Pos 4”</em>).</li>
</ul>
</section>
<section id="the-implementation-pytorch-1" class="level3">
<h3 class="anchored" data-anchor-id="the-implementation-pytorch-1">The Implementation (PyTorch)</h3>
<p>It’s almost too simple to show code for:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LearnablePositionalEncoding(nn.Module):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, max_len<span class="op">=</span><span class="dv">5000</span>):</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Just a standard embedding layer!</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Input ID 0 gives vector for pos 0, Input ID 5 gives vector for pos 5.</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_embedding <span class="op">=</span> nn.Embedding(max_len, d_model)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x shape: (batch_size, seq_len, d_model)</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        seq_len <span class="op">=</span> x.size(<span class="dv">1</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create indices [0, 1, ... seq_len-1] on the right device</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        positions <span class="op">=</span> torch.arange(seq_len, device<span class="op">=</span>x.device).unsqueeze(<span class="dv">0</span>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Lookup the learnable vectors</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        pos_vectors <span class="op">=</span> <span class="va">self</span>.pos_embedding(positions)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add them to the word embeddings</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="op">+</span> pos_vectors</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
</section>
<section id="the-showdown-sinusoidal-vs.-learnable" class="level2">
<h2 class="anchored" data-anchor-id="the-showdown-sinusoidal-vs.-learnable">The Showdown: Sinusoidal vs.&nbsp;Learnable</h2>
<p>Which one should you use? For a long time, this was a hot debate.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Sinusoidal (Fixed)</th>
<th style="text-align: left;">Learnable</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>How it works</strong></td>
<td style="text-align: left;">Deterministic math formula.</td>
<td style="text-align: left;">Learned weights during training.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Parameters</strong></td>
<td style="text-align: left;">Zero trainable parameters.</td>
<td style="text-align: left;"><code>max_len * d_model</code> parameters.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Extensibility</strong></td>
<td style="text-align: left;"><strong>Excellent.</strong> Can theoretically handle sequences longer than seen during training.</td>
<td style="text-align: left;"><strong>Poor.</strong> Cannot handle positions beyond <code>max_len</code> seen in training.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Flexibility</strong></td>
<td style="text-align: left;">Rigid structure imposed by humans.</td>
<td style="text-align: left;">Flexible; model learns whatever structure the data demands.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Implementation</strong></td>
<td style="text-align: left;">Slightly complex math to setup.</td>
<td style="text-align: left;">Very simple (just an Embedding layer).</td>
</tr>
</tbody>
</table>
<section id="the-verdict" class="level3">
<h3 class="anchored" data-anchor-id="the-verdict">The Verdict</h3>
<p>In practice, for standard tasks like translation, both perform remarkably similarly. The model is usually smart enough to figure out ordering regardless of which method you use, as long as the tags are unique.</p>
<p>However, <strong>Sinusoidal</strong> is often preferred in research because it doesn’t add parameters and has that theoretical ability to extrapolate to longer sequences. <strong>Learnable</strong> is preferred for its simplicity and is often the default in libraries like Hugging Face’s BERT implementation.</p>
</section>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>Without Positional Encoding, a Transformer is just a powerful bag-of-words model. By injecting these positional signals—whether through fixed trigonometry or learned embeddings—we provide the necessary geography for the model to understand the structure of language.</p>
<p>The sinusoidal approach, while mathematically dense at first glance, is an elegant solution that uses alternating frequencies to create unique, bounded time-stamps for every token in a sequence.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<div class="footer-social-links" style="margin-bottom: 20px;">
<p><a href="mailto:your@email.com" title="Email" style="margin: 0 12px; color: #888;"><i class="bi bi-envelope-fill"></i></a> <a href="https://github.com/RamuNalla" title="GitHub" style="margin: 0 12px; color: #888;"><i class="bi bi-github"></i></a> <a href="https://linkedin.com/in/ramu-nalla-2959b8b3/" title="LinkedIn" style="margin: 0 12px; color: #888;"><i class="bi bi-linkedin"></i></a> <a href="https://twitter.com" title="Twitter" style="margin: 0 12px; color: #888;"><i class="bi bi-twitter-x"></i></a> <a href="https://instagram.com" title="Instagram" style="margin: 0 12px; color: #888;"><i class="bi bi-instagram"></i></a></p>
</div>
<div class="footer-text" style="color: #888; font-size: 0.9rem; margin-bottom: 10px;">
<p>Best to reach out to me over email or DM me on LinkedIn.</p>
</div>
<div class="custom-copyright" style="color: #666; font-size: 0.8rem;">
<p>© 2026 Ramu Nalla. All rights reserved.</p>
</div>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>