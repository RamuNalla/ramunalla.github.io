[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/scaled-dot-product-attention/index.html",
    "href": "posts/scaled-dot-product-attention/index.html",
    "title": "Scaled Dot-Product Attention: A Tensor-First Approach",
    "section": "",
    "text": "Published by Ramu Nalla - January 10, 2026\nIf you have ever tried to read the ‚ÄúAttention Is All You Need‚Äù paper or dig into the source code of a Transformer model, you have likely hit a wall. That wall is usually made of Tensors.\nYou see shapes like [32, 8, 10, 64]. You see operations like transpose(-2, -1). You see dot products and softmaxes flying around. It‚Äôs easy to get lost in the dimensionality and lose sight of what is actually happening: Communication.\nAt its core, the attention mechanism is just a way for words in a sentence to ‚Äútalk‚Äù to each other and figure out who they should be focusing on.\nIn this post, I am going to bypass the academic jargon. I will take a standard PyTorch implementation of Scaled Dot-Product Attention, break it down line-by-line, and crucially, I will trace the math with real numbers so you can see exactly what happens to the data."
  },
  {
    "objectID": "posts/scaled-dot-product-attention/index.html#the-code-scaled-dot-product-attention",
    "href": "posts/scaled-dot-product-attention/index.html#the-code-scaled-dot-product-attention",
    "title": "Scaled Dot-Product Attention: A Tensor-First Approach",
    "section": "The Code: Scaled Dot-Product Attention",
    "text": "The Code: Scaled Dot-Product Attention\nLet‚Äôs start with the raw code. This is a standard implementation you might find in any modern NLP library.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass ScaledDotProductAttention(nn.Module):\n    \"\"\"\n    Scaled Dot-Product Attention\n    Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V\n    \"\"\"\n\n    def __init__(self, d_k: int, dropout: float = 0.1):\n        super(ScaledDotProductAttention, self).__init__()\n        self.d_k = d_k\n        self.dropout = nn.Dropout(dropout)\n        self.scale = math.sqrt(d_k)\n\n    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor,\n                mask: torch.Tensor = None):\n        \"\"\"\n        Args:\n            Q: Query (batch_size, num_heads, seq_len, d_k)\n            K: Key   (batch_size, num_heads, seq_len, d_k)\n            V: Value (batch_size, num_heads, seq_len, d_v)\n            mask:    (batch_size, 1, seq_len, seq_len)\n        \"\"\"\n        # 1. Compute attention scores\n        # QK^T: (batch_size, num_heads, seq_len, seq_len)\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n        \n        # 2. Apply mask (set masked positions to large negative value)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        \n        # 3. Apply softmax to get attention weights\n        attention_weights = F.softmax(scores, dim=-1)\n        attention_weights = self.dropout(attention_weights)\n        \n        # 4. Apply attention weights to values\n        output = torch.matmul(attention_weights, V)\n        \n        return output, attention_weights\nIf that looks intimidating, don‚Äôt worry. We need to fix how we visualize tensors first."
  },
  {
    "objectID": "posts/scaled-dot-product-attention/index.html#strategy-mastering-tensor-dimensions",
    "href": "posts/scaled-dot-product-attention/index.html#strategy-mastering-tensor-dimensions",
    "title": "Scaled Dot-Product Attention: A Tensor-First Approach",
    "section": "Strategy: Mastering Tensor Dimensions",
    "text": "Strategy: Mastering Tensor Dimensions\nThe biggest source of confusion in PyTorch is seeing a tensor just as a list of numbers. You must start thinking of them as Named Dimensions.\nFor Attention, memorize this hierarchy (B, H, L, D):\n\nBatch (\\(B\\)): How many examples are we processing? (e.g., 32 sentences).\nHeads (\\(H\\)): How many ‚Äúperspectives‚Äù does the model have? (e.g., 8 heads).\nLength (\\(L\\)): How long is the sequence? (e.g., 10 words).\nDimension (\\(D\\)): How much data represents a single word? (e.g., a vector of size 64).\n\nWhen we do operations, we are usually manipulating just the last two dimensions (\\(L\\) and \\(D\\)). The Batch and Heads just come along."
  },
  {
    "objectID": "posts/scaled-dot-product-attention/index.html#a-concrete-example-i-love-ai",
    "href": "posts/scaled-dot-product-attention/index.html#a-concrete-example-i-love-ai",
    "title": "Scaled Dot-Product Attention: A Tensor-First Approach",
    "section": "A Concrete Example: ‚ÄúI Love AI‚Äù",
    "text": "A Concrete Example: ‚ÄúI Love AI‚Äù\nLet‚Äôs trace the code with a tiny example. We will ignore Batch and Heads for a moment and focus on the math for a single sequence.\n\nSequence: ‚ÄúI‚Äù, ‚ÄúLove‚Äù, ‚ÄúAI‚Äù (Length = 3)\nDimension (\\(d_k\\)): 2 (Each word is a vector of 2 numbers)\n\n\nStep 1: The Inputs (Q, K, V)\nImagine our model has learned the following representations for these words.\n\\[Q = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} \\quad (\\text{I, Love, AI})\\]\n\\[K = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} \\quad (\\text{I, Love, AI})\\]\n\\[V = \\begin{bmatrix} 10 & 20 \\\\ 30 & 40 \\\\ 50 & 60 \\end{bmatrix} \\quad (\\text{Content we want to extract})\\]\nNote: In reality, Q and K are different linear projections, but for simplicity, we keep them identical here.\n\n\nStep 2: The Similarity Search (scores)\nThis corresponds to this line of code:\nscores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\nWe want to know: How similar is every word to every other word?\nWe do this by taking the dot product of the Query (\\(Q\\)) and the Transpose of the Key (\\(K^T\\)).\n\\[\\text{Scores} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 1 & 1 & 2 \\end{bmatrix}\\]\nWhat does this matrix tell us?\nLook at the last row (representing the word ‚ÄúAI‚Äù).\n\nColumn 1 (Score 1): ‚ÄúAI‚Äù has some similarity to ‚ÄúI‚Äù.\nColumn 2 (Score 1): ‚ÄúAI‚Äù has some similarity to ‚ÄúLove‚Äù.\nColumn 3 (Score 2): ‚ÄúAI‚Äù is most similar to itself.\n\n\n\nStep 3: Scaling\nWe divide by self.scale, which is \\(\\sqrt{d_k}\\). Since our dimension is \\(d_k=4\\) (hypothetically), \\(\\sqrt{4}=2\\).\n\\[\\text{Scaled Scores} = \\frac{\\text{Scores}}{2} = \\begin{bmatrix} 0.5 & 0 & 0.5 \\\\ 0 & 0.5 & 0.5 \\\\ 0.5 & 0.5 & 1.0 \\end{bmatrix}\\]\nWhy do we do this?\nWithout scaling, dot products can grow huge as dimensions increase. Huge numbers into a Softmax function result in gradients close to zero (the ‚Äúvanishing gradient‚Äù problem), which kills training. Scaling keeps things stable.\n\n\nStep 4: The Softmax (Probabilities)\nattention_weights = F.softmax(scores, dim=-1)\nWe convert raw scores into probabilities that sum to 1. Let‚Äôs look at the first row (Word: ‚ÄúI‚Äù) which had scaled scores [0.5, 0, 0.5]. After Softmax, this might look like: [0.38, 0.24, 0.38].\nThis tells the model: ‚ÄúTo understand the word ‚ÄòI‚Äô, put 38% focus on ‚ÄòI‚Äô, 24% focus on ‚ÄòLove‚Äô, and 38% focus on ‚ÄòAI‚Äô.‚Äù\n\n\nStep 5: The Weighted Sum (The Output)\noutput = torch.matmul(attention_weights, V)\nFinally, we update the word‚Äôs meaning by taking a weighted sum of the Values (\\(V\\)). For the first word ‚ÄúI‚Äù, the calculation is:\n\\[\\text{Output}_{\\text{row1}} = (0.38 \\cdot V_{\\text{I}}) + (0.24 \\cdot V_{\\text{Love}}) + (0.38 \\cdot V_{\\text{AI}})\\]\n\\[= 0.38[10, 20] + 0.24[30, 40] + 0.38[50, 60]\\]\n\\[= [3.8, 7.6] + [7.2, 9.6] + [19, 22.8] = [30, 40]\\]\nThe word ‚ÄúI‚Äù started as vector [10, 20]. After attention, it became [30, 40]. It has absorbed context from the other words in the sentence."
  },
  {
    "objectID": "posts/scaled-dot-product-attention/index.html#handling-the-mask",
    "href": "posts/scaled-dot-product-attention/index.html#handling-the-mask",
    "title": "Scaled Dot-Product Attention: A Tensor-First Approach",
    "section": "Handling the Mask",
    "text": "Handling the Mask\nOne line we glossed over:\nscores = scores.masked_fill(mask == 0, -1e9)\nIn tasks like text generation, the model isn‚Äôt allowed to see the future. When predicting the 2nd word, it shouldn‚Äôt know what the 3rd word is.\nWe enforce this by applying a mask. We take the positions we want to hide and replace their scores with a massive negative number (like -1,000,000,000).\nWhy?\nBecause \\(e^{-1000000000}\\) is effectively zero. When we run Softmax, those words get 0% probability, effectively vanishing from the calculation."
  },
  {
    "objectID": "posts/scaled-dot-product-attention/index.html#summary",
    "href": "posts/scaled-dot-product-attention/index.html#summary",
    "title": "Scaled Dot-Product Attention: A Tensor-First Approach",
    "section": "Summary",
    "text": "Summary\nWhen you look at the ScaledDotProductAttention class now, try to see the story it tells:\n\nTranspose & Matmul: Create a grid showing how much every word relates to every other word.\nScale: Shrink the numbers so gradients don‚Äôt vanish.\nMask: Hide the words we aren‚Äôt allowed to see.\nSoftmax: Convert raw relationship scores into percentages.\nMatmul (Final): Create a new representation of the word that is a blend of all the relevant context it found.\n\nThe beauty of the Transformer isn‚Äôt just in its performance; it‚Äôs in this elegance of using simple linear algebra to model the complexity of language."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html",
    "title": "The Definitive Guide to BLEU Score: The Mathematics of Machine Translation",
    "section": "",
    "text": "Published by Ramu Nalla - January 01, 2026\nIn the world of supervised learning‚Äîlike spam classification or customer churn prediction‚Äîevaluation is straightforward. The model is right, or it‚Äôs wrong. Accuracy, precision, and recall tell us everything we need to know.\nBut what happens when you are evaluating a machine translation model?\nIf the human reference translates a French sentence as ‚ÄúThe cat sat on the mat,‚Äù but your model outputs ‚ÄúA cat was sitting on the mat,‚Äù is the model wrong? No.¬†It conveys the same meaning. It just uses different words.\nLanguage is inherently ambiguous. There is rarely one single ‚Äúgold standard‚Äù translation. This subjectivity makes automated evaluation of Generative AI incredibly difficult.\nFor two decades, the de facto standard for solving this problem in Machine Translation (MT) has been the BLEU (Bilingual Evaluation Understudy) score. While newer semantic metrics like BERTScore are gaining traction, understanding BLEU is non-negotiable for any serious NLP practitioner. It is the bedrock upon which modern metrics are built.\nIn this post, we are going to tear apart the BLEU score. We won‚Äôt just look at the final formula; we will build it piece by piece to understand the mathematical intuition behind why it works‚Äîand crucial for us data scientists‚Äîwhere it breaks."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#the-core-intuition-precision-vs.-recall",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#the-core-intuition-precision-vs.-recall",
    "title": "The Definitive Guide to BLEU Score: The Mathematics of Machine Translation",
    "section": "The Core Intuition: Precision vs.¬†Recall",
    "text": "The Core Intuition: Precision vs.¬†Recall\nWhen evaluating generated text against a reference, we have two fundamental choices:\n\nPrecision: How much of what the model generated was correct?\nRecall: How much of the reference did the model capture?\n\nBLEU is fundamentally a Precision-based metric.\nWhy? Imagine a translation task where the goal is to translate a complex German technical document.\n\nA high-recall model might output a messy 5-page document that definitely contains all the right information, but buries it in hallucinations and noise.\nA high-precision model might output a shorter, 3-page document. It might miss a nuance or two, but everything it did generate is accurate and fluent.\n\nIn translation, we generally prefer the latter. We want the generated text to be high quality, even if it‚Äôs slightly incomplete."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#step-1-the-trap-of-raw-precision",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#step-1-the-trap-of-raw-precision",
    "title": "The Definitive Guide to BLEU Score: The Mathematics of Machine Translation",
    "section": "Step 1: The Trap of Raw Precision",
    "text": "Step 1: The Trap of Raw Precision\nLet‚Äôs start with the simplest possible approach: counting word matches.\n\nReference 1: ‚ÄúThe cat is on the mat.‚Äù\nReference 2: ‚ÄúThere is a cat on the mat.‚Äù\n\nNow, imagine a poorly trained model generates this candidate:\n\nCandidate: ‚Äúthe the the the the the‚Äù\n\nIf we calculate standard unigram (1-word) precision:\n\nTotal candidate words: 6\nNumber of matches: 6 (The word ‚Äúthe‚Äù appears in the references).\nPrecision: 6/6 = 100%\n\nThis is obviously a catastrophic failure. A model could game the system by finding the most common word in the reference language and repeating it endlessly."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#step-2-the-fix-modified-clipped-precision",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#step-2-the-fix-modified-clipped-precision",
    "title": "The Definitive Guide to BLEU Score: The Mathematics of Machine Translation",
    "section": "Step 2: The Fix ‚Äî Modified (Clipped) Precision",
    "text": "Step 2: The Fix ‚Äî Modified (Clipped) Precision\n\n\n\nA visual representation of how BLEU calculates ‚ÄúClipped Precision,‚Äù mapping candidate words to reference constraints.\n\n\nTo solve the repetition problem, BLEU introduces clipped precision.\nThe rule is simple: A word in the candidate sentence can only be counted as a match up to the maximum number of times it appears in a single reference sentence.\nLet‚Äôs re-evaluate our bad candidate:\n\nCandidate: ‚Äúthe the the the the the‚Äù\nReference Constraint: The word ‚Äúthe‚Äù appears a maximum of two times (in Reference 1).\n\nThe math changes:\n\nTotal candidate words: 6\nClipped matches: 2 (We count the first two ‚Äúthe‚Äùs, and discard the remaining four).\nClipped Precision: 2/6 = 33.3%\n\nThis is much more reasonable. The metric now punishes absurd repetition.\nAs shown in the image at the top of this post, think of the reference sentences as providing a limited ‚Äúbudget‚Äù for each word count. Once the candidate uses up that budget, subsequent uses of that word are worthless."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#step-3-fluency-and-n-grams",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#step-3-fluency-and-n-grams",
    "title": "The Definitive Guide to BLEU Score: The Mathematics of Machine Translation",
    "section": "Step 3: Fluency and N-Grams",
    "text": "Step 3: Fluency and N-Grams\nMatching individual words is good for ensuring the content is roughly adequate, but it doesn‚Äôt ensure the sentence flows naturally.\n‚ÄúMat on is the cat the‚Äù contains all the right words, but it‚Äôs terrible English.\nBLEU solves this by calculating clipped precision not just for unigrams (1-grams), but also for bigrams (2-grams), trigrams (3-grams), and typically up to 4-grams.\n\n1-grams measure adequacy (are the right concepts there?).\n3-grams and 4-grams measure fluency (is the phrasing natural?).\n\nIf a model gets high precision on 4-grams, it means it‚Äôs getting long sequences of words exactly right, which usually correlates with high-quality, fluent text.\n\nCombining the Scores\nSo, we have four separate precision scores (\\(p_1, p_2, p_3, p_4\\)). How do we combine them into a single number?\nWe don‚Äôt take the arithmetic mean (an average). We take the geometric mean.\n\n\nWhy?\nThe geometric mean is highly sensitive to low scores. If a translation has excellent unigram precision (all the right words) but zero 4-gram precision (the order is completely scrambled), the geometric mean will crash toward zero. The arithmetic mean would still give it a decent pass. We want the metric to be harsh on models that fail at any level of granularity.\nThe combined precision score looks like this:\n\\[\\exp\\left( \\sum_{n=1}^{N} w_n \\log p_n \\right)\\]\nTypically, \\(N=4\\) and the weights \\(w_n\\) are uniform (\\(1/4\\))."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#step-4-the-brevity-penalty-preventing-the-gaming",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#step-4-the-brevity-penalty-preventing-the-gaming",
    "title": "The Definitive Guide to BLEU Score: The Mathematics of Machine Translation",
    "section": "Step 4: The Brevity Penalty (Preventing the Gaming)",
    "text": "Step 4: The Brevity Penalty (Preventing the Gaming)\nWe established that BLEU is a precision metric. But pure precision metrics have a massive loophole: Length.\nIf I have to translate a 50-word sentence, I could output a single, perfect 3-word phrase that I know is correct. My precision would be 100%. But I failed to translate the vast majority of the source sentence.\nTo prevent models from outputting overly short, ‚Äúsafe‚Äù sentences to maximize precision, BLEU applies a Brevity Penalty (BP).\nThe penalty is calculated based on two variables: 1. \\(c\\): The length of the candidate translation. 2. \\(r\\): The effective reference length.\nThe formula for BP is:\n\\[\nBP = \\begin{cases}\n1 & \\text{if } c &gt; r \\\\\ne^{(1 - r/c)} & \\text{if } c \\le r\n\\end{cases}\n\\]\nIf the candidate is longer than the reference (\\(c &gt; r\\)), there is no penalty (\\(BP = 1\\)). We don‚Äôt punish the model for being verbose (that‚Äôs what the precision score already does if the extra words are wrong).\nIf the candidate is shorter than the reference (\\(c \\le r\\)), the penalty kicks in exponentially.\nLet‚Äôs visualize this.\n\n\n\nA graph showing the BLEU Brevity Penalty. The Y-axis is the penalty multiplier (0 to 1), and the X-axis is the ratio of candidate length to reference length (c/r). The curve drops sharply below 1.0.\n\n\nAs you can see in the graph above, as soon as the candidate length drops below the reference length (ratio &lt; 1.0), the score multiplier starts dropping from 1.0 toward 0.0. A candidate that is half the length of the reference gets heavily penalized, ensuring that high-BLEU systems must produce output of comparable length to humans."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#the-complete-formula",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#the-complete-formula",
    "title": "The Definitive Guide to BLEU Score: The Mathematics of Machine Translation",
    "section": "The Complete Formula",
    "text": "The Complete Formula\nPutting it all together, the BLEU score is the geometric mean of the n-gram clipped precisions, multiplied by the brevity penalty.\n\\[\\textbf{BLEU} = \\underbrace{BP}_{\\text{Brevity Penalty}} \\cdot \\underbrace{\\exp\\left( \\sum_{n=1}^{4} w_n \\log p_n \\right)}_{\\text{Geometric Mean of N-Gram Precisions}}\\]\nThe final score is between 0 and 1, though in practice, it is almost always reported as a percentage between 0 and 100.\nA BLEU score over 30 is generally considered understandable. A score over 40-50 is often considered high quality, depending on the language pair (translating English to French is easier than English to Chinese, so expected scores vary)."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#limitations",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#limitations",
    "title": "The Definitive Guide to BLEU Score: The Mathematics of Machine Translation",
    "section": "Limitations",
    "text": "Limitations\nWe‚Äôve covered how BLEU works. Now, let‚Äôs talk about why it‚Äôs often frustrating. As data scientists, we need to know the limitations of our tools.\n1. The Synonym Problem (Semantic Blindness). BLEU relies on exact string matching. It has no concept of meaning.\n\nReference: ‚ÄúThe path is steep.‚Äù\nCandidate: ‚ÄúThe trail is uphill.‚Äù\n\nBLEU will give this a very low score, despite it being a perfect translation. It punishes creative or varied vocabulary. This is why semantic embeddings-based metrics like BERTScore were invented.\n2. It Ignores Global Structure While n-grams capture local ordering, BLEU has no sense of overall sentence structure. ‚ÄúCat the sat mat on the‚Äù might get decent unigram scores, and it wouldn‚Äôt necessarily get a zero if some bigrams accidentally align.\n3. Human Correlation is Wobbly Crucially, an increase in BLEU score does not always correspond to an increase in human-perceived quality. A model might game the metric by optimizing for 4-grams in a way that sounds mechanical to a human reader. BLEU is useful for tracking progress during training, but the final sanity check must always be human evaluation.\n4. The Preprocessing Nightmare (sacrebleu) Because BLEU is based on exact string matching, it is incredibly sensitive to preprocessing. Do you tokenize text? Do you lowercase everything? How do you handle punctuation? Different implementations used to yield vastly different scores for the same model output.\nIndustry Standard: If you are reporting BLEU scores in a paper or a serious report, never roll your own implementation. Always use the sacrebleu Python library. It standardizes tokenization and preprocessing to ensure your scores are actually comparable to other people‚Äôs scores."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#final-thoughts",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#final-thoughts",
    "title": "The Definitive Guide to BLEU Score: The Mathematics of Machine Translation",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nBLEU is imperfect. It is rigid, semantically blind, and sometimes fails to match human judgment.\nHowever, it remains widely adopted in machine translation evaluation. It is fast to compute, easy to understand, and language-agnostic. Understanding the mechanics of clipped precision and the brevity penalty gives you the intuition needed to interpret these scores correctly‚Äîand to know when it‚Äôs time to move on to something more advanced."
  },
  {
    "objectID": "posts/multi-head-attention/index.html",
    "href": "posts/multi-head-attention/index.html",
    "title": "Multi-Head Attention: A Tensor-First Walkthrough",
    "section": "",
    "text": "Published by Ramu Nalla - January 12, 2026\nIn my previous post, I have broken down the Scaled Dot-Product Attention mechanism. We saw how a Query finds similar Keys to extract Values.\nBut if you look at the actual Transformer architecture, you rarely see just ‚ÄúAttention.‚Äù You see Multi-Head Attention.\nWhy?\nImagine the sentence: ‚ÄúThe animal didn‚Äôt cross the street because it was too tired.‚Äù\nAs a human, you know ‚Äúit‚Äù refers to the ‚Äúanimal.‚Äù How do you know? 1. Grammatical perspective: You are looking for a noun that agrees with the pronoun. 2. Semantic perspective: You know that ‚Äústreets‚Äù don‚Äôt get ‚Äútired,‚Äù but ‚Äúanimals‚Äù do.\nIf a model only has a single attention head, it has to average these different types of relationships into one messy score. Multi-Head Attention allows the model to create distinct ‚Äúexperts.‚Äù Head 1 can focus on grammar, while Head 2 focuses on semantic context, running in parallel without interfering with each other.\nIn this post, I will walk through the standard PyTorch implementation of Multi-Head Attention. I will define a simple scenario and trace exactly what happens to the tensors at every step."
  },
  {
    "objectID": "posts/multi-head-attention/index.html#the-code-pytorch-multiheadattention",
    "href": "posts/multi-head-attention/index.html#the-code-pytorch-multiheadattention",
    "title": "Multi-Head Attention: A Tensor-First Walkthrough",
    "section": "The Code: PyTorch MultiHeadAttention",
    "text": "The Code: PyTorch MultiHeadAttention\nHere is the complete implementation I will be dissecting.\nimport torch\nimport torch.nn as nn\n# Assuming ScaledDotProductAttention is defined as in the previous post\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n        super(MultiHeadAttention, self).__init__()\n        \n        # Ensure the model dimension can be split evenly across heads\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n        \n        self.d_model = d_model\n        self.num_heads = num_heads\n        # d_k is the dimension of the vector *per head*\n        self.d_k = d_model // num_heads  \n        self.d_v = d_model // num_heads  \n        \n        # Linear projections for Q, K, V\n        # Note: We use single large projections instead of h separate ones\n        self.W_Q = nn.Linear(d_model, d_model)\n        self.W_K = nn.Linear(d_model, d_model)\n        self.W_V = nn.Linear(d_model, d_model)\n        \n        # Output projection (The Mixer)\n        self.W_O = nn.Linear(d_model, d_model)\n        \n        # Scaled dot-product attention mechanism\n        self.attention = ScaledDotProductAttention(self.d_k, dropout)\n        self.dropout = nn.Dropout(dropout)\n\n    def split_heads(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Split tensor into multiple heads and transpose for parallel processing\"\"\"\n        batch_size, seq_len, d_model = x.size()\n        \n        # Reshape: (B, Seq, d_model) -&gt; (B, Seq, H, d_k)\n        x = x.view(batch_size, seq_len, self.num_heads, self.d_k)\n        \n        # Transpose: (B, Seq, H, d_k) -&gt; (B, H, Seq, d_k)\n        # This puts Heads next to Batch for parallel attention computation\n        x = x.transpose(1, 2)\n        return x\n\n    def combine_heads(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Combine multiple heads back into a single continuous vector\"\"\"\n        # x shape: (B, H, Seq, d_k)\n        batch_size, num_heads, seq_len, d_k = x.size()\n        \n        # Transpose back: (B, H, Seq, d_k) -&gt; (B, Seq, H, d_k)\n        x = x.transpose(1, 2).contiguous()\n        \n        # Flatten: (B, Seq, H, d_k) -&gt; (B, Seq, d_model)\n        # H * d_k equals d_model again\n        x = x.view(batch_size, seq_len, self.d_model)\n        return x\n\n    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor,\n                mask: torch.Tensor = None):\n        batch_size = Q.size(0)\n        \n        # 1. Linear projections (Apply the \"lenses\")\n        # Shape remains: (B, Seq, d_model)\n        Q = self.W_Q(Q)  \n        K = self.W_K(K)  \n        V = self.W_V(V)  \n        \n        # 2. Split into multiple heads\n        # Shape becomes: (B, H, Seq, d_k)\n        Q = self.split_heads(Q)  \n        K = self.split_heads(K)  \n        V = self.split_heads(V)  \n        \n        # 3. Apply attention (in parallel across heads)\n        # Output shape: (B, H, Seq, d_v)\n        attn_output, attention_weights = self.attention(Q, K, V, mask)\n        \n        # 4. Combine heads (Stitch them back together)\n        # Shape becomes: (B, Seq, d_model)\n        attn_output = self.combine_heads(attn_output)\n        \n        # 5. Final linear projection (Mix the experts' opinions)\n        # Shape remains: (B, Seq, d_model)\n        output = self.W_O(attn_output)\n        output = self.dropout(output)\n        \n        return output, attention_weights"
  },
  {
    "objectID": "posts/multi-head-attention/index.html#the-tensor-walkthrough",
    "href": "posts/multi-head-attention/index.html#the-tensor-walkthrough",
    "title": "Multi-Head Attention: A Tensor-First Walkthrough",
    "section": "The Tensor Walkthrough",
    "text": "The Tensor Walkthrough\nTo truly understand Multi-Head Attention, we need to trace the shapes.\n\nOur Scenario\n\nWe are processing 1 sentence (Batch Size \\(B=1\\)).\nThe sentence has 3 words (Sequence Length \\(L=3\\), e.g., ‚ÄúI‚Äù, ‚ÄúLove‚Äù, ‚ÄúAI‚Äù).\nEach word is represented by a tiny vector of size 4 (\\(d_{model}=4\\)).\nWe want 2 parallel heads (\\(H=2\\)).\nTherefore, each head will operate on vectors of size 2 (\\(d_k = 4 / 2 = 2\\)).\n\n\n\nStep 1: The Inputs and Projections\nThe inputs \\(Q\\), \\(K\\), and \\(V\\) all start with the same shape.\n\nInput Shape: (1, 3, 4) \\(\\rightarrow\\) (Batch, Seq, \\(d_{model}\\))\n\nWe pass them through linear layers (\\(W_Q\\), \\(W_K\\), and \\(W_V\\)). These are essentially filters that transform the raw word embeddings into ‚ÄúQuery space,‚Äù ‚ÄúKey space,‚Äù and ‚ÄúValue space.‚Äù\n# 1. Linear projections\nQ = self.W_Q(Q)\nK = self.W_K(K)\nV = self.W_V(V)\n\nOutput Shape: (1, 3, 4)\n\nCrucially, the shape hasn‚Äôt changed yet. We have just transformed the data inside the vectors.\n\n\nStep 2: Splitting the Heads (‚ÄúThe Magic Trick‚Äù)\nThis is the most confusing part for beginners. How do we get multiple heads out of one vector?\nWe take the final dimension (\\(d_{model}=4\\)) and physically chop it into \\(H\\) chunks of size \\(d_k\\).\n# 2. Split into multiple heads\nQ = self.split_heads(Q)\n# ... K and V do the same ...\n1. View (Reshape): We tell the system to interpret the (..., 4) dimension as (..., 2, 2).\n\nShape changes from (1, 3, 4) to (1, 3, 2, 2) \\(\\rightarrow\\) (Batch, Seq, Heads, \\(d_k\\)).\n\n2. Transpose (Swap): We swap the ‚ÄúSequence‚Äù dimension (dim 1) with the ‚ÄúHeads‚Äù dimension (dim 2).\n\nShape changes from (1, 3, 2, 2) to (1, 2, 3, 2) \\(\\rightarrow\\) (Batch, Heads, Seq, \\(d_k\\)).\n\nWhy Transpose? Matrix multiplication operations typically operate on the last two dimensions. By moving ‚ÄúHeads‚Äù to the second position, the system treats (1, 2) as a ‚Äúsuper-batch.‚Äù It effectively tricks the GPU into running the attention mechanism on Head 1 and Head 2 simultaneously in parallel.\n\n\nStep 3: Apply Attention\nWe now pass these 4D tensors to the attention function. Because of the shape (1, 2, 3, 2), the attention module calculates dot products on the last two dimensions (3, 2).\n# 3. Apply attention\nattn_output, attention_weights = self.attention(Q, K, V, mask)\n\nHead 1 performs attention on its (3, 2) data.\nHead 2 performs attention on its (3, 2) data.\n\nOutput Shape: (1, 2, 3, 2). The results are still separated by head.\n\n\nStep 4: Combining Heads\nWe have the results, but they are split. We need to stitch them back together into a single representation for each word. We essentially reverse Step 2.\n# 4. Combine heads\nattn_output = self.combine_heads(attn_output)\n1. Transpose Back: Swap Heads and Seq dimensions.\n\nShape changes from (1, 2, 3, 2) to (1, 3, 2, 2).\n\n2. View (Flatten): Merge the ‚ÄúHeads‚Äù dimension (size 2) and ‚Äú\\(d_k\\)‚Äù dimension (size 2) back into ‚Äú\\(d_{model}\\)‚Äù (size 4).\n\nShape changes from (1, 3, 2, 2) to (1, 3, 4).\n\nEffectively, the vector of size 2 from Head 1 and the vector of size 2 from Head 2 are now concatenated side-by-side to make a vector of size 4.\n\n\nStep 5: The Final Mixer (\\(W_O\\))\nWe have a vector of size 4, but the top half is purely from Head 1, and the bottom half is purely from Head 2. They haven‚Äôt interacted yet.\n# 5. Final linear projection\noutput = self.W_O(attn_output)\nWe pass this through a final linear layer, \\(W_O\\) (Output weights). This layer mixes these features together. It allows the model to synthesize the ‚Äúgrammar insights‚Äù from Head 1 with the ‚Äúsemantic insights‚Äù from Head 2 into a single, unified representation for the word.\n\nFinal Output Shape: (1, 3, 4)\n\nWe started with (1, 3, 4) and ended with (1, 3, 4), but the vectors now contain rich, contextual information gathered from multiple perspectives across the whole sentence."
  },
  {
    "objectID": "posts/multi-head-attention/index.html#deep-dive-why-one-big-matrix",
    "href": "posts/multi-head-attention/index.html#deep-dive-why-one-big-matrix",
    "title": "Multi-Head Attention: A Tensor-First Walkthrough",
    "section": "Deep Dive: Why ‚ÄúOne Big Matrix‚Äù?",
    "text": "Deep Dive: Why ‚ÄúOne Big Matrix‚Äù?\nYou might wonder why we typically use one large linear layer (size \\(d_{model}\\)) instead of creating a list of smaller separate layers (size \\(d_k\\)).\nMathematically, they are identical. Computationally, one big matrix is much faster.\nGPUs love crunching massive matrices. It is significantly more efficient to perform one massive matrix multiplication (\\(512 \\times 512\\)) and then slice the result in memory, rather than launching 8 separate, smaller kernel operations (\\(512 \\times 64\\)) in a loop. This implementation is a standard trick to maximize hardware utilization."
  },
  {
    "objectID": "posts/multi-head-attention/index.html#summary",
    "href": "posts/multi-head-attention/index.html#summary",
    "title": "Multi-Head Attention: A Tensor-First Walkthrough",
    "section": "Summary",
    "text": "Summary\nMulti-Head Attention is a sophisticated way of saying ‚Äúdo parallel processing and mix the results.‚Äù\nBy splitting the embedding dimension into distinct heads, we allow the Transformer to learn different types of relationships simultaneously. The intricate steps of view and transpose operations is simply the mechanism required to arrange the data efficiently so GPUs can perform this parallel computation and then stitch the diverse insights back together."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ramu Nalla",
    "section": "",
    "text": "I am a Senior Data Scientist at HighLevel, where I focus on building and scaling Agentic AI systems using Large Language Models (LLMs). My work involves tackling complex NLP challenges at scale, specifically specializing in engineering production-grade RAG pipelines with tools like LangChain and LangGraph. My work also involves developing AI models to enhance SaaS business intelligence and establishing robust evaluation frameworks for LLM-based systems to drive measurable business impact.\nBefore HighLevel, I worked as a Senior ML Engineer at Enphase Energy for nearly five years, where I spearheaded chatbot initiatives and developed AI models to detect hardware-product failures.\nMy career has been primarily focused on applied machine learning, bridging the gap between innovation and real-world applications.\nI earned my B.Tech and M.Tech, along with an MBA minor, from IIT Kanpur.\nI enjoy blogging about Machine Learning, and I am currently working on a series of posts about NLP, MLOps, and Agentic AI architectures."
  },
  {
    "objectID": "index.html#latest-posts",
    "href": "index.html#latest-posts",
    "title": "Ramu Nalla",
    "section": "Latest Posts",
    "text": "Latest Posts\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        date - Oldest\n      \n      \n        date - Newest\n      \n      \n        title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\ndate\n\n\n\ntitle\n\n\n\n\n\n\n\n\nJan 20, 2026\n\n\nNamed Entity Recognition: The ‚ÄòWho, What, Where‚Äô of NLP\n\n\n\n\n\n\nJan 13, 2026\n\n\nPositional Encoding in Transformers: The Geography of Language\n\n\n\n\n\n\nJan 12, 2026\n\n\nMulti-Head Attention: A Tensor-First Walkthrough\n\n\n\n\n\n\nJan 10, 2026\n\n\nScaled Dot-Product Attention: A Tensor-First Approach\n\n\n\n\n\n\nJan 1, 2026\n\n\nThe Definitive Guide to BLEU Score: The Mathematics of Machine Translation\n\n\n\n\n\n\nDec 17, 2025\n\n\nBuilding a Minimal RAG Pipeline from Scratch\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#news",
    "href": "index.html#news",
    "title": "Ramu Nalla",
    "section": "News",
    "text": "News\n\n\n\nDate\nUpdate\n\n\n\n\nOct 29, 2025\nJoined HighLevel as Sr.¬†Data Scientist\n\n\n\n\n\n\n    \n\n\nBest to reach out to me over email or DM me on LinkedIn.\n\n\n¬© 2026 Ramu Nalla. All rights reserved."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Download Full CV (PDF)"
  },
  {
    "objectID": "posts/2026-01-17-simple-rag-tutorial/index.html",
    "href": "posts/2026-01-17-simple-rag-tutorial/index.html",
    "title": "Building a Minimal RAG Pipeline from Scratch",
    "section": "",
    "text": "Published by Ramu Nalla - December 17, 2025\nIf you have worked with Large Language Models (LLMs) for more than five minutes, you have likely run into their biggest flaw: hallucinations. They sound confident, but they don‚Äôt know your private data, and their knowledge cutoff is always in the past.\nRetrieval-Augmented Generation (RAG) is the industry standard solution to this. It bridges the gap between a ‚Äúfrozen‚Äù LLM and your dynamic, private data.\nBefore I started using complex vector databases like Pinecone or Milvus, I wanted to understand exactly what was happening under the hood. So, I built a minimal RAG pipeline using nothing but Python and NumPy."
  },
  {
    "objectID": "posts/2026-01-17-simple-rag-tutorial/index.html#the-core-concept",
    "href": "posts/2026-01-17-simple-rag-tutorial/index.html#the-core-concept",
    "title": "Building a Minimal RAG Pipeline from Scratch",
    "section": "The Core Concept",
    "text": "The Core Concept\nRAG isn‚Äôt a single algorithm; it‚Äôs a workflow.\n\nRetrieval: Find the most relevant documents for a user‚Äôs question.\nAugmentation: Paste those documents into the LLM‚Äôs prompt context.\nGeneration: Ask the LLM to answer the question using only that context.\n\nThe ‚Äúmagic‚Äù happens in step 1. How does a computer know that ‚Äúneural network‚Äù and ‚Äúdeep learning‚Äù are related? The answer is Vector Embeddings."
  },
  {
    "objectID": "posts/2026-01-17-simple-rag-tutorial/index.html#the-code-vector-search-in-pure-python",
    "href": "posts/2026-01-17-simple-rag-tutorial/index.html#the-code-vector-search-in-pure-python",
    "title": "Building a Minimal RAG Pipeline from Scratch",
    "section": "The Code: ‚ÄúVector Search‚Äù in Pure Python",
    "text": "The Code: ‚ÄúVector Search‚Äù in Pure Python\nIn a real production environment, we use embedding models (like OpenAI‚Äôs text-embedding-3-small) to turn text into massive lists of numbers. For this demo, I‚Äôll manually create small 3-dimensional vectors to visualize the math.\nHere is the entire logic in one script:\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# 1. Our \"Knowledge Base\"\n# In reality, this would be thousands of PDF chunks stored in a Vector DB.\ndocuments = [\n    \"RAG bridges the gap between LLMs and private data.\",\n    \"LangChain is a popular framework for building AI agents.\",\n    \"Vector databases like Pinecone store embeddings for fast retrieval.\"\n]\n\n# 2. Mock Embeddings\n# Imagine these are the output of an embedding model.\n# Notice how Doc 1 (0.9, 0.1, ...) and Doc 3 (0.1, 0.2, ...) are mathematically different.\ndoc_vectors = np.array([\n    [0.9, 0.1, 0.1],  # Embedding for Document 1\n    [0.2, 0.8, 0.2],  # Embedding for Document 2\n    [0.1, 0.2, 0.9]   # Embedding for Document 3\n])\n\n# 3. The User's Query\n# \"How do I use private data with LLMs?\" \n# This query is semantically similar to Document 1.\nquery_vector = np.array([[0.85, 0.15, 0.1]]) \n\n# 4. The Retrieval Step (Cosine Similarity)\n# We calculate the angle between the Query and every Document.\n# Higher score = Closer match.\nscores = cosine_similarity(query_vector, doc_vectors).flatten()\n\n# 5. Get the Winner\nbest_match_idx = np.argmax(scores)\nretrieved_doc = documents[best_match_idx]\nconfidence = scores[best_match_idx]\n\nprint(f\"‚úÖ User Query Mapped to: '{retrieved_doc}'\")\nprint(f\"üìä Confidence Score: {confidence:.4f}\")\n\nWhat just happened?\nWhen we ran cosine_similarity, Python calculated the angle between our query vector and the document vectors.\n\nThe query vector [0.85, 0.15, 0.1] was heavily weighted towards the first dimension.\nDocument 1 [0.9, 0.1, 0.1] was also weighted towards the first dimension.\nTherefore, the math says: ‚ÄúThese two ideas are related.‚Äù"
  },
  {
    "objectID": "posts/2026-01-17-simple-rag-tutorial/index.html#moving-to-production",
    "href": "posts/2026-01-17-simple-rag-tutorial/index.html#moving-to-production",
    "title": "Building a Minimal RAG Pipeline from Scratch",
    "section": "Moving to Production",
    "text": "Moving to Production\nWhile this numpy example is great for intuition, it doesn‚Äôt scale to millions of rows. In my professional work, moving from this proof-of-concept to production involves:\n\nReal Embeddings: Replacing manual vectors with sentence-transformers or OpenAI embeddings.\nVector Store: Using a dedicated database (ChromaDB, Weaviate, or Snowflake) to index millions of vectors.\nReranking: Adding a second step to double-check the relevance of the retrieved documents.\n\nI‚Äôll be writing more about building Agentic workflows on top of this RAG architecture in future posts. Stay tuned!"
  },
  {
    "objectID": "posts/named-entity-recognition/index.html",
    "href": "posts/named-entity-recognition/index.html",
    "title": "Named Entity Recognition: The ‚ÄòWho, What, Where‚Äô of NLP",
    "section": "",
    "text": "Published by Ramu Nalla - January 20, 2026\nImagine you are a bank analyst. Every morning, you receive 10,000 news articles about the stock market. Somewhere in that mountain of text is a sentence saying: ‚ÄúApple Inc.¬†is acquiring a startup in Berlin for $50 million.‚Äù\nTo a human, the important facts jump out immediately:\nTo a computer, that sentence is just a string of 62 meaningless characters.\nNamed Entity Recognition (NER) is the sub-field of NLP that bridges this gap. It is the task of locating and classifying specific entities in unstructured text into predefined categories.\nIn this post, I will strip away the complexity of modern Large Language Models and go back to basics. We will understand how NER works under the hood and build a simple, functional NER model using raw PyTorch."
  },
  {
    "objectID": "posts/named-entity-recognition/index.html#why-do-we-need-ner",
    "href": "posts/named-entity-recognition/index.html#why-do-we-need-ner",
    "title": "Named Entity Recognition: The ‚ÄòWho, What, Where‚Äô of NLP",
    "section": "Why Do We Need NER?",
    "text": "Why Do We Need NER?\nNER is rarely the ‚Äúfinal product.‚Äù It is usually the first step in a larger pipeline. It turns unstructured chaos into structured tables.\n\n1. Customer Support Routing\nIf a user tweets: ‚ÄúMy iPhone 14 screen cracked and I need a repair in Chicago,‚Äù an NER system detects:\n\nProduct: iPhone 14\nIssue: Screen cracked\nLocation: Chicago\n\nThe system can then automatically route this ticket to the ‚ÄúHardware Repair Team‚Äù in the ‚ÄúIllinois Region,‚Äù saving hours of manual sorting.\n\n\n2. Algorithmic Trading\nFinancial bots scrape news feeds milliseconds after they are published. ‚ÄúTesla (TSLA) announces battery factory expansion in Texas.‚Äù The bot identifies ORG: Tesla and LOC: Texas, cross-references this with sentiment analysis, and executes a trade before a human trader has even sipped their coffee.\n\n\n3. Healthcare (Clinical NER)\nDoctors write messy notes. ‚ÄúPatient prescribed 50mg Metoprolol daily.‚Äù NER extracts Drug: Metoprolol and Dosage: 50mg to automatically check for dangerous interactions with other drugs the patient is taking."
  },
  {
    "objectID": "posts/named-entity-recognition/index.html#how-it-works-the-bio-scheme",
    "href": "posts/named-entity-recognition/index.html#how-it-works-the-bio-scheme",
    "title": "Named Entity Recognition: The ‚ÄòWho, What, Where‚Äô of NLP",
    "section": "How it Works: The ‚ÄúBIO‚Äù Scheme",
    "text": "How it Works: The ‚ÄúBIO‚Äù Scheme\nNER is technically a Token Classification task. We don‚Äôt classify the whole sentence; we classify every single word.\nBut how do we handle multi-word entities like ‚ÄúNew York City‚Äù? We can‚Äôt just tag ‚ÄúNew‚Äù as Location, ‚ÄúYork‚Äù as Location, and ‚ÄúCity‚Äù as Location, because the model might think they are three separate cities.\nWe use the BIO tagging scheme:\n\nB-XXX (Begin): The first token of an entity.\nI-XXX (Inside): Subsequent tokens of the same entity.\nO (Outside): Not an entity.\n\nExample: ‚ÄúSteve Jobs founded Apple‚Äù\n\n\n\nWord\nTag\nMeaning\n\n\n\n\nSteve\nB-PER\nBeginning of Person\n\n\nJobs\nI-PER\nInside of Person\n\n\nfounded\nO\nOutside\n\n\nApple\nB-ORG\nBeginning of Organization"
  },
  {
    "objectID": "posts/named-entity-recognition/index.html#building-a-simple-ner-model-in-pytorch",
    "href": "posts/named-entity-recognition/index.html#building-a-simple-ner-model-in-pytorch",
    "title": "Named Entity Recognition: The ‚ÄòWho, What, Where‚Äô of NLP",
    "section": "Building a Simple NER Model in PyTorch",
    "text": "Building a Simple NER Model in PyTorch\nWe will build a Long Short-Term Memory (LSTM) network. While Transformers are the state-of-the-art, LSTMs remain excellent for understanding the sequential nature of NER (e.g., seeing ‚ÄúNew‚Äù makes ‚ÄúYork‚Äù more likely to be a location).\n\n1. The Setup\nFirst, let‚Äôs define our ‚Äúvocabulary‚Äù (words the model knows) and our ‚Äútag set‚Äù (labels we want to predict).\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# A tiny dummy dataset\n# Sentence: \"Apple is looking at buying U.K. startup for $1 billion\"\ntraining_data = [\n    (\n        \"Apple is looking at buying U.K. startup for $1 billion\".split(),\n        [\"B-ORG\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\", \"O\", \"B-MONEY\", \"I-MONEY\", \"I-MONEY\"]\n    ),\n    (\n        \"Steve Jobs founded Apple\".split(),\n        [\"B-PER\", \"I-PER\", \"O\", \"B-ORG\"]\n    )\n]\n\n# Create mappings (Word -&gt; ID, Tag -&gt; ID)\nword_to_ix = {}\nfor sent, tags in training_data:\n    for word in sent:\n        if word not in word_to_ix:\n            word_to_ix[word] = len(word_to_ix)\n\ntag_to_ix = {\n    \"O\": 0, \n    \"B-ORG\": 1, \"I-ORG\": 2, \n    \"B-PER\": 3, \"I-PER\": 4, \n    \"B-LOC\": 5, \"I-LOC\": 6,\n    \"B-MONEY\": 7, \"I-MONEY\": 8\n}\n\n\n2. The Model Architecture\nOur model will have three layers:\n\nEmbedding Layer: Converts word IDs into dense vectors.\nLSTM Layer: Reads the sequence and captures context (history).\nLinear (Hidden) Layer: Projects the LSTM output to the number of tags.\n\nclass NERModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, tagset_size):\n        super(NERModel, self).__init__()\n        self.hidden_dim = hidden_dim\n\n        # 1. Word Embeddings\n        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n\n        # 2. LSTM\n        # Input: Embedding Vector\n        # Output: Hidden Vector (Context)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n\n        # 3. Output Layer\n        # Maps hidden state -&gt; Tag probability scores\n        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n\n    def forward(self, sentence):\n        # Step 1: Embed the words\n        # Shape: (Seq_Len) -&gt; (Seq_Len, Embedding_Dim)\n        embeds = self.word_embeddings(sentence)\n        \n        # Step 2: Run LSTM\n        # view(len, 1, -1) adds a batch dimension of 1 because LSTM expects (Seq, Batch, Dim)\n        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n        \n        # Step 3: Project to Tag Space\n        # Shape: (Seq_Len, Tagset_Size)\n        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n        \n        # Step 4: Softmax (Log Softmax for stability)\n        tag_scores = nn.functional.log_softmax(tag_space, dim=1)\n        \n        return tag_scores\n\n\n3. Training the Model\nWe use Negative Log Likelihood Loss (NLLLoss) because we are doing multi-class classification for each word. We‚Äôll set up a standard training loop using Stochastic Gradient Descent (SGD).\n# Hyperparameters\nEMBEDDING_DIM = 6\nHIDDEN_DIM = 6\n\n# Initialize model\nmodel = NERModel(len(word_to_ix), EMBEDDING_DIM, HIDDEN_DIM, len(tag_to_ix))\nloss_function = nn.NLLLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\n# Helper to convert sentence to indices\ndef prepare_sequence(seq, to_ix):\n    idxs = [to_ix[w] for w in seq]\n    return torch.tensor(idxs, dtype=torch.long)\n\n# Training Loop\nfor epoch in range(300): \n    for sentence, tags in training_data:\n        # 1. Clear gradients\n        model.zero_grad()\n\n        # 2. Prepare inputs\n        sentence_in = prepare_sequence(sentence, word_to_ix)\n        targets = prepare_sequence(tags, tag_to_ix)\n\n        # 3. Forward pass\n        tag_scores = model(sentence_in)\n\n        # 4. Calculate Loss & Backprop\n        loss = loss_function(tag_scores, targets)\n        loss.backward()\n        optimizer.step()\n\n\n4. Testing the Model\nLet‚Äôs see if the model memorized our simple examples. We will pass a test sentence through the trained model and decode the indices back into readable tags.\nwith torch.no_grad():\n    # Test sentence: \"Steve Jobs founded Apple\"\n    inputs = prepare_sequence(training_data[1][0], word_to_ix)\n    tag_scores = model(inputs)\n    \n    # Get the index of the highest score for each word\n    _, predicted_indices = torch.max(tag_scores, 1)\n    \n    # Convert indices back to tag names\n    ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n    predicted_tags = [ix_to_tag[idx.item()] for idx in predicted_indices]\n\n    print(f\"Sentence: {training_data[1][0]}\")\n    print(f\"Predicted Tags: {predicted_tags}\")\n\n# Output:\n# Sentence: ['Steve', 'Jobs', 'founded', 'Apple']\n# Predicted Tags: ['B-PER', 'I-PER', 'O', 'B-ORG']"
  },
  {
    "objectID": "posts/named-entity-recognition/index.html#understanding-the-tensor-shapes",
    "href": "posts/named-entity-recognition/index.html#understanding-the-tensor-shapes",
    "title": "Named Entity Recognition: The ‚ÄòWho, What, Where‚Äô of NLP",
    "section": "Understanding the Tensor Shapes",
    "text": "Understanding the Tensor Shapes\nThis is where most people get stuck in PyTorch. Let‚Äôs trace the shapes for the sentence ‚ÄúSteve Jobs founded Apple‚Äù (Length 4).\n\nInput: [ID_Steve, ID_Jobs, ...] \\(\\rightarrow\\) Shape (4)\nEmbedding: Each ID becomes a vector of size 6. \\(\\rightarrow\\) Shape (4, 6)\nLSTM Input: PyTorch LSTMs expect 3D inputs: (Seq_Len, Batch, Dim). We reshape. \\(\\rightarrow\\) Shape (4, 1, 6)\nLSTM Output: Returns vectors of hidden_dim (6). \\(\\rightarrow\\) Shape (4, 1, 6)\nLinear Layer: We squash the batch dimension to feed into the Linear layer. Input (4, 6). Output is (4, num_tags).\n\nFor every word (4 words), we get 9 scores (one for each tag in our tagset)."
  },
  {
    "objectID": "posts/named-entity-recognition/index.html#summary",
    "href": "posts/named-entity-recognition/index.html#summary",
    "title": "Named Entity Recognition: The ‚ÄòWho, What, Where‚Äô of NLP",
    "section": "Summary",
    "text": "Summary\nNamed Entity Recognition is about context. The word ‚ÄúApple‚Äù is a fruit if ‚Äúate‚Äù is nearby, but an Organization if ‚Äúfounded‚Äù is nearby.\nBy using an LSTM (or Transformer), we allow the model to carry this context across the sentence. While our example was tiny, modern NER systems scaling this up can read millions of documents a day, structuring the world‚Äôs information one entity at a time."
  },
  {
    "objectID": "posts/positional-encodings/index.html",
    "href": "posts/positional-encodings/index.html",
    "title": "Positional Encoding in Transformers: The Geography of Language",
    "section": "",
    "text": "Published by Ramu Nalla - January 13, 2026\nImagine you tossed the sentence ‚ÄúThe cat ate the fish‚Äù into a blender. You know exactly which words are in the mix, but you have lost the story. ‚ÄúThe fish ate the cat‚Äù would look identical in that blender.\nThis is the fundamental problem with the standard Transformer architecture.\nUnlike Recurrent Neural Networks (RNNs), which process words sequentially (\\(t_1\\), then \\(t_2\\), then \\(t_3\\)), Transformers process all words in a sentence simultaneously in parallel. This parallelism is what makes them so fast, but it comes at a steep cost: the model is inherently invariant to word order. It sees a ‚Äúbag of words,‚Äù not a sentence. To fix this, we must inject a sense of order back into the data. We need to ‚Äútag‚Äù every word with its geographic location in the sentence before it enters the Transformer.\nThis technique is called Positional Encoding (PE).\nIn this post, we are going to explore the two main strategies for doing this: the elegant, fixed mathematical approach (Sinusoidal) used in the original ‚ÄúAttention Is All You Need‚Äù paper, and the simpler, data-driven approach (Learnable)."
  },
  {
    "objectID": "posts/positional-encodings/index.html#the-goal-a-unique-fingerprint",
    "href": "posts/positional-encodings/index.html#the-goal-a-unique-fingerprint",
    "title": "Positional Encoding in Transformers: The Geography of Language",
    "section": "The Goal: A Unique Fingerprint",
    "text": "The Goal: A Unique Fingerprint\nWhat properties should a good positional tag have?\n\nUnique: Position 5 must look different from Position 6.\nConsistent distance: The ‚Äúmathematical distance‚Äù between Position 5 and 6 should be the same as between Position 10 and 11.\nExtensible: Ideally, the model should be able to handle sentences longer than ones it saw during training.\nDeterministic: The tag for position 5 should always be the same.\n\nYou might think, ‚ÄúWhy not just add the number 1 to the first word vector, 2 to the second, and so on?‚Äù\nThe problem is scale. For a 1000-word document, the 1000th word would have massive values added to it, completely drowning out the actual semantic meaning of the word embedding (which usually consists of small numbers between -1 and 1).\nWe need a system that stays within a bounded range. Enter trigonometry."
  },
  {
    "objectID": "posts/positional-encodings/index.html#strategy-1-sinusoidal-positional-encoding-the-fixed-approach",
    "href": "posts/positional-encodings/index.html#strategy-1-sinusoidal-positional-encoding-the-fixed-approach",
    "title": "Positional Encoding in Transformers: The Geography of Language",
    "section": "Strategy 1: Sinusoidal Positional Encoding (The Fixed Approach)",
    "text": "Strategy 1: Sinusoidal Positional Encoding (The Fixed Approach)\nThe authors of the original Transformer paper proposed a brilliant, non-learnable solution based on sine and cosine waves of different frequencies.\nHere is the intuition: Imagine a row of analog clocks.\n\nThe first clock has a hand that spins very fast (seconds).\nThe next spins slower (minutes).\nThe next spins even slower (hours).\n\nIf you take a snapshot of all the clock hands at any given moment, that specific combination of hand positions is a unique timestamp.\nSinusoidal encoding works similarly. Every position gets a unique combination of sine and cosine values generated by waves oscillating at different speeds across the embedding dimension.\n\nThe Formula\nThis is the part that usually scares people away. Let‚Äôs look at it and then decode it.\nGiven a position pos in the sentence, and a specific dimension index i in the embedding vector, the encoding is computed as:\n\\[PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\\]\n\\[PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\\]\nDon‚Äôt panic. Let‚Äôs break down the technical interpretation.\n\n\n\nConceptual Illustration of Sine - Cosine Positional Encoding.\n\n\n\n\nTechnical Interpretation: Position vs.¬†Dimension\nThe crucial thing to understand is that the frequency of the wave changes depending on where you are in the embedding vector.\n1. Interpreting Along the Dimensions (Fixing position pos)\nImagine you are looking at the word at Position 10. Its positional encoding is a vector of size \\(d_{model}\\) (e.g., 512).\n\nAt the start of that vector (low i indices), the denominator is small. The frequency is high. The values change rapidly.\nAt the end of that vector (high i indices), the denominator is huge. The frequency is very low. The wave is stretched out incredibly long.\n\nThis means early dimensions encode local order, while later dimensions encode global position.\n2. Interpreting Along the Positions (Fixing dimension i)\nNow, imagine you are looking at just Dimension 0 across the entire sentence. You will see a perfect high-frequency sine wave as you read from word 1 to word 100. If you look at Dimension 256, you will see a much slower cosine wave.\n\n\nA Concrete Example (Tiny Math Trace)\nLet‚Äôs use tiny numbers to see it work.\n\n\\(d_{model} = 4\\)\nSequence Length = 2 (Positions 0 and 1)\n\nWe need to build a \\((2 \\times 4)\\) matrix.\nThe Frequencies (The Denominator term):\nWe have pairs of dimensions \\(2i\\) and \\(2i+1\\). The indices \\(i\\) are \\(0\\) and \\(1\\). The denominator is \\(10000^{(2i/4)}\\).\n\nFor dimensions 0 and 1 (\\(i=0\\)): Denominator is \\(1\\). Frequency term is \\(1\\).\nFor dimensions 2 and 3 (\\(i=1\\)): Denominator is \\(100\\). Frequency term is \\(0.01\\).\n\nCalculating Position 0:\n\nDim 0 (sin, freq 1): \\(\\sin(0) = 0\\)\nDim 1 (cos, freq 1): \\(\\cos(0) = 1\\)\nDim 2 (sin, freq 0.01): \\(\\sin(0) = 0\\)\nDim 3 (cos, freq 0.01): \\(\\cos(0) = 1\\)\nPE at Pos 0: [0, 1, 0, 1]\n\nCalculating Position 1:\n\nDim 0 (sin, freq 1): \\(\\sin(1) \\approx 0.84\\)\nDim 1 (cos, freq 1): \\(\\cos(1) \\approx 0.54\\)\nDim 2 (sin, freq 0.01): \\(\\sin(0.01) \\approx 0.01\\)\nDim 3 (cos, freq 0.01): \\(\\cos(0.01) \\approx 0.99\\)\nPE at Pos 1: [0.84, 0.54, 0.01, 0.99]\n\nWe now have unique vectors for position 0 and 1 that are bounded between -1 and 1. We literally add these to the semantic word embeddings.\n\n\nThe Implementation (PyTorch)\nImplementing this efficiently requires some clever broadcasting and a numerical stability trick.\nimport torch\nimport torch.nn as nn\nimport math\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"\n    Sinusoidal positional encoding implementation.\n    \"\"\"\n    def __init__(self, d_model: int, max_len: int = 5000, dropout_p: float = 0.1):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout_p)\n\n        # 1. Create the Matrix Buffer\n        # Initialize a matrix (max_len x d_model) with zeros.\n        pe = torch.zeros(max_len, d_model) \n\n        # 2. Create Position Indices\n        # A column vector [[0], [1], [2], ...]. Shape: (max_len, 1)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n\n        # 3. Calculate the Frequencies (The \"div_term\")\n        # The formula uses 10000^(2i/d_model).\n        # To avoid huge numbers, we use the log trick: a^b = exp(b * ln(a))\n        # This calculates the denominator term for the even indices (0, 2, 4...).\n        # Shape: (d_model / 2)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n\n        # 4. Apply Sine and Cosine using Broadcasting\n        # Multiply position (col vector) by frequencies (row vector) to get the\n        # arguments for sin/cos. Fill even cols with sin, odd with cos.\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        # 5. Add Batch Dimension and Register\n        # Shape becomes (1, max_len, d_model) for easy addition later.\n        pe = pe.unsqueeze(0)\n\n        # register_buffer means: \"Save this tensor with the model weights, \n        # but do NOT update it during training with gradients.\"\n        self.register_buffer('pe', pe)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Args:\n            x: Input embeddings of shape (batch_size, actual_seq_len, d_model)\n        \"\"\"\n        # 1. Slice the pre-computed matrix to match the actual sequence length.\n        # If input is 10 words long, take the first 10 rows of 'pe'.\n        # 2. ADD the encoding to the input embeddings.\n        x = x + self.pe[:, :x.size(1), :]\n        \n        # Apply dropout for regularization\n        return self.dropout(x)\nThe beauty of this code is that pe is computed once when the model is initialized. During training or inference, it‚Äôs just a fast lookup and addition operation."
  },
  {
    "objectID": "posts/positional-encodings/index.html#strategy-2-learnable-positional-encoding-the-data-driven-approach",
    "href": "posts/positional-encodings/index.html#strategy-2-learnable-positional-encoding-the-data-driven-approach",
    "title": "Positional Encoding in Transformers: The Geography of Language",
    "section": "Strategy 2: Learnable Positional Encoding (The Data-Driven Approach)",
    "text": "Strategy 2: Learnable Positional Encoding (The Data-Driven Approach)\nAfter the original Transformer paper, researchers asked a pragmatic question: ‚ÄúWhy work so hard with trigonometry? Why are we forcing a human-designed pattern onto the model? Why not just let the neural network learn the best position tags itself?‚Äù\nThis is the Learnable approach. It is deceptively simple, yet it powers some of the most famous models in history, including BERT and the early GPT series.\n\nThe Intuition: A Second Vocabulary\nIn this approach, we treat ‚ÄúPosition‚Äù exactly the same way we treat ‚ÄúWords.‚Äù\nJust as the model has a lookup table (an embedding layer) for words‚Äîwhere ID 256 might map to the vector for ‚ÄúApple‚Äù‚Äîit has a second lookup table for positions.\n\nPosition ID 0 maps to a learned vector \\(v_0\\).\nPosition ID 1 maps to a learned vector \\(v_1\\).\n‚Ä¶\nPosition ID 511 maps to a learned vector \\(v_{511}\\).\n\nThese vectors are initialized with random noise. During the training process, while the model is learning to translate English to French, it is simultaneously adjusting these position vectors via backpropagation. It figures out exactly what numbers need to be in those vectors to help it distinguish ‚ÄúThe cat ate the mouse‚Äù from ‚ÄúThe mouse ate the cat.‚Äù\n\n\nTechnical Interpretation: The ‚ÄúBlack Box‚Äù Dimensions\nUnlike the Sinusoidal approach, where we know exactly what Dimension 0 represents (a high-frequency wave) and what Dimension 511 represents (a low-frequency wave), Learnable Encodings are opaque.\n1. No Fixed Mathematical Meaning\nIf you look at Dimension \\(i\\) in a learnable position vector, it does not necessarily correspond to a frequency or a clear pattern.\n\nDimension 0 might learn to be a simple counter.\nDimension 1 might learn to toggle on and off every 5 words.\nDimension 2 might look like random noise to us, but contain a specific signal the model finds useful for attention heads.\n\nWe lose the interpretability of the sine waves, but we gain flexibility. The model is free to invent its own coordinate system.\n2. Neighbor Similarity\nDespite the lack of a fixed formula, a well-trained model usually converges on a logical structure. It tends to learn vectors such that Position 5 and Position 6 are very similar (they have a high cosine similarity).\nWhy? Because words next to each other usually share context. If the positional tags for neighbors were wildly different (orthogonal), it would be much harder for the Attention mechanism to learn local relationships like adjective-noun pairings. The model naturally drifts toward making neighbors ‚Äúlook‚Äù similar in vector space to minimize its loss.\n\n\nHow It Works Overall: The ‚ÄúSummation‚Äù Step\nA common point of confusion is where this positional information enters the network. Does it go into the Query? The Key?\nIn the standard architecture (like BERT or the original Transformer), the positional vector is added directly to the Input Embedding before the data ever touches the first Attention layer.\n\nStep 1: Word Lookup. We convert the word ‚ÄúCat‚Äù into a 512-dimensional semantic vector.\nStep 2: Position Lookup. We look up the learned vector for ‚ÄúPosition 5.‚Äù\nStep 3: Addition. We element-wise add them together.\n\n\\[Input = Word\\_Embedding + Position\\_Embedding\\]\nWait, doesn‚Äôt adding two vectors corrupt the information? If ‚ÄúCat‚Äù is [0.5, 0.2] and Position 5 is [0.1, 0.9], the result [0.6, 1.1] is neither purely ‚ÄúCat‚Äù nor purely ‚ÄúPosition 5.‚Äù\nThis is true. However, in high-dimensional space (e.g., 512, 1024, or 4096 dimensions), the model learns to segregate the information. It might use the first few dozen dimensions primarily for positional signaling and the rest for semantic meaning, or interleave them in complex ways.\nOnce this ‚Äúsummed‚Äù vector enters the first Attention layer:\n\nIt is projected into Queries (\\(Q\\)), Keys (\\(K\\)), and Values (\\(V\\)).\nBecause the position information was baked into the input, \\(Q\\), \\(K\\), and \\(V\\) all contain traces of that positional signal.\nWhen \\(Q\\) and \\(K\\) interact (dot product), the positional traces help determine the attention score. (e.g., ‚ÄúI am a noun at Pos 5, looking for an adjective near Pos 4‚Äù).\n\n\n\nThe Implementation (PyTorch)\nIt‚Äôs almost too simple to show code for:\nclass LearnablePositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        # Just a standard embedding layer!\n        # Input ID 0 gives vector for pos 0, Input ID 5 gives vector for pos 5.\n        self.pos_embedding = nn.Embedding(max_len, d_model)\n\n    def forward(self, x):\n        # x shape: (batch_size, seq_len, d_model)\n        seq_len = x.size(1)\n        \n        # Create indices [0, 1, ... seq_len-1] on the right device\n        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n        \n        # Lookup the learnable vectors\n        pos_vectors = self.pos_embedding(positions)\n        \n        # Add them to the word embeddings\n        return x + pos_vectors"
  },
  {
    "objectID": "posts/positional-encodings/index.html#the-showdown-sinusoidal-vs.-learnable",
    "href": "posts/positional-encodings/index.html#the-showdown-sinusoidal-vs.-learnable",
    "title": "Positional Encoding in Transformers: The Geography of Language",
    "section": "The Showdown: Sinusoidal vs.¬†Learnable",
    "text": "The Showdown: Sinusoidal vs.¬†Learnable\nWhich one should you use? For a long time, this was a hot debate.\n\n\n\n\n\n\n\n\nFeature\nSinusoidal (Fixed)\nLearnable\n\n\n\n\nHow it works\nDeterministic math formula.\nLearned weights during training.\n\n\nParameters\nZero trainable parameters.\nmax_len * d_model parameters.\n\n\nExtensibility\nExcellent. Can theoretically handle sequences longer than seen during training.\nPoor. Cannot handle positions beyond max_len seen in training.\n\n\nFlexibility\nRigid structure imposed by humans.\nFlexible; model learns whatever structure the data demands.\n\n\nImplementation\nSlightly complex math to setup.\nVery simple (just an Embedding layer).\n\n\n\n\nThe Verdict\nIn practice, for standard tasks like translation, both perform remarkably similarly. The model is usually smart enough to figure out ordering regardless of which method you use, as long as the tags are unique.\nHowever, Sinusoidal is often preferred in research because it doesn‚Äôt add parameters and has that theoretical ability to extrapolate to longer sequences. Learnable is preferred for its simplicity and is often the default in libraries like Hugging Face‚Äôs BERT implementation."
  },
  {
    "objectID": "posts/positional-encodings/index.html#summary",
    "href": "posts/positional-encodings/index.html#summary",
    "title": "Positional Encoding in Transformers: The Geography of Language",
    "section": "Summary",
    "text": "Summary\nWithout Positional Encoding, a Transformer is just a powerful bag-of-words model. By injecting these positional signals‚Äîwhether through fixed trigonometry or learned embeddings‚Äîwe provide the necessary geography for the model to understand the structure of language.\nThe sinusoidal approach, while mathematically dense at first glance, is an elegant solution that uses alternating frequencies to create unique, bounded time-stamps for every token in a sequence."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blogs",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nNamed Entity Recognition: The ‚ÄòWho, What, Where‚Äô of NLP\n\n8 min\n\n\nNLP\n\nDeep Learning\n\nPyTorch\n\nInformation Extraction\n\n\n\nFrom messy text to structured data. We explore what NER is, why it powers everything from search engines to trading bots, and build a simple tagger from scratch in PyTorch.\n\n\n\nJan 20, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nPositional Encoding in Transformers: The Geography of Language\n\n13 min\n\n\nDeep Learning\n\nTransformers\n\nNLP\n\nPyTorch\n\n\n\nHow does a Transformer treat ‚ÄòThe cat ate the fish‚Äô different than ‚ÄòThe fish ate the cat‚Äô? We explore the mathematics of Positional Encoding to see how models learn order‚Ä¶\n\n\n\nJan 13, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-Head Attention: A Tensor-First Walkthrough\n\n10 min\n\n\nDeep Learning\n\nTransformers\n\nPyTorch\n\nNLP\n\n\n\nWhy one attention head isn‚Äôt enough. I will take a deep dive into the PyTorch implementation of Multi-Head Attention, tracing tensor shapes to understand how models gain‚Ä¶\n\n\n\nJan 12, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nScaled Dot-Product Attention: A Tensor-First Approach\n\n7 min\n\n\nDeep Learning\n\nTransformers\n\nPyTorch\n\nNLP\n\n\n\nConfused by Query, Key, and Value? I break down the heart of the Transformer architecture with a line-by-line PyTorch walkthrough and a concrete mathematical trace.\n\n\n\nJan 10, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Definitive Guide to BLEU Score: The Mathematics of Machine Translation\n\n8 min\n\n\nNLP\n\nGenAI\n\nMetrics\n\nMachine Translation\n\n\n\nA deep dive for data scientists into the most famous, and often misunderstood, metric in NLP. We unpack the math of n-grams, clipped precision, and the brevity penalty.\n\n\n\nJan 1, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Minimal RAG Pipeline from Scratch\n\n3 min\n\n\nNLP\n\nLLMs\n\nRAG\n\nPython\n\n\n\nA look under the hood of Retrieval-Augmented Generation. We build a semantic search engine using just NumPy to understand the core math behind the magic.\n\n\n\nDec 17, 2025\n\n\n\n\n\n\nNo matching items"
  }
]