[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/scaled-dot-product-attention/index.html",
    "href": "posts/scaled-dot-product-attention/index.html",
    "title": "Scaled Dot-Product Attention: A Tensor-First Approach",
    "section": "",
    "text": "Published by Ramu Nalla - January 10, 2026\nIf you have ever tried to read the ‚ÄúAttention Is All You Need‚Äù paper or dig into the source code of a Transformer model, you have likely hit a wall. That wall is usually made of Tensors.\nYou see shapes like [32, 8, 10, 64]. You see operations like transpose(-2, -1). You see dot products and softmaxes flying around. It‚Äôs easy to get lost in the dimensionality and lose sight of what is actually happening: Communication.\nAt its core, the attention mechanism is just a way for words in a sentence to ‚Äútalk‚Äù to each other and figure out who they should be focusing on.\nIn this post, I am going to bypass the academic jargon. I will take a standard PyTorch implementation of Scaled Dot-Product Attention, break it down line-by-line, and crucially, I will trace the math with real numbers so you can see exactly what happens to the data."
  },
  {
    "objectID": "posts/scaled-dot-product-attention/index.html#the-code-scaled-dot-product-attention",
    "href": "posts/scaled-dot-product-attention/index.html#the-code-scaled-dot-product-attention",
    "title": "Scaled Dot-Product Attention: A Tensor-First Approach",
    "section": "The Code: Scaled Dot-Product Attention",
    "text": "The Code: Scaled Dot-Product Attention\nLet‚Äôs start with the raw code. This is a standard implementation you might find in any modern NLP library.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass ScaledDotProductAttention(nn.Module):\n    \"\"\"\n    Scaled Dot-Product Attention\n    Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V\n    \"\"\"\n\n    def __init__(self, d_k: int, dropout: float = 0.1):\n        super(ScaledDotProductAttention, self).__init__()\n        self.d_k = d_k\n        self.dropout = nn.Dropout(dropout)\n        self.scale = math.sqrt(d_k)\n\n    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor,\n                mask: torch.Tensor = None):\n        \"\"\"\n        Args:\n            Q: Query (batch_size, num_heads, seq_len, d_k)\n            K: Key   (batch_size, num_heads, seq_len, d_k)\n            V: Value (batch_size, num_heads, seq_len, d_v)\n            mask:    (batch_size, 1, seq_len, seq_len)\n        \"\"\"\n        # 1. Compute attention scores\n        # QK^T: (batch_size, num_heads, seq_len, seq_len)\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n        \n        # 2. Apply mask (set masked positions to large negative value)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        \n        # 3. Apply softmax to get attention weights\n        attention_weights = F.softmax(scores, dim=-1)\n        attention_weights = self.dropout(attention_weights)\n        \n        # 4. Apply attention weights to values\n        output = torch.matmul(attention_weights, V)\n        \n        return output, attention_weights\nIf that looks intimidating, don‚Äôt worry. We need to fix how we visualize tensors first."
  },
  {
    "objectID": "posts/scaled-dot-product-attention/index.html#strategy-mastering-tensor-dimensions",
    "href": "posts/scaled-dot-product-attention/index.html#strategy-mastering-tensor-dimensions",
    "title": "Scaled Dot-Product Attention: A Tensor-First Approach",
    "section": "Strategy: Mastering Tensor Dimensions",
    "text": "Strategy: Mastering Tensor Dimensions\nThe biggest source of confusion in PyTorch is seeing a tensor just as a list of numbers. You must start thinking of them as Named Dimensions.\nFor Attention, memorize this hierarchy (B, H, L, D):\n\nBatch (\\(B\\)): How many examples are we processing? (e.g., 32 sentences).\nHeads (\\(H\\)): How many ‚Äúperspectives‚Äù does the model have? (e.g., 8 heads).\nLength (\\(L\\)): How long is the sequence? (e.g., 10 words).\nDimension (\\(D\\)): How much data represents a single word? (e.g., a vector of size 64).\n\nWhen we do operations, we are usually manipulating just the last two dimensions (\\(L\\) and \\(D\\)). The Batch and Heads just come along."
  },
  {
    "objectID": "posts/scaled-dot-product-attention/index.html#a-concrete-example-i-love-ai",
    "href": "posts/scaled-dot-product-attention/index.html#a-concrete-example-i-love-ai",
    "title": "Scaled Dot-Product Attention: A Tensor-First Approach",
    "section": "A Concrete Example: ‚ÄúI Love AI‚Äù",
    "text": "A Concrete Example: ‚ÄúI Love AI‚Äù\nLet‚Äôs trace the code with a tiny example. We will ignore Batch and Heads for a moment and focus on the math for a single sequence.\n\nSequence: ‚ÄúI‚Äù, ‚ÄúLove‚Äù, ‚ÄúAI‚Äù (Length = 3)\nDimension (\\(d_k\\)): 2 (Each word is a vector of 2 numbers)\n\n\nStep 1: The Inputs (Q, K, V)\nImagine our model has learned the following representations for these words.\n\\[Q = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} \\quad (\\text{I, Love, AI})\\]\n\\[K = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} \\quad (\\text{I, Love, AI})\\]\n\\[V = \\begin{bmatrix} 10 & 20 \\\\ 30 & 40 \\\\ 50 & 60 \\end{bmatrix} \\quad (\\text{Content we want to extract})\\]\nNote: In reality, Q and K are different linear projections, but for simplicity, we keep them identical here.\n\n\nStep 2: The Similarity Search (scores)\nThis corresponds to this line of code:\nscores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\nWe want to know: How similar is every word to every other word?\nWe do this by taking the dot product of the Query (\\(Q\\)) and the Transpose of the Key (\\(K^T\\)).\n\\[\\text{Scores} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 1 & 1 & 2 \\end{bmatrix}\\]\nWhat does this matrix tell us?\nLook at the last row (representing the word ‚ÄúAI‚Äù).\n\nColumn 1 (Score 1): ‚ÄúAI‚Äù has some similarity to ‚ÄúI‚Äù.\nColumn 2 (Score 1): ‚ÄúAI‚Äù has some similarity to ‚ÄúLove‚Äù.\nColumn 3 (Score 2): ‚ÄúAI‚Äù is most similar to itself.\n\n\n\nStep 3: Scaling\nWe divide by self.scale, which is \\(\\sqrt{d_k}\\). Since our dimension is \\(d_k=4\\) (hypothetically), \\(\\sqrt{4}=2\\).\n\\[\\text{Scaled Scores} = \\frac{\\text{Scores}}{2} = \\begin{bmatrix} 0.5 & 0 & 0.5 \\\\ 0 & 0.5 & 0.5 \\\\ 0.5 & 0.5 & 1.0 \\end{bmatrix}\\]\nWhy do we do this?\nWithout scaling, dot products can grow huge as dimensions increase. Huge numbers into a Softmax function result in gradients close to zero (the ‚Äúvanishing gradient‚Äù problem), which kills training. Scaling keeps things stable.\n\n\nStep 4: The Softmax (Probabilities)\nattention_weights = F.softmax(scores, dim=-1)\nWe convert raw scores into probabilities that sum to 1. Let‚Äôs look at the first row (Word: ‚ÄúI‚Äù) which had scaled scores [0.5, 0, 0.5]. After Softmax, this might look like: [0.38, 0.24, 0.38].\nThis tells the model: ‚ÄúTo understand the word ‚ÄòI‚Äô, put 38% focus on ‚ÄòI‚Äô, 24% focus on ‚ÄòLove‚Äô, and 38% focus on ‚ÄòAI‚Äô.‚Äù\n\n\nStep 5: The Weighted Sum (The Output)\noutput = torch.matmul(attention_weights, V)\nFinally, we update the word‚Äôs meaning by taking a weighted sum of the Values (\\(V\\)). For the first word ‚ÄúI‚Äù, the calculation is:\n\\[\\text{Output}_{\\text{row1}} = (0.38 \\cdot V_{\\text{I}}) + (0.24 \\cdot V_{\\text{Love}}) + (0.38 \\cdot V_{\\text{AI}})\\]\n\\[= 0.38[10, 20] + 0.24[30, 40] + 0.38[50, 60]\\]\n\\[= [3.8, 7.6] + [7.2, 9.6] + [19, 22.8] = [30, 40]\\]\nThe word ‚ÄúI‚Äù started as vector [10, 20]. After attention, it became [30, 40]. It has absorbed context from the other words in the sentence."
  },
  {
    "objectID": "posts/scaled-dot-product-attention/index.html#handling-the-mask",
    "href": "posts/scaled-dot-product-attention/index.html#handling-the-mask",
    "title": "Scaled Dot-Product Attention: A Tensor-First Approach",
    "section": "Handling the Mask",
    "text": "Handling the Mask\nOne line we glossed over:\nscores = scores.masked_fill(mask == 0, -1e9)\nIn tasks like text generation, the model isn‚Äôt allowed to see the future. When predicting the 2nd word, it shouldn‚Äôt know what the 3rd word is.\nWe enforce this by applying a mask. We take the positions we want to hide and replace their scores with a massive negative number (like -1,000,000,000).\nWhy?\nBecause \\(e^{-1000000000}\\) is effectively zero. When we run Softmax, those words get 0% probability, effectively vanishing from the calculation."
  },
  {
    "objectID": "posts/scaled-dot-product-attention/index.html#summary",
    "href": "posts/scaled-dot-product-attention/index.html#summary",
    "title": "Scaled Dot-Product Attention: A Tensor-First Approach",
    "section": "Summary",
    "text": "Summary\nWhen you look at the ScaledDotProductAttention class now, try to see the story it tells:\n\nTranspose & Matmul: Create a grid showing how much every word relates to every other word.\nScale: Shrink the numbers so gradients don‚Äôt vanish.\nMask: Hide the words we aren‚Äôt allowed to see.\nSoftmax: Convert raw relationship scores into percentages.\nMatmul (Final): Create a new representation of the word that is a blend of all the relevant context it found.\n\nThe beauty of the Transformer isn‚Äôt just in its performance; it‚Äôs in this elegance of using simple linear algebra to model the complexity of language."
  },
  {
    "objectID": "posts/multi-head-attention/index.html",
    "href": "posts/multi-head-attention/index.html",
    "title": "Multi-Head Attention: A Tensor-First Walkthrough",
    "section": "",
    "text": "Published by Ramu Nalla - January 12, 2026\nIn my previous post, I have broken down the Scaled Dot-Product Attention mechanism. We saw how a Query finds similar Keys to extract Values.\nBut if you look at the actual Transformer architecture, you rarely see just ‚ÄúAttention.‚Äù You see Multi-Head Attention.\nWhy?\nImagine the sentence: ‚ÄúThe animal didn‚Äôt cross the street because it was too tired.‚Äù\nAs a human, you know ‚Äúit‚Äù refers to the ‚Äúanimal.‚Äù How do you know? 1. Grammatical perspective: You are looking for a noun that agrees with the pronoun. 2. Semantic perspective: You know that ‚Äústreets‚Äù don‚Äôt get ‚Äútired,‚Äù but ‚Äúanimals‚Äù do.\nIf a model only has a single attention head, it has to average these different types of relationships into one messy score. Multi-Head Attention allows the model to create distinct ‚Äúexperts.‚Äù Head 1 can focus on grammar, while Head 2 focuses on semantic context, running in parallel without interfering with each other.\nIn this post, I will walk through the standard PyTorch implementation of Multi-Head Attention. I will define a simple scenario and trace exactly what happens to the tensors at every step."
  },
  {
    "objectID": "posts/multi-head-attention/index.html#the-code-pytorch-multiheadattention",
    "href": "posts/multi-head-attention/index.html#the-code-pytorch-multiheadattention",
    "title": "Multi-Head Attention: A Tensor-First Walkthrough",
    "section": "The Code: PyTorch MultiHeadAttention",
    "text": "The Code: PyTorch MultiHeadAttention\nHere is the complete implementation I will be dissecting.\nimport torch\nimport torch.nn as nn\n# Assuming ScaledDotProductAttention is defined as in the previous post\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n        super(MultiHeadAttention, self).__init__()\n        \n        # Ensure the model dimension can be split evenly across heads\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n        \n        self.d_model = d_model\n        self.num_heads = num_heads\n        # d_k is the dimension of the vector *per head*\n        self.d_k = d_model // num_heads  \n        self.d_v = d_model // num_heads  \n        \n        # Linear projections for Q, K, V\n        # Note: We use single large projections instead of h separate ones\n        self.W_Q = nn.Linear(d_model, d_model)\n        self.W_K = nn.Linear(d_model, d_model)\n        self.W_V = nn.Linear(d_model, d_model)\n        \n        # Output projection (The Mixer)\n        self.W_O = nn.Linear(d_model, d_model)\n        \n        # Scaled dot-product attention mechanism\n        self.attention = ScaledDotProductAttention(self.d_k, dropout)\n        self.dropout = nn.Dropout(dropout)\n\n    def split_heads(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Split tensor into multiple heads and transpose for parallel processing\"\"\"\n        batch_size, seq_len, d_model = x.size()\n        \n        # Reshape: (B, Seq, d_model) -&gt; (B, Seq, H, d_k)\n        x = x.view(batch_size, seq_len, self.num_heads, self.d_k)\n        \n        # Transpose: (B, Seq, H, d_k) -&gt; (B, H, Seq, d_k)\n        # This puts Heads next to Batch for parallel attention computation\n        x = x.transpose(1, 2)\n        return x\n\n    def combine_heads(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Combine multiple heads back into a single continuous vector\"\"\"\n        # x shape: (B, H, Seq, d_k)\n        batch_size, num_heads, seq_len, d_k = x.size()\n        \n        # Transpose back: (B, H, Seq, d_k) -&gt; (B, Seq, H, d_k)\n        x = x.transpose(1, 2).contiguous()\n        \n        # Flatten: (B, Seq, H, d_k) -&gt; (B, Seq, d_model)\n        # H * d_k equals d_model again\n        x = x.view(batch_size, seq_len, self.d_model)\n        return x\n\n    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor,\n                mask: torch.Tensor = None):\n        batch_size = Q.size(0)\n        \n        # 1. Linear projections (Apply the \"lenses\")\n        # Shape remains: (B, Seq, d_model)\n        Q = self.W_Q(Q)  \n        K = self.W_K(K)  \n        V = self.W_V(V)  \n        \n        # 2. Split into multiple heads\n        # Shape becomes: (B, H, Seq, d_k)\n        Q = self.split_heads(Q)  \n        K = self.split_heads(K)  \n        V = self.split_heads(V)  \n        \n        # 3. Apply attention (in parallel across heads)\n        # Output shape: (B, H, Seq, d_v)\n        attn_output, attention_weights = self.attention(Q, K, V, mask)\n        \n        # 4. Combine heads (Stitch them back together)\n        # Shape becomes: (B, Seq, d_model)\n        attn_output = self.combine_heads(attn_output)\n        \n        # 5. Final linear projection (Mix the experts' opinions)\n        # Shape remains: (B, Seq, d_model)\n        output = self.W_O(attn_output)\n        output = self.dropout(output)\n        \n        return output, attention_weights"
  },
  {
    "objectID": "posts/multi-head-attention/index.html#the-tensor-walkthrough",
    "href": "posts/multi-head-attention/index.html#the-tensor-walkthrough",
    "title": "Multi-Head Attention: A Tensor-First Walkthrough",
    "section": "The Tensor Walkthrough",
    "text": "The Tensor Walkthrough\nTo truly understand Multi-Head Attention, we need to trace the shapes.\n\nOur Scenario\n\nWe are processing 1 sentence (Batch Size \\(B=1\\)).\nThe sentence has 3 words (Sequence Length \\(L=3\\), e.g., ‚ÄúI‚Äù, ‚ÄúLove‚Äù, ‚ÄúAI‚Äù).\nEach word is represented by a tiny vector of size 4 (\\(d_{model}=4\\)).\nWe want 2 parallel heads (\\(H=2\\)).\nTherefore, each head will operate on vectors of size 2 (\\(d_k = 4 / 2 = 2\\)).\n\n\n\nStep 1: The Inputs and Projections\nThe inputs \\(Q\\), \\(K\\), and \\(V\\) all start with the same shape.\n\nInput Shape: (1, 3, 4) \\(\\rightarrow\\) (Batch, Seq, \\(d_{model}\\))\n\nWe pass them through linear layers (\\(W_Q\\), \\(W_K\\), and \\(W_V\\)). These are essentially filters that transform the raw word embeddings into ‚ÄúQuery space,‚Äù ‚ÄúKey space,‚Äù and ‚ÄúValue space.‚Äù\n# 1. Linear projections\nQ = self.W_Q(Q)\nK = self.W_K(K)\nV = self.W_V(V)\n\nOutput Shape: (1, 3, 4)\n\nCrucially, the shape hasn‚Äôt changed yet. We have just transformed the data inside the vectors.\n\n\nStep 2: Splitting the Heads (‚ÄúThe Magic Trick‚Äù)\nThis is the most confusing part for beginners. How do we get multiple heads out of one vector?\nWe take the final dimension (\\(d_{model}=4\\)) and physically chop it into \\(H\\) chunks of size \\(d_k\\).\n# 2. Split into multiple heads\nQ = self.split_heads(Q)\n# ... K and V do the same ...\n1. View (Reshape): We tell the system to interpret the (..., 4) dimension as (..., 2, 2).\n\nShape changes from (1, 3, 4) to (1, 3, 2, 2) \\(\\rightarrow\\) (Batch, Seq, Heads, \\(d_k\\)).\n\n2. Transpose (Swap): We swap the ‚ÄúSequence‚Äù dimension (dim 1) with the ‚ÄúHeads‚Äù dimension (dim 2).\n\nShape changes from (1, 3, 2, 2) to (1, 2, 3, 2) \\(\\rightarrow\\) (Batch, Heads, Seq, \\(d_k\\)).\n\nWhy Transpose? Matrix multiplication operations typically operate on the last two dimensions. By moving ‚ÄúHeads‚Äù to the second position, the system treats (1, 2) as a ‚Äúsuper-batch.‚Äù It effectively tricks the GPU into running the attention mechanism on Head 1 and Head 2 simultaneously in parallel.\n\n\nStep 3: Apply Attention\nWe now pass these 4D tensors to the attention function. Because of the shape (1, 2, 3, 2), the attention module calculates dot products on the last two dimensions (3, 2).\n# 3. Apply attention\nattn_output, attention_weights = self.attention(Q, K, V, mask)\n\nHead 1 performs attention on its (3, 2) data.\nHead 2 performs attention on its (3, 2) data.\n\nOutput Shape: (1, 2, 3, 2). The results are still separated by head.\n\n\nStep 4: Combining Heads\nWe have the results, but they are split. We need to stitch them back together into a single representation for each word. We essentially reverse Step 2.\n# 4. Combine heads\nattn_output = self.combine_heads(attn_output)\n1. Transpose Back: Swap Heads and Seq dimensions.\n\nShape changes from (1, 2, 3, 2) to (1, 3, 2, 2).\n\n2. View (Flatten): Merge the ‚ÄúHeads‚Äù dimension (size 2) and ‚Äú\\(d_k\\)‚Äù dimension (size 2) back into ‚Äú\\(d_{model}\\)‚Äù (size 4).\n\nShape changes from (1, 3, 2, 2) to (1, 3, 4).\n\nEffectively, the vector of size 2 from Head 1 and the vector of size 2 from Head 2 are now concatenated side-by-side to make a vector of size 4.\n\n\nStep 5: The Final Mixer (\\(W_O\\))\nWe have a vector of size 4, but the top half is purely from Head 1, and the bottom half is purely from Head 2. They haven‚Äôt interacted yet.\n# 5. Final linear projection\noutput = self.W_O(attn_output)\nWe pass this through a final linear layer, \\(W_O\\) (Output weights). This layer mixes these features together. It allows the model to synthesize the ‚Äúgrammar insights‚Äù from Head 1 with the ‚Äúsemantic insights‚Äù from Head 2 into a single, unified representation for the word.\n\nFinal Output Shape: (1, 3, 4)\n\nWe started with (1, 3, 4) and ended with (1, 3, 4), but the vectors now contain rich, contextual information gathered from multiple perspectives across the whole sentence."
  },
  {
    "objectID": "posts/multi-head-attention/index.html#deep-dive-why-one-big-matrix",
    "href": "posts/multi-head-attention/index.html#deep-dive-why-one-big-matrix",
    "title": "Multi-Head Attention: A Tensor-First Walkthrough",
    "section": "Deep Dive: Why ‚ÄúOne Big Matrix‚Äù?",
    "text": "Deep Dive: Why ‚ÄúOne Big Matrix‚Äù?\nYou might wonder why we typically use one large linear layer (size \\(d_{model}\\)) instead of creating a list of smaller separate layers (size \\(d_k\\)).\nMathematically, they are identical. Computationally, one big matrix is much faster.\nGPUs love crunching massive matrices. It is significantly more efficient to perform one massive matrix multiplication (\\(512 \\times 512\\)) and then slice the result in memory, rather than launching 8 separate, smaller kernel operations (\\(512 \\times 64\\)) in a loop. This implementation is a standard trick to maximize hardware utilization."
  },
  {
    "objectID": "posts/multi-head-attention/index.html#summary",
    "href": "posts/multi-head-attention/index.html#summary",
    "title": "Multi-Head Attention: A Tensor-First Walkthrough",
    "section": "Summary",
    "text": "Summary\nMulti-Head Attention is a sophisticated way of saying ‚Äúdo parallel processing and mix the results.‚Äù\nBy splitting the embedding dimension into distinct heads, we allow the Transformer to learn different types of relationships simultaneously. The intricate steps of view and transpose operations is simply the mechanism required to arrange the data efficiently so GPUs can perform this parallel computation and then stitch the diverse insights back together."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ramu Nalla",
    "section": "",
    "text": "I am a Senior Data Scientist at HighLevel, where I focus on building and scaling Agentic AI systems using Large Language Models (LLMs). My work involves tackling complex NLP challenges at scale, specifically specializing in engineering production-grade RAG pipelines with tools like LangChain and LangGraph. My work also involves developing AI models to enhance SaaS business intelligence and establishing robust evaluation frameworks for LLM-based systems to drive measurable business impact.\nBefore HighLevel, I worked as a Senior ML Engineer at Enphase Energy for nearly five years, where I spearheaded chatbot initiatives and developed AI models to detect hardware-product failures.\nMy career has been primarily focused on applied machine learning, bridging the gap between innovation and real-world applications.\nI earned my B.Tech and M.Tech, along with an MBA minor, from IIT Kanpur.\nI enjoy blogging about Machine Learning, and I am currently working on a series of posts about NLP, MLOps, and Agentic AI architectures."
  },
  {
    "objectID": "index.html#latest-posts",
    "href": "index.html#latest-posts",
    "title": "Ramu Nalla",
    "section": "Latest Posts",
    "text": "Latest Posts\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        date - Oldest\n      \n      \n        date - Newest\n      \n      \n        title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\ndate\n\n\n\ntitle\n\n\n\n\n\n\n\n\nJan 12, 2026\n\n\nMulti-Head Attention: A Tensor-First Walkthrough\n\n\n\n\n\n\nJan 10, 2026\n\n\nScaled Dot-Product Attention: A Tensor-First Approach\n\n\n\n\n\n\nJan 1, 2026\n\n\nThe Definitive Guide to BLEU Score: The Mathematics of Machine Translation\n\n\n\n\n\n\nDec 17, 2025\n\n\nBuilding a Minimal RAG Pipeline from Scratch\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#news",
    "href": "index.html#news",
    "title": "Ramu Nalla",
    "section": "News",
    "text": "News\n\n\n\nDate\nUpdate\n\n\n\n\nOct 29, 2025\nJoined HighLevel as Sr.¬†Data Scientist\n\n\n\n\n\n\n    \n\n\nBest to reach out to me over email or DM me on LinkedIn.\n\n\n¬© 2026 Ramu Nalla. All rights reserved."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Download Full CV (PDF)"
  },
  {
    "objectID": "posts/2026-01-17-simple-rag-tutorial/index.html",
    "href": "posts/2026-01-17-simple-rag-tutorial/index.html",
    "title": "Building a Minimal RAG Pipeline from Scratch",
    "section": "",
    "text": "Published by Ramu Nalla - December 17, 2025\nIf you have worked with Large Language Models (LLMs) for more than five minutes, you have likely run into their biggest flaw: hallucinations. They sound confident, but they don‚Äôt know your private data, and their knowledge cutoff is always in the past.\nRetrieval-Augmented Generation (RAG) is the industry standard solution to this. It bridges the gap between a ‚Äúfrozen‚Äù LLM and your dynamic, private data.\nBefore I started using complex vector databases like Pinecone or Milvus, I wanted to understand exactly what was happening under the hood. So, I built a minimal RAG pipeline using nothing but Python and NumPy."
  },
  {
    "objectID": "posts/2026-01-17-simple-rag-tutorial/index.html#the-core-concept",
    "href": "posts/2026-01-17-simple-rag-tutorial/index.html#the-core-concept",
    "title": "Building a Minimal RAG Pipeline from Scratch",
    "section": "The Core Concept",
    "text": "The Core Concept\nRAG isn‚Äôt a single algorithm; it‚Äôs a workflow.\n\nRetrieval: Find the most relevant documents for a user‚Äôs question.\nAugmentation: Paste those documents into the LLM‚Äôs prompt context.\nGeneration: Ask the LLM to answer the question using only that context.\n\nThe ‚Äúmagic‚Äù happens in step 1. How does a computer know that ‚Äúneural network‚Äù and ‚Äúdeep learning‚Äù are related? The answer is Vector Embeddings."
  },
  {
    "objectID": "posts/2026-01-17-simple-rag-tutorial/index.html#the-code-vector-search-in-pure-python",
    "href": "posts/2026-01-17-simple-rag-tutorial/index.html#the-code-vector-search-in-pure-python",
    "title": "Building a Minimal RAG Pipeline from Scratch",
    "section": "The Code: ‚ÄúVector Search‚Äù in Pure Python",
    "text": "The Code: ‚ÄúVector Search‚Äù in Pure Python\nIn a real production environment, we use embedding models (like OpenAI‚Äôs text-embedding-3-small) to turn text into massive lists of numbers. For this demo, I‚Äôll manually create small 3-dimensional vectors to visualize the math.\nHere is the entire logic in one script:\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# 1. Our \"Knowledge Base\"\n# In reality, this would be thousands of PDF chunks stored in a Vector DB.\ndocuments = [\n    \"RAG bridges the gap between LLMs and private data.\",\n    \"LangChain is a popular framework for building AI agents.\",\n    \"Vector databases like Pinecone store embeddings for fast retrieval.\"\n]\n\n# 2. Mock Embeddings\n# Imagine these are the output of an embedding model.\n# Notice how Doc 1 (0.9, 0.1, ...) and Doc 3 (0.1, 0.2, ...) are mathematically different.\ndoc_vectors = np.array([\n    [0.9, 0.1, 0.1],  # Embedding for Document 1\n    [0.2, 0.8, 0.2],  # Embedding for Document 2\n    [0.1, 0.2, 0.9]   # Embedding for Document 3\n])\n\n# 3. The User's Query\n# \"How do I use private data with LLMs?\" \n# This query is semantically similar to Document 1.\nquery_vector = np.array([[0.85, 0.15, 0.1]]) \n\n# 4. The Retrieval Step (Cosine Similarity)\n# We calculate the angle between the Query and every Document.\n# Higher score = Closer match.\nscores = cosine_similarity(query_vector, doc_vectors).flatten()\n\n# 5. Get the Winner\nbest_match_idx = np.argmax(scores)\nretrieved_doc = documents[best_match_idx]\nconfidence = scores[best_match_idx]\n\nprint(f\"‚úÖ User Query Mapped to: '{retrieved_doc}'\")\nprint(f\"üìä Confidence Score: {confidence:.4f}\")\n\nWhat just happened?\nWhen we ran cosine_similarity, Python calculated the angle between our query vector and the document vectors.\n\nThe query vector [0.85, 0.15, 0.1] was heavily weighted towards the first dimension.\nDocument 1 [0.9, 0.1, 0.1] was also weighted towards the first dimension.\nTherefore, the math says: ‚ÄúThese two ideas are related.‚Äù"
  },
  {
    "objectID": "posts/2026-01-17-simple-rag-tutorial/index.html#moving-to-production",
    "href": "posts/2026-01-17-simple-rag-tutorial/index.html#moving-to-production",
    "title": "Building a Minimal RAG Pipeline from Scratch",
    "section": "Moving to Production",
    "text": "Moving to Production\nWhile this numpy example is great for intuition, it doesn‚Äôt scale to millions of rows. In my professional work, moving from this proof-of-concept to production involves:\n\nReal Embeddings: Replacing manual vectors with sentence-transformers or OpenAI embeddings.\nVector Store: Using a dedicated database (ChromaDB, Weaviate, or Snowflake) to index millions of vectors.\nReranking: Adding a second step to double-check the relevance of the retrieved documents.\n\nI‚Äôll be writing more about building Agentic workflows on top of this RAG architecture in future posts. Stay tuned!"
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html",
    "title": "The Definitive Guide to BLEU Score: The Mathematics of Machine Translation",
    "section": "",
    "text": "Published by Ramu Nalla - January 01, 2026\nIn the world of supervised learning‚Äîlike spam classification or customer churn prediction‚Äîevaluation is straightforward. The model is right, or it‚Äôs wrong. Accuracy, precision, and recall tell us everything we need to know.\nBut what happens when you are evaluating a machine translation model?\nIf the human reference translates a French sentence as ‚ÄúThe cat sat on the mat,‚Äù but your model outputs ‚ÄúA cat was sitting on the mat,‚Äù is the model wrong? No.¬†It conveys the same meaning. It just uses different words.\nLanguage is inherently ambiguous. There is rarely one single ‚Äúgold standard‚Äù translation. This subjectivity makes automated evaluation of Generative AI incredibly difficult.\nFor two decades, the de facto standard for solving this problem in Machine Translation (MT) has been the BLEU (Bilingual Evaluation Understudy) score. While newer semantic metrics like BERTScore are gaining traction, understanding BLEU is non-negotiable for any serious NLP practitioner. It is the bedrock upon which modern metrics are built.\nIn this post, we are going to tear apart the BLEU score. We won‚Äôt just look at the final formula; we will build it piece by piece to understand the mathematical intuition behind why it works‚Äîand crucial for us data scientists‚Äîwhere it breaks."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#the-core-intuition-precision-vs.-recall",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#the-core-intuition-precision-vs.-recall",
    "title": "The Definitive Guide to BLEU Score: The Mathematics of Machine Translation",
    "section": "The Core Intuition: Precision vs.¬†Recall",
    "text": "The Core Intuition: Precision vs.¬†Recall\nWhen evaluating generated text against a reference, we have two fundamental choices:\n\nPrecision: How much of what the model generated was correct?\nRecall: How much of the reference did the model capture?\n\nBLEU is fundamentally a Precision-based metric.\nWhy? Imagine a translation task where the goal is to translate a complex German technical document.\n\nA high-recall model might output a messy 5-page document that definitely contains all the right information, but buries it in hallucinations and noise.\nA high-precision model might output a shorter, 3-page document. It might miss a nuance or two, but everything it did generate is accurate and fluent.\n\nIn translation, we generally prefer the latter. We want the generated text to be high quality, even if it‚Äôs slightly incomplete."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#step-1-the-trap-of-raw-precision",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#step-1-the-trap-of-raw-precision",
    "title": "The Definitive Guide to BLEU Score: The Mathematics of Machine Translation",
    "section": "Step 1: The Trap of Raw Precision",
    "text": "Step 1: The Trap of Raw Precision\nLet‚Äôs start with the simplest possible approach: counting word matches.\n\nReference 1: ‚ÄúThe cat is on the mat.‚Äù\nReference 2: ‚ÄúThere is a cat on the mat.‚Äù\n\nNow, imagine a poorly trained model generates this candidate:\n\nCandidate: ‚Äúthe the the the the the‚Äù\n\nIf we calculate standard unigram (1-word) precision:\n\nTotal candidate words: 6\nNumber of matches: 6 (The word ‚Äúthe‚Äù appears in the references).\nPrecision: 6/6 = 100%\n\nThis is obviously a catastrophic failure. A model could game the system by finding the most common word in the reference language and repeating it endlessly."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#step-2-the-fix-modified-clipped-precision",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#step-2-the-fix-modified-clipped-precision",
    "title": "The Definitive Guide to BLEU Score: The Mathematics of Machine Translation",
    "section": "Step 2: The Fix ‚Äî Modified (Clipped) Precision",
    "text": "Step 2: The Fix ‚Äî Modified (Clipped) Precision\n\n\n\nA visual representation of how BLEU calculates ‚ÄúClipped Precision,‚Äù mapping candidate words to reference constraints.\n\n\nTo solve the repetition problem, BLEU introduces clipped precision.\nThe rule is simple: A word in the candidate sentence can only be counted as a match up to the maximum number of times it appears in a single reference sentence.\nLet‚Äôs re-evaluate our bad candidate:\n\nCandidate: ‚Äúthe the the the the the‚Äù\nReference Constraint: The word ‚Äúthe‚Äù appears a maximum of two times (in Reference 1).\n\nThe math changes:\n\nTotal candidate words: 6\nClipped matches: 2 (We count the first two ‚Äúthe‚Äùs, and discard the remaining four).\nClipped Precision: 2/6 = 33.3%\n\nThis is much more reasonable. The metric now punishes absurd repetition.\nAs shown in the image at the top of this post, think of the reference sentences as providing a limited ‚Äúbudget‚Äù for each word count. Once the candidate uses up that budget, subsequent uses of that word are worthless."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#step-3-fluency-and-n-grams",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#step-3-fluency-and-n-grams",
    "title": "The Definitive Guide to BLEU Score: The Mathematics of Machine Translation",
    "section": "Step 3: Fluency and N-Grams",
    "text": "Step 3: Fluency and N-Grams\nMatching individual words is good for ensuring the content is roughly adequate, but it doesn‚Äôt ensure the sentence flows naturally.\n‚ÄúMat on is the cat the‚Äù contains all the right words, but it‚Äôs terrible English.\nBLEU solves this by calculating clipped precision not just for unigrams (1-grams), but also for bigrams (2-grams), trigrams (3-grams), and typically up to 4-grams.\n\n1-grams measure adequacy (are the right concepts there?).\n3-grams and 4-grams measure fluency (is the phrasing natural?).\n\nIf a model gets high precision on 4-grams, it means it‚Äôs getting long sequences of words exactly right, which usually correlates with high-quality, fluent text.\n\nCombining the Scores\nSo, we have four separate precision scores (\\(p_1, p_2, p_3, p_4\\)). How do we combine them into a single number?\nWe don‚Äôt take the arithmetic mean (an average). We take the geometric mean.\n\n\nWhy?\nThe geometric mean is highly sensitive to low scores. If a translation has excellent unigram precision (all the right words) but zero 4-gram precision (the order is completely scrambled), the geometric mean will crash toward zero. The arithmetic mean would still give it a decent pass. We want the metric to be harsh on models that fail at any level of granularity.\nThe combined precision score looks like this:\n\\[\\exp\\left( \\sum_{n=1}^{N} w_n \\log p_n \\right)\\]\nTypically, \\(N=4\\) and the weights \\(w_n\\) are uniform (\\(1/4\\))."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#step-4-the-brevity-penalty-preventing-the-gaming",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#step-4-the-brevity-penalty-preventing-the-gaming",
    "title": "The Definitive Guide to BLEU Score: The Mathematics of Machine Translation",
    "section": "Step 4: The Brevity Penalty (Preventing the Gaming)",
    "text": "Step 4: The Brevity Penalty (Preventing the Gaming)\nWe established that BLEU is a precision metric. But pure precision metrics have a massive loophole: Length.\nIf I have to translate a 50-word sentence, I could output a single, perfect 3-word phrase that I know is correct. My precision would be 100%. But I failed to translate the vast majority of the source sentence.\nTo prevent models from outputting overly short, ‚Äúsafe‚Äù sentences to maximize precision, BLEU applies a Brevity Penalty (BP).\nThe penalty is calculated based on two variables: 1. \\(c\\): The length of the candidate translation. 2. \\(r\\): The effective reference length.\nThe formula for BP is:\n\\[\nBP = \\begin{cases}\n1 & \\text{if } c &gt; r \\\\\ne^{(1 - r/c)} & \\text{if } c \\le r\n\\end{cases}\n\\]\nIf the candidate is longer than the reference (\\(c &gt; r\\)), there is no penalty (\\(BP = 1\\)). We don‚Äôt punish the model for being verbose (that‚Äôs what the precision score already does if the extra words are wrong).\nIf the candidate is shorter than the reference (\\(c \\le r\\)), the penalty kicks in exponentially.\nLet‚Äôs visualize this.\n\n\n\nA graph showing the BLEU Brevity Penalty. The Y-axis is the penalty multiplier (0 to 1), and the X-axis is the ratio of candidate length to reference length (c/r). The curve drops sharply below 1.0.\n\n\nAs you can see in the graph above, as soon as the candidate length drops below the reference length (ratio &lt; 1.0), the score multiplier starts dropping from 1.0 toward 0.0. A candidate that is half the length of the reference gets heavily penalized, ensuring that high-BLEU systems must produce output of comparable length to humans."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#the-complete-formula",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#the-complete-formula",
    "title": "The Definitive Guide to BLEU Score: The Mathematics of Machine Translation",
    "section": "The Complete Formula",
    "text": "The Complete Formula\nPutting it all together, the BLEU score is the geometric mean of the n-gram clipped precisions, multiplied by the brevity penalty.\n\\[\\textbf{BLEU} = \\underbrace{BP}_{\\text{Brevity Penalty}} \\cdot \\underbrace{\\exp\\left( \\sum_{n=1}^{4} w_n \\log p_n \\right)}_{\\text{Geometric Mean of N-Gram Precisions}}\\]\nThe final score is between 0 and 1, though in practice, it is almost always reported as a percentage between 0 and 100.\nA BLEU score over 30 is generally considered understandable. A score over 40-50 is often considered high quality, depending on the language pair (translating English to French is easier than English to Chinese, so expected scores vary)."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#limitations",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#limitations",
    "title": "The Definitive Guide to BLEU Score: The Mathematics of Machine Translation",
    "section": "Limitations",
    "text": "Limitations\nWe‚Äôve covered how BLEU works. Now, let‚Äôs talk about why it‚Äôs often frustrating. As data scientists, we need to know the limitations of our tools.\n1. The Synonym Problem (Semantic Blindness). BLEU relies on exact string matching. It has no concept of meaning.\n\nReference: ‚ÄúThe path is steep.‚Äù\nCandidate: ‚ÄúThe trail is uphill.‚Äù\n\nBLEU will give this a very low score, despite it being a perfect translation. It punishes creative or varied vocabulary. This is why semantic embeddings-based metrics like BERTScore were invented.\n2. It Ignores Global Structure While n-grams capture local ordering, BLEU has no sense of overall sentence structure. ‚ÄúCat the sat mat on the‚Äù might get decent unigram scores, and it wouldn‚Äôt necessarily get a zero if some bigrams accidentally align.\n3. Human Correlation is Wobbly Crucially, an increase in BLEU score does not always correspond to an increase in human-perceived quality. A model might game the metric by optimizing for 4-grams in a way that sounds mechanical to a human reader. BLEU is useful for tracking progress during training, but the final sanity check must always be human evaluation.\n4. The Preprocessing Nightmare (sacrebleu) Because BLEU is based on exact string matching, it is incredibly sensitive to preprocessing. Do you tokenize text? Do you lowercase everything? How do you handle punctuation? Different implementations used to yield vastly different scores for the same model output.\nIndustry Standard: If you are reporting BLEU scores in a paper or a serious report, never roll your own implementation. Always use the sacrebleu Python library. It standardizes tokenization and preprocessing to ensure your scores are actually comparable to other people‚Äôs scores."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#final-thoughts",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#final-thoughts",
    "title": "The Definitive Guide to BLEU Score: The Mathematics of Machine Translation",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nBLEU is imperfect. It is rigid, semantically blind, and sometimes fails to match human judgment.\nHowever, it remains widely adopted in machine translation evaluation. It is fast to compute, easy to understand, and language-agnostic. Understanding the mechanics of clipped precision and the brevity penalty gives you the intuition needed to interpret these scores correctly‚Äîand to know when it‚Äôs time to move on to something more advanced."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blogs",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nMulti-Head Attention: A Tensor-First Walkthrough\n\n10 min\n\n\nDeep Learning\n\nTransformers\n\nPyTorch\n\nNLP\n\n\n\nWhy one attention head isn‚Äôt enough. I will take a deep dive into the PyTorch implementation of Multi-Head Attention, tracing tensor shapes to understand how models gain‚Ä¶\n\n\n\nJan 12, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nScaled Dot-Product Attention: A Tensor-First Approach\n\n7 min\n\n\nDeep Learning\n\nTransformers\n\nPyTorch\n\nNLP\n\n\n\nConfused by Query, Key, and Value? I break down the heart of the Transformer architecture with a line-by-line PyTorch walkthrough and a concrete mathematical trace.\n\n\n\nJan 10, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Definitive Guide to BLEU Score: The Mathematics of Machine Translation\n\n8 min\n\n\nNLP\n\nGenAI\n\nMetrics\n\nMachine Translation\n\n\n\nA deep dive for data scientists into the most famous, and often misunderstood, metric in NLP. We unpack the math of n-grams, clipped precision, and the brevity penalty.\n\n\n\nJan 1, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Minimal RAG Pipeline from Scratch\n\n3 min\n\n\nNLP\n\nLLMs\n\nRAG\n\nPython\n\n\n\nA look under the hood of Retrieval-Augmented Generation. We build a semantic search engine using just NumPy to understand the core math behind the magic.\n\n\n\nDec 17, 2025\n\n\n\n\n\n\nNo matching items"
  }
]