[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "My First Blog Post",
    "section": "",
    "text": "Welcome to my new Data Science blog!"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Download Full CV (PDF)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ramu Nalla",
    "section": "",
    "text": "I am a Senior Data Scientist at HighLevel, where I focus on building and scaling Agentic AI systems using Large Language Models (LLMs). My work involves tackling complex NLP challenges at scale, specifically specializing in engineering production-grade RAG pipelines with tools like LangChain and LangGraph. My work also involves developing AI models to enhance SaaS business intelligence and establishing robust evaluation frameworks for LLM-based systems to drive measurable business impact.\nBefore HighLevel, I worked as a Senior ML Engineer at Enphase Energy for nearly five years, where I spearheaded chatbot initiatives and developed AI models to detect hardware-product failures.\nMy career has been primarily focused on applied machine learning, bridging the gap between innovation and real-world applications.\nI earned my B.Tech and M.Tech, along with an MBA minor, from IIT Kanpur.\nI enjoy blogging about Machine Learning, and I am currently working on a series of posts about NLP, MLOps, and Agentic AI architectures."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blogs",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nScaled Dot-Product Attention: A Tensor-First Approach\n\n7 min\n\n\nDeep Learning\n\nTransformers\n\nPyTorch\n\nNLP\n\n\n\nConfused by Query, Key, and Value? I break down the heart of the Transformer architecture with a line-by-line PyTorch walkthrough and a concrete mathematical trace.\n\n\n\nJan 10, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Definitive Guide to BLEU Score: The Mathematics of Machine Translation\n\n8 min\n\n\nNLP\n\nGenAI\n\nMetrics\n\nMachine Translation\n\n\n\nA deep dive for data scientists into the most famous, and often misunderstood, metric in NLP. We unpack the math of n-grams, clipped precision, and the brevity penalty.\n\n\n\nJan 1, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Minimal RAG Pipeline from Scratch\n\n3 min\n\n\nNLP\n\nLLMs\n\nRAG\n\nPython\n\n\n\nA look under the hood of Retrieval-Augmented Generation. We build a semantic search engine using just NumPy to understand the core math behind the magic.\n\n\n\nDec 17, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#latest-thinking",
    "href": "index.html#latest-thinking",
    "title": "Ramu Nalla",
    "section": "Latest Thinking",
    "text": "Latest Thinking\n\n\n\n\n\nMy First Blog Post\n\n\n\n\n\n\n\n\nMay 20, 2024\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#updates",
    "href": "index.html#updates",
    "title": "Ramu Nalla",
    "section": "Updates",
    "text": "Updates\n\n\n\nDate\nEvent\n\n\n\n\nJan 17, 2026\nüöÄ Updated portfolio architecture\n\n\nJan 15, 2026\nüéâ Launched personal website\n\n\n\n\n\n\n‚úâÔ∏è üêô üíº ùïè\n\n\n¬© 2026 Ramu Nalla. Built with Quarto & Python."
  },
  {
    "objectID": "index.html#latest-posts",
    "href": "index.html#latest-posts",
    "title": "Ramu Nalla",
    "section": "Latest Posts",
    "text": "Latest Posts\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        date - Oldest\n      \n      \n        date - Newest\n      \n      \n        title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\ndate\n\n\n\ntitle\n\n\n\n\n\n\n\n\nJan 19, 2026\n\n\nThe Definitive Guide to BLEU Score: The Mathematics of Machine Translation\n\n\n\n\n\n\nJan 12, 2026\n\n\nMulti-Head Attention: A Tensor-First Walkthrough\n\n\n\n\n\n\nJan 10, 2026\n\n\nScaled Dot-Product Attention: A Tensor-First Approach\n\n\n\n\n\n\nDec 17, 2025\n\n\nBuilding a Minimal RAG Pipeline from Scratch\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#news",
    "href": "index.html#news",
    "title": "Ramu Nalla",
    "section": "News",
    "text": "News\n\n\n\nDate\nUpdate\n\n\n\n\nOct 29, 2025\nJoined HighLevel as Sr.¬†Data Scientist\n\n\n\n\n\n\n    \n\n\nBest to reach out to me over email or DM me on LinkedIn.\n\n\n¬© 2026 Ramu Nalla. All rights reserved."
  },
  {
    "objectID": "posts/2026-01-17-simple-rag-tutorial/index.html",
    "href": "posts/2026-01-17-simple-rag-tutorial/index.html",
    "title": "Building a Minimal RAG Pipeline from Scratch",
    "section": "",
    "text": "Published by Ramu Nalla - December 17, 2025\nIf you have worked with Large Language Models (LLMs) for more than five minutes, you have likely run into their biggest flaw: hallucinations. They sound confident, but they don‚Äôt know your private data, and their knowledge cutoff is always in the past.\nRetrieval-Augmented Generation (RAG) is the industry standard solution to this. It bridges the gap between a ‚Äúfrozen‚Äù LLM and your dynamic, private data.\nBefore I started using complex vector databases like Pinecone or Milvus, I wanted to understand exactly what was happening under the hood. So, I built a minimal RAG pipeline using nothing but Python and NumPy."
  },
  {
    "objectID": "posts/2026-01-17-simple-rag-tutorial/index.html#the-retrieval-mechanism",
    "href": "posts/2026-01-17-simple-rag-tutorial/index.html#the-retrieval-mechanism",
    "title": "Building a Minimal RAG Pipeline",
    "section": "",
    "text": "The most critical part of RAG is semantic search. We calculate the cosine similarity between the user‚Äôs query vector and our document vectors.\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# 1. Dummy Knowledge Base\ndocuments = [\n    \"RAG improves LLM accuracy by providing external context.\",\n    \"LangChain is a framework for building applications with LLMs.\",\n    \"Vector databases store embeddings for efficient semantic search.\"\n]\n\n# 2. Mock Embeddings (3-dimensional vectors for demo)\ndoc_vectors = np.array([\n    [0.9, 0.1, 0.1], \n    [0.2, 0.8, 0.2], \n    [0.1, 0.2, 0.9]\n])\n\n# 3. User Query Vector\nquery_vector = np.array([[0.85, 0.15, 0.1]]) \n\n# 4. Calculate Similarity\nscores = cosine_similarity(query_vector, doc_vectors).flatten()\n\n# 5. Retrieve Top Result\nbest_match_idx = np.argmax(scores)\nprint(f\"Query matched: '{documents[best_match_idx]}'\")\nprint(f\"Confidence Score: {scores[best_match_idx]:.4f}\")"
  },
  {
    "objectID": "posts/2026-01-17-simple-rag-tutorial/index.html#conclusion",
    "href": "posts/2026-01-17-simple-rag-tutorial/index.html#conclusion",
    "title": "Building a Minimal RAG Pipeline",
    "section": "",
    "text": "This is a simplified view of how retrieval works. In a production environment, we would scale this using tools like Pinecone or Milvus."
  },
  {
    "objectID": "posts/2026-01-17-simple-rag-tutorial/index.html#the-core-concept",
    "href": "posts/2026-01-17-simple-rag-tutorial/index.html#the-core-concept",
    "title": "Building a Minimal RAG Pipeline from Scratch",
    "section": "The Core Concept",
    "text": "The Core Concept\nRAG isn‚Äôt a single algorithm; it‚Äôs a workflow.\n\nRetrieval: Find the most relevant documents for a user‚Äôs question.\nAugmentation: Paste those documents into the LLM‚Äôs prompt context.\nGeneration: Ask the LLM to answer the question using only that context.\n\nThe ‚Äúmagic‚Äù happens in step 1. How does a computer know that ‚Äúneural network‚Äù and ‚Äúdeep learning‚Äù are related? The answer is Vector Embeddings."
  },
  {
    "objectID": "posts/2026-01-17-simple-rag-tutorial/index.html#the-code-vector-search-in-pure-python",
    "href": "posts/2026-01-17-simple-rag-tutorial/index.html#the-code-vector-search-in-pure-python",
    "title": "Building a Minimal RAG Pipeline from Scratch",
    "section": "The Code: ‚ÄúVector Search‚Äù in Pure Python",
    "text": "The Code: ‚ÄúVector Search‚Äù in Pure Python\nIn a real production environment, we use embedding models (like OpenAI‚Äôs text-embedding-3-small) to turn text into massive lists of numbers. For this demo, I‚Äôll manually create small 3-dimensional vectors to visualize the math.\nHere is the entire logic in one script:\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# 1. Our \"Knowledge Base\"\n# In reality, this would be thousands of PDF chunks stored in a Vector DB.\ndocuments = [\n    \"RAG bridges the gap between LLMs and private data.\",\n    \"LangChain is a popular framework for building AI agents.\",\n    \"Vector databases like Pinecone store embeddings for fast retrieval.\"\n]\n\n# 2. Mock Embeddings\n# Imagine these are the output of an embedding model.\n# Notice how Doc 1 (0.9, 0.1, ...) and Doc 3 (0.1, 0.2, ...) are mathematically different.\ndoc_vectors = np.array([\n    [0.9, 0.1, 0.1],  # Embedding for Document 1\n    [0.2, 0.8, 0.2],  # Embedding for Document 2\n    [0.1, 0.2, 0.9]   # Embedding for Document 3\n])\n\n# 3. The User's Query\n# \"How do I use private data with LLMs?\" \n# This query is semantically similar to Document 1.\nquery_vector = np.array([[0.85, 0.15, 0.1]]) \n\n# 4. The Retrieval Step (Cosine Similarity)\n# We calculate the angle between the Query and every Document.\n# Higher score = Closer match.\nscores = cosine_similarity(query_vector, doc_vectors).flatten()\n\n# 5. Get the Winner\nbest_match_idx = np.argmax(scores)\nretrieved_doc = documents[best_match_idx]\nconfidence = scores[best_match_idx]\n\nprint(f\"‚úÖ User Query Mapped to: '{retrieved_doc}'\")\nprint(f\"üìä Confidence Score: {confidence:.4f}\")\n\nWhat just happened?\nWhen we ran cosine_similarity, Python calculated the angle between our query vector and the document vectors.\n\nThe query vector [0.85, 0.15, 0.1] was heavily weighted towards the first dimension.\nDocument 1 [0.9, 0.1, 0.1] was also weighted towards the first dimension.\nTherefore, the math says: ‚ÄúThese two ideas are related.‚Äù"
  },
  {
    "objectID": "posts/2026-01-17-simple-rag-tutorial/index.html#moving-to-production",
    "href": "posts/2026-01-17-simple-rag-tutorial/index.html#moving-to-production",
    "title": "Building a Minimal RAG Pipeline from Scratch",
    "section": "Moving to Production",
    "text": "Moving to Production\nWhile this numpy example is great for intuition, it doesn‚Äôt scale to millions of rows. In my professional work, moving from this proof-of-concept to production involves:\n\nReal Embeddings: Replacing manual vectors with sentence-transformers or OpenAI embeddings.\nVector Store: Using a dedicated database (ChromaDB, Weaviate, or Snowflake) to index millions of vectors.\nReranking: Adding a second step to double-check the relevance of the retrieved documents.\n\nI‚Äôll be writing more about building Agentic workflows on top of this RAG architecture in future posts. Stay tuned!"
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html",
    "title": "The Definitive Guide to BLEU Score: The Mathematics of Machine Translation",
    "section": "",
    "text": "Published by Ramu Nalla - January 01, 2026\nIn the world of supervised learning‚Äîlike spam classification or customer churn prediction‚Äîevaluation is straightforward. The model is right, or it‚Äôs wrong. Accuracy, precision, and recall tell us everything we need to know.\nBut what happens when you are evaluating a machine translation model?\nIf the human reference translates a French sentence as ‚ÄúThe cat sat on the mat,‚Äù but your model outputs ‚ÄúA cat was sitting on the mat,‚Äù is the model wrong? No.¬†It conveys the same meaning. It just uses different words.\nLanguage is inherently ambiguous. There is rarely one single ‚Äúgold standard‚Äù translation. This subjectivity makes automated evaluation of Generative AI incredibly difficult.\nFor two decades, the de facto standard for solving this problem in Machine Translation (MT) has been the BLEU (Bilingual Evaluation Understudy) score. While newer semantic metrics like BERTScore are gaining traction, understanding BLEU is non-negotiable for any serious NLP practitioner. It is the bedrock upon which modern metrics are built.\nIn this post, we are going to tear apart the BLEU score. We won‚Äôt just look at the final formula; we will build it piece by piece to understand the mathematical intuition behind why it works‚Äîand crucial for us data scientists‚Äîwhere it breaks."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#the-n-gram-era-measuring-overlap",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#the-n-gram-era-measuring-overlap",
    "title": "Beyond Accuracy: A Deep Dive into NLP Evaluation Metrics",
    "section": "The N-Gram Era: Measuring Overlap",
    "text": "The N-Gram Era: Measuring Overlap\nBefore deep learning took over, our best bet for evaluating text was counting how many words (n-grams) in the model‚Äôs output matched the words in a human-written reference. These metrics are fast, interpretable, and still widely used benchmarks today.\n\nBLEU (Bilingual Evaluation Understudy)\nThe Purpose: BLEU was one of the first metrics to achieve a high correlation with human judgments in machine translation. Its primary focus is Precision.\nThe Mechanism: BLEU asks the question: ‚ÄúHow much of the generated summary appeared in the reference summary?‚Äù\nIt calculates the n-gram overlap (usually up to 4-grams) between the candidate text and one or more reference texts. Crucially, it includes a Brevity Penalty. Without it, a model could game the metric by outputting a single correct word (achieving 100% precision) while ignoring the rest of the sentence.\nThe Value Range: 0 to 1 (or 0 to 100). Higher is better. A score over 30-40 is often considered very good for translation tasks.\nA Detailed Example:\nLet‚Äôs look at how 1-gram (unigram) precision is calculated.\n\nReference (Human): ‚ÄúThe cat is on the mat.‚Äù\nCandidate (Model): ‚ÄúThe the the cat mat.‚Äù\n\nIf we just count matches, the model gets 5 matches (‚ÄúThe‚Äù, ‚Äúthe‚Äù, ‚Äúthe‚Äù, ‚Äúcat‚Äù, ‚Äúmat‚Äù) out of 5 generated words. That‚Äôs 100% precision. This is obviously wrong.\nBLEU uses clipped precision. It caps the count of each word by the maximum number of times that word appears in the reference. * ‚Äúthe‚Äù appears a maximum of 2 times in the reference. * ‚Äúcat‚Äù appears 1 time. * ‚Äúmat‚Äù appears 1 time.\nClipped matches: 2 (‚Äúthe‚Äù) + 1 (‚Äúcat‚Äù) + 1 (‚Äúmat‚Äù) = 4. Total candidate words: 5. BLEU-1 Precision: 4 / 5 = 0.8\nReal-world BLEU usually combines BLEU-1, -2, -3, and -4 into a geometric mean and applies the brevity penalty.\nIndustry Variants: * BLEU-4: The standard configuration using cumulative 4-gram scores. * SacreBLEU: A standardized Python library that solves the issue of different preprocessing steps yielding different BLEU scores. If you are reporting academic results, use SacreBLEU.\nWhere it is used: Machine Translation (MT) remains its stronghold. It is rarely the best choice for creative generation or summarization.\n\n\n\nROUGE (Recall-Oriented Understudy for Gisting Evaluation)\nThe Purpose: While BLEU focuses on precision, ROUGE focuses on Recall. It was designed specifically for evaluating text summarization.\nThe Mechanism: ROUGE asks the question: ‚ÄúHow much of the reference summary did the generated summary capture?‚Äù\nIt measures n-gram overlap from the perspective of the reference text.\nThe Value Range: 0 to 1. Higher is better.\nA Detailed Example:\n\nReference: ‚ÄúClimate change is causing extreme weather events globally.‚Äù\nCandidate: ‚ÄúExtreme weather is happening.‚Äù\n\nLet‚Äôs calculate ROUGE-1 (Unigram) Recall: * Number of overlapping words: 3 (‚Äúextreme‚Äù, ‚Äúweather‚Äù, ‚Äúis‚Äù) * Total words in Reference: 8 * ROUGE-1 Recall: 3 / 8 = 0.375\nThe model missed ‚ÄúClimate change‚Äù, ‚Äúcausing‚Äù, ‚Äúevents‚Äù, and ‚Äúglobally‚Äù. A low recall score reflects this information loss.\nIndustry Variants: * ROUGE-N: Measures n-gram overlap (e.g., ROUGE-1, ROUGE-2). ROUGE-2 is a very popular metric for summarization as it captures basic phrasing. * ROUGE-L (Longest Common Subsequence): This is highly valuable because it doesn‚Äôt require consecutive matches but does require the words to appear in the same relative order. It rewards structure without being overly rigid on exact phrasing.\nWhere it is used: Text Summarization is its primary domain. It is occasionally used in question answering."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#the-embedding-era-measuring-meaning",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#the-embedding-era-measuring-meaning",
    "title": "Beyond Accuracy: A Deep Dive into NLP Evaluation Metrics",
    "section": "The Embedding Era: Measuring Meaning",
    "text": "The Embedding Era: Measuring Meaning\nThe fundamental flaw of n-gram metrics like BLEU and ROUGE is that they are semantically blind. They don‚Äôt know that ‚Äúcar‚Äù and ‚Äúautomobile‚Äù mean the same thing. If the reference uses one and the model uses the other, n-gram metrics score it as a zero-match error.\nAs NLP moved toward embeddings (Word2Vec, BERT), evaluation metrics followed suit.\n\nBERTScore\nThe Purpose: BERTScore aims to evaluate semantic similarity rather than exact lexical overlap. It leverages pre-trained contextual embeddings (like BERT, RoBERTa, or DeBERTa) to understand if two sentences mean the same thing, even if they use different words.\nThe Mechanism: 1. Generate contextual embeddings for every token in the candidate sentence and the reference sentence using a Transformer model. 2. Calculate the cosine similarity between every candidate token and every reference token to create a similarity matrix. 3. Use ‚Äúgreedy matching‚Äù to find the highest similarity score for each token. 4. Compute the average of these maximum similarity scores to get Precision, Recall, and F1.\nThe Value Range: Generally -1 to 1 (due to cosine similarity), but practically ranges from 0 to 1. It often occupies a narrower, higher range than BLEU; a score change of 0.02 can be significant.\nA Detailed Example:\n\nReference: ‚ÄúThe weather is chilly.‚Äù\nCandidate: ‚ÄúIt‚Äôs cold outside.‚Äù\n\nBLEU and ROUGE would give this a near-zero score as there is almost no word overlap.\nBERTScore, however, knows that the embedding vector for ‚Äúchilly‚Äù in this context is highly similar to the vector for ‚Äúcold‚Äù. It also recognizes the semantic relationship between ‚Äúweather‚Äù and ‚Äúoutside‚Äù. It will assign a high similarity score, correctly identifying that the model preserved the meaning.\nKey Advantages: * Synonym Awareness: Handles paraphrasing effectively. * Contextual: The embedding for ‚Äúbank‚Äù changes depending on whether you mean a river bank or a financial institution.\nWhere it is used: Almost universally applicable across text generation tasks, including translation, summarization, and dialogue systems. It generally correlates much better with human judgment than n-gram metrics."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#the-frontier-measuring-distribution",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#the-frontier-measuring-distribution",
    "title": "Beyond Accuracy: A Deep Dive into NLP Evaluation Metrics",
    "section": "The Frontier: Measuring Distribution",
    "text": "The Frontier: Measuring Distribution\nThe metrics above compare one generated sample against one (or a few) references. But what about open-ended generation, like creative writing or dialogue? There are thousands of valid ways to continue a story. Comparing a model‚Äôs output to just one single human example is often unfair and misleading.\nThis brings us to distributional metrics, which compare the entire probability distribution of model-generated text against human text.\n\nMAUVE (Open-Ended Text Generation)\nThe Purpose: MAUVE is designed for open-ended generation tasks where diversity and creativity matter as much as correctness. It attempts to measure the ‚Äúgap‚Äù between the distribution of human text and machine text.\nIt specifically addresses two common failures in modern LLMs: 1. Type I Error (Quality issues): The model generates text that is unrealistic or degenerate (gibberish, repetition). 2. Type II Error (Diversity issues): The model generates safe, repetitive, ‚Äúboring‚Äù text and fails to capture the full richness of human expression (mode collapse).\nThe Mechanism (Simplified Intuition): Imagine all possible human text exists in a ‚Äúprobability space‚Äù shaped like a cloud. The model also has its own cloud.\nMAUVE uses embeddings (usually from GPT-2 or similar) to map generated text and human text into this space. It then uses statistical divergence measures (related to Kullback-Leibler divergence) to quantify how much the ‚Äúmodel cloud‚Äù overlaps with the ‚Äúhuman cloud.‚Äù\nIt provides a single score that represents a trade-off between measuring how much of the model text looks human (quality) and how much of the human distribution the model manages to cover (diversity).\nThe Value Range: 0 to 1. Higher is better. A low score indicates the model is either generating low-quality text or is too repetitive and ‚Äúsafe.‚Äù\nWhere it is used: * Open-ended story generation. * Creative writing assistants. * Dialogue systems (chatbots) where engaging, diverse responses are required."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#summary-comparison-table",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#summary-comparison-table",
    "title": "Beyond Accuracy: A Deep Dive into NLP Evaluation Metrics",
    "section": "Summary Comparison Table",
    "text": "Summary Comparison Table\n\n\n\n\n\n\n\n\n\n\nMetric\nCore Focus\nMechanism\nBest Use Case\nKey Limitation\n\n\n\n\nBLEU\nPrecision\nN-gram overlap with brevity penalty.\nMachine Translation\nIgnores synonyms and meaning.\n\n\nROUGE\nRecall\nN-gram overlap from reference perspective.\nSummarization\nIgnores synonyms; rewards redundancy.\n\n\nBERTScore\nSemantic Meaning\nCosine similarity of contextual embeddings.\nGeneral Purpose GenAI\nComputationally expensive; depends on base model bias.\n\n\nMAUVE\nDistribution & Diversity\nStatistical divergence between embedding clusters.\nOpen-Ended Generation\nComplex to calculate; requires large sample size."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#final-thoughts",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#final-thoughts",
    "title": "The Definitive Guide to BLEU Score: The Mathematics of Machine Translation",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nBLEU is imperfect. It is rigid, semantically blind, and sometimes fails to match human judgment.\nHowever, it remains widely adopted in machine translation evaluation. It is fast to compute, easy to understand, and language-agnostic. Understanding the mechanics of clipped precision and the brevity penalty gives you the intuition needed to interpret these scores correctly‚Äîand to know when it‚Äôs time to move on to something more advanced."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#bleu-bilingual-evaluation-understudy",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#bleu-bilingual-evaluation-understudy",
    "title": "Beyond Accuracy: A Deep Dive into NLP Evaluation Metrics",
    "section": "1. BLEU (Bilingual Evaluation Understudy)",
    "text": "1. BLEU (Bilingual Evaluation Understudy)\nCore Concept: Precision-based N-gram overlap.\nPrimary Use Case: Machine Translation (MT).\n\nMechanism\nBLEU calculates the Geometric Mean of modified n-gram precisions (usually up to \\(N=4\\)), multiplied by a Brevity Penalty (BP). The BP prevents the model from gaming the metric by outputting short, high-precision sentences (e.g., outputting ‚Äúthe‚Äù for a reference ‚Äúthe cat sat‚Äù).\n\n\nCalculation Logic\n\nCount N-grams: Calculate counts for 1-grams through 4-grams in the Candidate.\nClip Counts: \\(\\text{Count}_{\\text{clip}} = \\min(\\text{Count}_{\\text{Candidate}}, \\text{Count}_{\\text{Reference}})\\).\nPrecision (\\(p_n\\)): \\(\\sum \\text{Clipped Counts} / \\sum \\text{Total Candidate N-grams}\\).\nBrevity Penalty: If Candidate length &lt; Reference length, penalize score exponentially.\n\n\n\nTechnical Example\n\nReference: ‚ÄúThe cat is on the mat‚Äù\nCandidate: ‚ÄúThe the the cat mat‚Äù\n\nUnigram Precision (BLEU-1): * Raw Counts: ‚Äúthe‚Äù: 3, ‚Äúcat‚Äù: 1, ‚Äúmat‚Äù: 1. Total: 5. * Reference Max: ‚Äúthe‚Äù: 2, ‚Äúcat‚Äù: 1, ‚Äúmat‚Äù: 1. * Clipped Counts: 2 + 1 + 1 = 4. * Score: \\(4/5 = 0.8\\).\n\n\nIndustry Standards\n\nSacreBLEU: Do not use standard NLTK BLEU for reporting. Use sacrebleu. It standardizes tokenization and detokenization to ensuring results are comparable across papers.\nSmoothing: Essential for sentence-level BLEU to avoid zero scores when higher-order n-grams (e.g., 4-grams) have no overlaps."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#rouge-recall-oriented-understudy-for-gisting-evaluation",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#rouge-recall-oriented-understudy-for-gisting-evaluation",
    "title": "Beyond Accuracy: A Deep Dive into NLP Evaluation Metrics",
    "section": "2. ROUGE (Recall-Oriented Understudy for Gisting Evaluation)",
    "text": "2. ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\nCore Concept: Recall-based N-gram overlap. Primary Use Case: Text Summarization.\n\nMechanism\nMeasures the overlap of N-grams from the Reference‚Äôs perspective. It quantifies how much of the gold-standard summary the model successfully retrieved.\n\n\nVariants & Implementation\n\nROUGE-N (N-gram):\n\nFormula: \\(\\frac{\\sum \\text{Count}_{\\text{match}}(\\text{gram}_n)}{\\sum \\text{Count}_{\\text{Reference}}(\\text{gram}_n)}\\)\nFocus: ROUGE-1 captures informativeness; ROUGE-2 captures fluency.\n\nROUGE-L (Longest Common Subsequence):\n\nIdentifies the longest sequence of words that appear in both texts in the same relative order.\nAdvantage: Does not require consecutive matches. Captures sentence structure better than fixed N-grams.\n\n\n\n\nTechnical Example\n\nReference: ‚ÄúClimate change causes storms.‚Äù (4 words)\nCandidate: ‚ÄúStorms are caused.‚Äù\n\nROUGE-1 Recall: * Overlaps: ‚Äúcauses‚Äù (stemmed match), ‚Äústorms‚Äù. * Score: \\(2 / 4 = 0.5\\)."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#bertscore",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#bertscore",
    "title": "Beyond Accuracy: A Deep Dive into NLP Evaluation Metrics",
    "section": "3. BERTScore",
    "text": "3. BERTScore\nCore Concept: Semantic Similarity via Contextual Embeddings. Primary Use Case: Paraphrasing, Chatbots, Translation.\n\nMechanism\nSolves the ‚Äúexact match‚Äù flaw of N-gram metrics. It uses pre-trained transformers (BERT/RoBERTa/DeBERTa) to generate contextual embeddings for every token and computes pairwise cosine similarity.\n\n\nAlgorithm Steps\n\nEmbedding: Compute vectors for Reference \\(x = \\langle x_1, \\dots, x_k \\rangle\\) and Candidate \\(\\hat{x} = \\langle \\hat{x}_1, \\dots, \\hat{x}_l \\rangle\\).\nPairwise Similarity: Compute cosine similarity matrix between all candidate and reference tokens.\nGreedy Matching:\n\nFor each token in Reference, find the max similarity in Candidate (Recall).\nFor each token in Candidate, find the max similarity in Reference (Precision).\n\nWeighting: Optional IDF weighting to downplay common words like ‚Äúthe‚Äù or ‚Äúis‚Äù.\n\n\n\nComparison: BLEU vs.¬†BERTScore\n\nReference: ‚ÄúThe weather is chilly.‚Äù\nCandidate: ‚ÄúIt is cold outside.‚Äù\nBLEU: ~0.0 (No token overlap beyond ‚Äúis‚Äù).\nBERTScore: ~0.9 (High cosine similarity between embeddings of chilly and cold)."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#mauve",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#mauve",
    "title": "Beyond Accuracy: A Deep Dive into NLP Evaluation Metrics",
    "section": "4. MAUVE",
    "text": "4. MAUVE\nCore Concept: Distributional Divergence (Quality vs.¬†Diversity). Primary Use Case: Open-Ended Text Generation (Creative Writing, Story Completion).\n\nThe Problem\nTraditional metrics compare 1 Sample vs 1 Reference. In open-ended generation, there are thousands of valid continuations. Comparing against a single reference yields False Negatives.\n\n\nMechanism\nMAUVE measures the gap between the distribution of human text (\\(P\\)) and model text (\\(Q\\)). 1. Embedding: Map generated text and human text into a dense vector space (using GPT-2/3 embeddings). 2. Clustering: Discretize the space using k-means clustering to form a discrete probability distribution. 3. Divergence: Compute the Kullback-Leibler (KL) divergence between the two distributions.\n\n\nInterpretation\n\nScore Range: 0 to 1.\nLow Score (Type I Error): Model generates unrealistic/degenerate text (diverges from human manifold).\nLow Score (Type II Error): Model generates repetitive/safe text (Mode Collapse - fails to cover the diversity of human manifold).\n\n\n\nSummary Table\n\n\n\n\n\n\n\n\n\nMetric\nType\nCore Math\nBest For\n\n\n\n\nBLEU\nLexical\nGeometric Mean of Precision\nTranslation\n\n\nROUGE\nLexical\nN-gram Recall\nSummarization\n\n\nBERTScore\nSemantic\nToken-level Cosine Similarity\nParaphrasing / Semantic Content\n\n\nMAUVE\nDistributional\nKL-Divergence of Clusters\nOpen-Ended Generation"
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#bleu-precision-focused",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#bleu-precision-focused",
    "title": "Beyond Accuracy: A Deep Dive into NLP Evaluation Metrics",
    "section": "1. BLEU (Precision-Focused)",
    "text": "1. BLEU (Precision-Focused)\nBLEU (Bilingual Evaluation Understudy) measures how much of the candidate text appears in the reference text. It effectively calculates n-gram precision.\n\nThe Mechanism\nBLEU calculates a geometric mean of n-gram modified precision scores (\\(p_n\\)), multiplied by a Brevity Penalty (BP) to punish overly short outputs.\n\\[\n\\text{BLEU} = BP \\cdot \\exp\\left( \\sum_{n=1}^{N} w_n \\log p_n \\right)\n\\]\nWhere: * \\(p_n\\): The ratio of n-gram matches to total candidate n-grams (clipped by reference count). * \\(w_n\\): The weight for each n-gram size (usually uniform \\(1/4\\) for \\(N=4\\)). * \\(BP\\): A decay factor if the candidate length (\\(c\\)) is less than the reference length (\\(r\\)).\n\\[\nBP = \\begin{cases}\n1 & \\text{if } c &gt; r \\\\\ne^{(1 - r/c)} & \\text{if } c \\le r\n\\end{cases}\n\\]\n\n\nExample\n\nReference: ‚ÄúThe cat is on the mat‚Äù\nCandidate: ‚ÄúThe the the cat mat‚Äù\n\nStandard precision would be \\(5/5\\) (100%). BLEU uses clipped precision. ‚ÄúThe‚Äù appears twice in reference, so we cap the count. \\[\n\\text{Precision} = \\frac{2 (\\text{\"the\"}) + 1 (\\text{\"cat\"}) + 1 (\\text{\"mat\"})}{5 \\text{ (Total Candidate Words)}} = 0.8\n\\]\nBest Use: Machine Translation. Limitation: It is purely exact-match based. ‚ÄúAutomobile‚Äù and ‚ÄúCar‚Äù are treated as unrelated errors."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#rouge-recall-focused",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#rouge-recall-focused",
    "title": "Beyond Accuracy: A Deep Dive into NLP Evaluation Metrics",
    "section": "2. ROUGE (Recall-Focused)",
    "text": "2. ROUGE (Recall-Focused)\nROUGE (Recall-Oriented Understudy for Gisting Evaluation) measures how much of the reference text was captured by the candidate. It is the standard for text summarization.\n\nThe Mechanism\nROUGE-N computes the n-gram recall between the candidate and the reference.\n\\[\n\\text{ROUGE-N} = \\frac{\\sum_{S \\in \\{Refs\\}} \\sum_{gram_n \\in S} \\text{Count}_{\\text{match}}(gram_n)}{\\sum_{S \\in \\{Refs\\}} \\sum_{gram_n \\in S} \\text{Count}(gram_n)}\n\\]\nBasically: \\[\n\\text{Recall} = \\frac{\\text{Overlapping N-grams}}{\\text{Total N-grams in Reference}}\n\\]\n\n\nKey Variants\n\nROUGE-1: Unigram overlap (informative for content coverage).\nROUGE-2: Bigram overlap (informative for fluency).\nROUGE-L: Longest Common Subsequence. Captures sentence structure matches even if they are not consecutive, rewarding proper word order.\n\nBest Use: Summarization. Limitation: Like BLEU, it fails to capture semantic paraphrasing."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#bertscore-semantic-similarity",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#bertscore-semantic-similarity",
    "title": "Beyond Accuracy: A Deep Dive into NLP Evaluation Metrics",
    "section": "3. BERTScore (Semantic Similarity)",
    "text": "3. BERTScore (Semantic Similarity)\nBERTScore solves the synonym problem by using contextual embeddings. It evaluates similarity in the vector space rather than token space.\n\nThe Mechanism\n\nEmbed: Compute contextual embeddings for candidate tokens (\\(x\\)) and reference tokens (\\(y\\)) using a Transformer (e.g., RoBERTa).\nPairwise Similarity: Compute the cosine similarity matrix between all candidate and reference tokens.\nGreedy Match: For each token in the candidate, find the max similarity in the reference (and vice-versa for recall).\n\n\\[\nR_{\\text{BERT}} = \\frac{1}{|x|} \\sum_{x_i \\in x} \\max_{y_j \\in y} (x_i^{\\top} y_j)\n\\]\n\n\nWhy it wins\nConsider: * Ref: ‚ÄúThe weather is chilly.‚Äù * Cand: ‚ÄúIt is cold outside.‚Äù\nBLEU/ROUGE score this near 0. BERTScore yields a high value because the vector \\(\\vec{v}_{\\text{chilly}} \\approx \\vec{v}_{\\text{cold}}\\).\nBest Use: General GenAI evaluation (Translation, Dialogue, Paraphrasing)."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#mauve-distributional-divergence",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#mauve-distributional-divergence",
    "title": "Beyond Accuracy: A Deep Dive into NLP Evaluation Metrics",
    "section": "4. MAUVE (Distributional Divergence)",
    "text": "4. MAUVE (Distributional Divergence)\nBLEU and BERTScore compare one output to one reference. MAUVE compares the distribution of the model‚Äôs generated text (\\(P\\)) to the distribution of human text (\\(Q\\)). It is essential for open-ended generation (e.g., story writing).\n\nThe Mechanism\nMAUVE measures the gap between the human text distribution and model text distribution using quantized embeddings.\n\nEncode: Map model outputs and human text into a dense vector space (e.g., using GPT-2).\nCluster: Discretize this space to form probability distributions \\(P\\) (Model) and \\(Q\\) (Human).\nDivergence: Compute the Kullback-Leibler (KL) divergence.\n\nConceptually, it balances two error types: * Type I (Quality): The model produces unrealistic text (mass in \\(P\\) is outside \\(Q\\)). * Type II (Diversity): The model is repetitive and ‚Äúsafe‚Äù (mass in \\(P\\) covers only a small subset of \\(Q\\)).\n\\[\n\\text{MAUVE} \\approx \\text{Area Under the Divergence Curve}\n\\]\nBest Use: Open-ended text generation, creative writing, chatbots."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#summary-cheat-sheet",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#summary-cheat-sheet",
    "title": "Beyond Accuracy: A Deep Dive into NLP Evaluation Metrics",
    "section": "Summary Cheat Sheet",
    "text": "Summary Cheat Sheet\n\n\n\n\n\n\n\n\n\nMetric\nGoal\nMathematical Core\nIdeal Use Case\n\n\n\n\nBLEU\nPrecision\nN-gram Overlap + Brevity Penalty\nTranslation\n\n\nROUGE\nRecall\nN-gram Overlap (Reference-based)\nSummarization\n\n\nBERTScore\nMeaning\nCosine Similarity of Embeddings\nParaphrasing / QA\n\n\nMAUVE\nDiversity\nKL-Divergence of Distributions\nOpen-Ended Generation"
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#the-core-intuition-precision-vs.-recall",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#the-core-intuition-precision-vs.-recall",
    "title": "The Definitive Guide to BLEU Score: The Mathematics of Machine Translation",
    "section": "The Core Intuition: Precision vs.¬†Recall",
    "text": "The Core Intuition: Precision vs.¬†Recall\nWhen evaluating generated text against a reference, we have two fundamental choices:\n\nPrecision: How much of what the model generated was correct?\nRecall: How much of the reference did the model capture?\n\nBLEU is fundamentally a Precision-based metric.\nWhy? Imagine a translation task where the goal is to translate a complex German technical document.\n\nA high-recall model might output a messy 5-page document that definitely contains all the right information, but buries it in hallucinations and noise.\nA high-precision model might output a shorter, 3-page document. It might miss a nuance or two, but everything it did generate is accurate and fluent.\n\nIn translation, we generally prefer the latter. We want the generated text to be high quality, even if it‚Äôs slightly incomplete."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#step-1-the-trap-of-raw-precision",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#step-1-the-trap-of-raw-precision",
    "title": "The Definitive Guide to BLEU Score: The Mathematics of Machine Translation",
    "section": "Step 1: The Trap of Raw Precision",
    "text": "Step 1: The Trap of Raw Precision\nLet‚Äôs start with the simplest possible approach: counting word matches.\n\nReference 1: ‚ÄúThe cat is on the mat.‚Äù\nReference 2: ‚ÄúThere is a cat on the mat.‚Äù\n\nNow, imagine a poorly trained model generates this candidate:\n\nCandidate: ‚Äúthe the the the the the‚Äù\n\nIf we calculate standard unigram (1-word) precision:\n\nTotal candidate words: 6\nNumber of matches: 6 (The word ‚Äúthe‚Äù appears in the references).\nPrecision: 6/6 = 100%\n\nThis is obviously a catastrophic failure. A model could game the system by finding the most common word in the reference language and repeating it endlessly."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#step-2-the-fix-modified-clipped-precision",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#step-2-the-fix-modified-clipped-precision",
    "title": "The Definitive Guide to BLEU Score: The Mathematics of Machine Translation",
    "section": "Step 2: The Fix ‚Äî Modified (Clipped) Precision",
    "text": "Step 2: The Fix ‚Äî Modified (Clipped) Precision\n\n\n\nA visual representation of how BLEU calculates ‚ÄúClipped Precision,‚Äù mapping candidate words to reference constraints.\n\n\nTo solve the repetition problem, BLEU introduces clipped precision.\nThe rule is simple: A word in the candidate sentence can only be counted as a match up to the maximum number of times it appears in a single reference sentence.\nLet‚Äôs re-evaluate our bad candidate:\n\nCandidate: ‚Äúthe the the the the the‚Äù\nReference Constraint: The word ‚Äúthe‚Äù appears a maximum of two times (in Reference 1).\n\nThe math changes:\n\nTotal candidate words: 6\nClipped matches: 2 (We count the first two ‚Äúthe‚Äùs, and discard the remaining four).\nClipped Precision: 2/6 = 33.3%\n\nThis is much more reasonable. The metric now punishes absurd repetition.\nAs shown in the image at the top of this post, think of the reference sentences as providing a limited ‚Äúbudget‚Äù for each word count. Once the candidate uses up that budget, subsequent uses of that word are worthless."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#step-3-fluency-and-n-grams",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#step-3-fluency-and-n-grams",
    "title": "The Definitive Guide to BLEU Score: The Mathematics of Machine Translation",
    "section": "Step 3: Fluency and N-Grams",
    "text": "Step 3: Fluency and N-Grams\nMatching individual words is good for ensuring the content is roughly adequate, but it doesn‚Äôt ensure the sentence flows naturally.\n‚ÄúMat on is the cat the‚Äù contains all the right words, but it‚Äôs terrible English.\nBLEU solves this by calculating clipped precision not just for unigrams (1-grams), but also for bigrams (2-grams), trigrams (3-grams), and typically up to 4-grams.\n\n1-grams measure adequacy (are the right concepts there?).\n3-grams and 4-grams measure fluency (is the phrasing natural?).\n\nIf a model gets high precision on 4-grams, it means it‚Äôs getting long sequences of words exactly right, which usually correlates with high-quality, fluent text.\n\nCombining the Scores\nSo, we have four separate precision scores (\\(p_1, p_2, p_3, p_4\\)). How do we combine them into a single number?\nWe don‚Äôt take the arithmetic mean (an average). We take the geometric mean.\n\n\nWhy?\nThe geometric mean is highly sensitive to low scores. If a translation has excellent unigram precision (all the right words) but zero 4-gram precision (the order is completely scrambled), the geometric mean will crash toward zero. The arithmetic mean would still give it a decent pass. We want the metric to be harsh on models that fail at any level of granularity.\nThe combined precision score looks like this:\n\\[\\exp\\left( \\sum_{n=1}^{N} w_n \\log p_n \\right)\\]\nTypically, \\(N=4\\) and the weights \\(w_n\\) are uniform (\\(1/4\\))."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#step-4-the-brevity-penalty-preventing-the-gaming",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#step-4-the-brevity-penalty-preventing-the-gaming",
    "title": "The Definitive Guide to BLEU Score: The Mathematics of Machine Translation",
    "section": "Step 4: The Brevity Penalty (Preventing the Gaming)",
    "text": "Step 4: The Brevity Penalty (Preventing the Gaming)\nWe established that BLEU is a precision metric. But pure precision metrics have a massive loophole: Length.\nIf I have to translate a 50-word sentence, I could output a single, perfect 3-word phrase that I know is correct. My precision would be 100%. But I failed to translate the vast majority of the source sentence.\nTo prevent models from outputting overly short, ‚Äúsafe‚Äù sentences to maximize precision, BLEU applies a Brevity Penalty (BP).\nThe penalty is calculated based on two variables: 1. \\(c\\): The length of the candidate translation. 2. \\(r\\): The effective reference length.\nThe formula for BP is:\n\\[\nBP = \\begin{cases}\n1 & \\text{if } c &gt; r \\\\\ne^{(1 - r/c)} & \\text{if } c \\le r\n\\end{cases}\n\\]\nIf the candidate is longer than the reference (\\(c &gt; r\\)), there is no penalty (\\(BP = 1\\)). We don‚Äôt punish the model for being verbose (that‚Äôs what the precision score already does if the extra words are wrong).\nIf the candidate is shorter than the reference (\\(c \\le r\\)), the penalty kicks in exponentially.\nLet‚Äôs visualize this.\n\n\n\nA graph showing the BLEU Brevity Penalty. The Y-axis is the penalty multiplier (0 to 1), and the X-axis is the ratio of candidate length to reference length (c/r). The curve drops sharply below 1.0.\n\n\nAs you can see in the graph above, as soon as the candidate length drops below the reference length (ratio &lt; 1.0), the score multiplier starts dropping from 1.0 toward 0.0. A candidate that is half the length of the reference gets heavily penalized, ensuring that high-BLEU systems must produce output of comparable length to humans."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#the-complete-formula",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#the-complete-formula",
    "title": "The Definitive Guide to BLEU Score: The Mathematics of Machine Translation",
    "section": "The Complete Formula",
    "text": "The Complete Formula\nPutting it all together, the BLEU score is the geometric mean of the n-gram clipped precisions, multiplied by the brevity penalty.\n\\[\\textbf{BLEU} = \\underbrace{BP}_{\\text{Brevity Penalty}} \\cdot \\underbrace{\\exp\\left( \\sum_{n=1}^{4} w_n \\log p_n \\right)}_{\\text{Geometric Mean of N-Gram Precisions}}\\]\nThe final score is between 0 and 1, though in practice, it is almost always reported as a percentage between 0 and 100.\nA BLEU score over 30 is generally considered understandable. A score over 40-50 is often considered high quality, depending on the language pair (translating English to French is easier than English to Chinese, so expected scores vary)."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#a-data-scientists-critical-review",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#a-data-scientists-critical-review",
    "title": "The Definitive Guide to BLEU Score: The Mathematics of Machine Translation",
    "section": "A Data Scientist‚Äôs Critical Review",
    "text": "A Data Scientist‚Äôs Critical Review\nWe‚Äôve covered how BLEU works. Now, let‚Äôs talk about why it‚Äôs often frustrating. As data scientists, we need to know the limitations of our tools.\n1. The Synonym Problem (Semantic Blindness) BLEU relies on exact string matching. It has no concept of meaning. * Reference: ‚ÄúThe path is steep.‚Äù * Candidate: ‚ÄúThe trail is uphill.‚Äù BLEU will give this a very low score, despite it being a perfect translation. It punishes creative or varied vocabulary. This is why semantic embeddings-based metrics like BERTScore were invented.\n2. It Ignores Global Structure While n-grams capture local ordering, BLEU has no sense of overall sentence structure. ‚ÄúCat the sat mat on the‚Äù might get decent unigram scores, and it wouldn‚Äôt necessarily get a zero if some bigrams accidentally align.\n3. Human Correlation is Wobbly Crucially, an increase in BLEU score does not always correspond to an increase in human-perceived quality. A model might game the metric by optimizing for 4-grams in a way that sounds mechanical to a human reader. BLEU is useful for tracking progress during training, but the final sanity check must always be human evaluation.\n4. The Preprocessing Nightmare (sacrebleu) Because BLEU is based on exact string matching, it is incredibly sensitive to preprocessing. Do you tokenize text? Do you lowercase everything? How do you handle punctuation? Different implementations used to yield vastly different scores for the same model output.\nIndustry Standard: If you are reporting BLEU scores in a paper or a serious report, never roll your own implementation. Always use the sacrebleu Python library. It standardizes tokenization and preprocessing to ensure your scores are actually comparable to other people‚Äôs scores."
  },
  {
    "objectID": "posts/nlp-evaluation-metrics_deepdive/index.html#limitations",
    "href": "posts/nlp-evaluation-metrics_deepdive/index.html#limitations",
    "title": "The Definitive Guide to BLEU Score: The Mathematics of Machine Translation",
    "section": "Limitations",
    "text": "Limitations\nWe‚Äôve covered how BLEU works. Now, let‚Äôs talk about why it‚Äôs often frustrating. As data scientists, we need to know the limitations of our tools.\n1. The Synonym Problem (Semantic Blindness). BLEU relies on exact string matching. It has no concept of meaning.\n\nReference: ‚ÄúThe path is steep.‚Äù\nCandidate: ‚ÄúThe trail is uphill.‚Äù\n\nBLEU will give this a very low score, despite it being a perfect translation. It punishes creative or varied vocabulary. This is why semantic embeddings-based metrics like BERTScore were invented.\n2. It Ignores Global Structure While n-grams capture local ordering, BLEU has no sense of overall sentence structure. ‚ÄúCat the sat mat on the‚Äù might get decent unigram scores, and it wouldn‚Äôt necessarily get a zero if some bigrams accidentally align.\n3. Human Correlation is Wobbly Crucially, an increase in BLEU score does not always correspond to an increase in human-perceived quality. A model might game the metric by optimizing for 4-grams in a way that sounds mechanical to a human reader. BLEU is useful for tracking progress during training, but the final sanity check must always be human evaluation.\n4. The Preprocessing Nightmare (sacrebleu) Because BLEU is based on exact string matching, it is incredibly sensitive to preprocessing. Do you tokenize text? Do you lowercase everything? How do you handle punctuation? Different implementations used to yield vastly different scores for the same model output.\nIndustry Standard: If you are reporting BLEU scores in a paper or a serious report, never roll your own implementation. Always use the sacrebleu Python library. It standardizes tokenization and preprocessing to ensure your scores are actually comparable to other people‚Äôs scores."
  },
  {
    "objectID": "posts/scaled-dot-product-attention/index.html",
    "href": "posts/scaled-dot-product-attention/index.html",
    "title": "Scaled Dot-Product Attention: A Tensor-First Approach",
    "section": "",
    "text": "Published by Ramu Nalla - January 10, 2026\nIf you have ever tried to read the ‚ÄúAttention Is All You Need‚Äù paper or dig into the source code of a Transformer model, you have likely hit a wall. That wall is usually made of Tensors.\nYou see shapes like [32, 8, 10, 64]. You see operations like transpose(-2, -1). You see dot products and softmaxes flying around. It‚Äôs easy to get lost in the dimensionality and lose sight of what is actually happening: Communication.\nAt its core, the attention mechanism is just a way for words in a sentence to ‚Äútalk‚Äù to each other and figure out who they should be focusing on.\nIn this post, I am going to bypass the academic jargon. I will take a standard PyTorch implementation of Scaled Dot-Product Attention, break it down line-by-line, and crucially, I will trace the math with real numbers so you can see exactly what happens to the data."
  },
  {
    "objectID": "posts/scaled-dot-product-attention/index.html#the-code-scaled-dot-product-attention",
    "href": "posts/scaled-dot-product-attention/index.html#the-code-scaled-dot-product-attention",
    "title": "Scaled Dot-Product Attention: A Tensor-First Approach",
    "section": "The Code: Scaled Dot-Product Attention",
    "text": "The Code: Scaled Dot-Product Attention\nLet‚Äôs start with the raw code. This is a standard implementation you might find in any modern NLP library.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass ScaledDotProductAttention(nn.Module):\n    \"\"\"\n    Scaled Dot-Product Attention\n    Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V\n    \"\"\"\n\n    def __init__(self, d_k: int, dropout: float = 0.1):\n        super(ScaledDotProductAttention, self).__init__()\n        self.d_k = d_k\n        self.dropout = nn.Dropout(dropout)\n        self.scale = math.sqrt(d_k)\n\n    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor,\n                mask: torch.Tensor = None):\n        \"\"\"\n        Args:\n            Q: Query (batch_size, num_heads, seq_len, d_k)\n            K: Key   (batch_size, num_heads, seq_len, d_k)\n            V: Value (batch_size, num_heads, seq_len, d_v)\n            mask:    (batch_size, 1, seq_len, seq_len)\n        \"\"\"\n        # 1. Compute attention scores\n        # QK^T: (batch_size, num_heads, seq_len, seq_len)\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n        \n        # 2. Apply mask (set masked positions to large negative value)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        \n        # 3. Apply softmax to get attention weights\n        attention_weights = F.softmax(scores, dim=-1)\n        attention_weights = self.dropout(attention_weights)\n        \n        # 4. Apply attention weights to values\n        output = torch.matmul(attention_weights, V)\n        \n        return output, attention_weights\nIf that looks intimidating, don‚Äôt worry. We need to fix how we visualize tensors first."
  },
  {
    "objectID": "posts/scaled-dot-product-attention/index.html#strategy-mastering-tensor-dimensions",
    "href": "posts/scaled-dot-product-attention/index.html#strategy-mastering-tensor-dimensions",
    "title": "Scaled Dot-Product Attention: A Tensor-First Approach",
    "section": "Strategy: Mastering Tensor Dimensions",
    "text": "Strategy: Mastering Tensor Dimensions\nThe biggest source of confusion in PyTorch is seeing a tensor just as a list of numbers. You must start thinking of them as Named Dimensions.\nFor Attention, memorize this hierarchy (B, H, L, D):\n\nBatch (\\(B\\)): How many examples are we processing? (e.g., 32 sentences).\nHeads (\\(H\\)): How many ‚Äúperspectives‚Äù does the model have? (e.g., 8 heads).\nLength (\\(L\\)): How long is the sequence? (e.g., 10 words).\nDimension (\\(D\\)): How much data represents a single word? (e.g., a vector of size 64).\n\nWhen we do operations, we are usually manipulating just the last two dimensions (\\(L\\) and \\(D\\)). The Batch and Heads just come along."
  },
  {
    "objectID": "posts/scaled-dot-product-attention/index.html#a-concrete-example-i-love-ai",
    "href": "posts/scaled-dot-product-attention/index.html#a-concrete-example-i-love-ai",
    "title": "Scaled Dot-Product Attention: A Tensor-First Approach",
    "section": "A Concrete Example: ‚ÄúI Love AI‚Äù",
    "text": "A Concrete Example: ‚ÄúI Love AI‚Äù\nLet‚Äôs trace the code with a tiny example. We will ignore Batch and Heads for a moment and focus on the math for a single sequence.\n\nSequence: ‚ÄúI‚Äù, ‚ÄúLove‚Äù, ‚ÄúAI‚Äù (Length = 3)\nDimension (\\(d_k\\)): 2 (Each word is a vector of 2 numbers)\n\n\nStep 1: The Inputs (Q, K, V)\nImagine our model has learned the following representations for these words.\n\\[Q = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} \\quad (\\text{I, Love, AI})\\]\n\\[K = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} \\quad (\\text{I, Love, AI})\\]\n\\[V = \\begin{bmatrix} 10 & 20 \\\\ 30 & 40 \\\\ 50 & 60 \\end{bmatrix} \\quad (\\text{Content we want to extract})\\]\nNote: In reality, Q and K are different linear projections, but for simplicity, we keep them identical here.\n\n\nStep 2: The Similarity Search (scores)\nThis corresponds to this line of code:\nscores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\nWe want to know: How similar is every word to every other word?\nWe do this by taking the dot product of the Query (\\(Q\\)) and the Transpose of the Key (\\(K^T\\)).\n\\[\\text{Scores} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 1 & 1 & 2 \\end{bmatrix}\\]\nWhat does this matrix tell us?\nLook at the last row (representing the word ‚ÄúAI‚Äù).\n\nColumn 1 (Score 1): ‚ÄúAI‚Äù has some similarity to ‚ÄúI‚Äù.\nColumn 2 (Score 1): ‚ÄúAI‚Äù has some similarity to ‚ÄúLove‚Äù.\nColumn 3 (Score 2): ‚ÄúAI‚Äù is most similar to itself.\n\n\n\nStep 3: Scaling\nWe divide by self.scale, which is \\(\\sqrt{d_k}\\). Since our dimension is \\(d_k=4\\) (hypothetically), \\(\\sqrt{4}=2\\).\n\\[\\text{Scaled Scores} = \\frac{\\text{Scores}}{2} = \\begin{bmatrix} 0.5 & 0 & 0.5 \\\\ 0 & 0.5 & 0.5 \\\\ 0.5 & 0.5 & 1.0 \\end{bmatrix}\\]\nWhy do we do this?\nWithout scaling, dot products can grow huge as dimensions increase. Huge numbers into a Softmax function result in gradients close to zero (the ‚Äúvanishing gradient‚Äù problem), which kills training. Scaling keeps things stable.\n\n\nStep 4: The Softmax (Probabilities)\nattention_weights = F.softmax(scores, dim=-1)\nWe convert raw scores into probabilities that sum to 1. Let‚Äôs look at the first row (Word: ‚ÄúI‚Äù) which had scaled scores [0.5, 0, 0.5]. After Softmax, this might look like: [0.38, 0.24, 0.38].\nThis tells the model: ‚ÄúTo understand the word ‚ÄòI‚Äô, put 38% focus on ‚ÄòI‚Äô, 24% focus on ‚ÄòLove‚Äô, and 38% focus on ‚ÄòAI‚Äô.‚Äù\n\n\nStep 5: The Weighted Sum (The Output)\noutput = torch.matmul(attention_weights, V)\nFinally, we update the word‚Äôs meaning by taking a weighted sum of the Values (\\(V\\)). For the first word ‚ÄúI‚Äù, the calculation is:\n\\[\\text{Output}_{\\text{row1}} = (0.38 \\cdot V_{\\text{I}}) + (0.24 \\cdot V_{\\text{Love}}) + (0.38 \\cdot V_{\\text{AI}})\\]\n\\[= 0.38[10, 20] + 0.24[30, 40] + 0.38[50, 60]\\]\n\\[= [3.8, 7.6] + [7.2, 9.6] + [19, 22.8] = [30, 40]\\]\nThe word ‚ÄúI‚Äù started as vector [10, 20]. After attention, it became [30, 40]. It has absorbed context from the other words in the sentence."
  },
  {
    "objectID": "posts/scaled-dot-product-attention/index.html#handling-the-mask",
    "href": "posts/scaled-dot-product-attention/index.html#handling-the-mask",
    "title": "Scaled Dot-Product Attention: A Tensor-First Approach",
    "section": "Handling the Mask",
    "text": "Handling the Mask\nOne line we glossed over:\nscores = scores.masked_fill(mask == 0, -1e9)\nIn tasks like text generation, the model isn‚Äôt allowed to see the future. When predicting the 2nd word, it shouldn‚Äôt know what the 3rd word is.\nWe enforce this by applying a mask. We take the positions we want to hide and replace their scores with a massive negative number (like -1,000,000,000).\nWhy?\nBecause \\(e^{-1000000000}\\) is effectively zero. When we run Softmax, those words get 0% probability, effectively vanishing from the calculation."
  },
  {
    "objectID": "posts/scaled-dot-product-attention/index.html#summary",
    "href": "posts/scaled-dot-product-attention/index.html#summary",
    "title": "Scaled Dot-Product Attention: A Tensor-First Approach",
    "section": "Summary",
    "text": "Summary\nWhen you look at the ScaledDotProductAttention class now, try to see the story it tells:\n\nTranspose & Matmul: Create a grid showing how much every word relates to every other word.\nScale: Shrink the numbers so gradients don‚Äôt vanish.\nMask: Hide the words we aren‚Äôt allowed to see.\nSoftmax: Convert raw relationship scores into percentages.\nMatmul (Final): Create a new representation of the word that is a blend of all the relevant context it found.\n\nThe beauty of the Transformer isn‚Äôt just in its performance; it‚Äôs in this elegance of using simple linear algebra to model the complexity of language."
  },
  {
    "objectID": "posts/multi-head-attention/index.html",
    "href": "posts/multi-head-attention/index.html",
    "title": "Multi-Head Attention: A Tensor-First Walkthrough",
    "section": "",
    "text": "Published by Ramu Nalla - January 28, 2026\nIn my previous post, I have broken down the Scaled Dot-Product Attention mechanism. We saw how a Query finds similar Keys to extract Values.\nBut if you look at the actual Transformer architecture, you rarely see just ‚ÄúAttention.‚Äù You see Multi-Head Attention.\nWhy?\nImagine the sentence: ‚ÄúThe animal didn‚Äôt cross the street because it was too tired.‚Äù\nAs a human, you know ‚Äúit‚Äù refers to the ‚Äúanimal.‚Äù How do you know? 1. Grammatical perspective: You are looking for a noun that agrees with the pronoun. 2. Semantic perspective: You know that ‚Äústreets‚Äù don‚Äôt get ‚Äútired,‚Äù but ‚Äúanimals‚Äù do.\nIf a model only has a single attention head, it has to average these different types of relationships into one messy score. Multi-Head Attention allows the model to create distinct ‚Äúexperts.‚Äù Head 1 can focus on grammar, while Head 2 focuses on semantic context, running in parallel without interfering with each other.\nIn this post, I will walk through the standard PyTorch implementation of Multi-Head Attention. I will define a simple scenario and trace exactly what happens to the tensors at every step."
  },
  {
    "objectID": "posts/multi-head-attention/index.html#the-code-pytorch-multiheadattention",
    "href": "posts/multi-head-attention/index.html#the-code-pytorch-multiheadattention",
    "title": "Multi-Head Attention: A Tensor-First Walkthrough",
    "section": "The Code: PyTorch MultiHeadAttention",
    "text": "The Code: PyTorch MultiHeadAttention\nHere is the complete implementation I will be dissecting.\nimport torch\nimport torch.nn as nn\n# Assuming ScaledDotProductAttention is defined as in the previous post\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n        super(MultiHeadAttention, self).__init__()\n        \n        # Ensure the model dimension can be split evenly across heads\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n        \n        self.d_model = d_model\n        self.num_heads = num_heads\n        # d_k is the dimension of the vector *per head*\n        self.d_k = d_model // num_heads  \n        self.d_v = d_model // num_heads  \n        \n        # Linear projections for Q, K, V\n        # Note: We use single large projections instead of h separate ones\n        self.W_Q = nn.Linear(d_model, d_model)\n        self.W_K = nn.Linear(d_model, d_model)\n        self.W_V = nn.Linear(d_model, d_model)\n        \n        # Output projection (The Mixer)\n        self.W_O = nn.Linear(d_model, d_model)\n        \n        # Scaled dot-product attention mechanism\n        self.attention = ScaledDotProductAttention(self.d_k, dropout)\n        self.dropout = nn.Dropout(dropout)\n\n    def split_heads(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Split tensor into multiple heads and transpose for parallel processing\"\"\"\n        batch_size, seq_len, d_model = x.size()\n        \n        # Reshape: (B, Seq, d_model) -&gt; (B, Seq, H, d_k)\n        x = x.view(batch_size, seq_len, self.num_heads, self.d_k)\n        \n        # Transpose: (B, Seq, H, d_k) -&gt; (B, H, Seq, d_k)\n        # This puts Heads next to Batch for parallel attention computation\n        x = x.transpose(1, 2)\n        return x\n\n    def combine_heads(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Combine multiple heads back into a single continuous vector\"\"\"\n        # x shape: (B, H, Seq, d_k)\n        batch_size, num_heads, seq_len, d_k = x.size()\n        \n        # Transpose back: (B, H, Seq, d_k) -&gt; (B, Seq, H, d_k)\n        x = x.transpose(1, 2).contiguous()\n        \n        # Flatten: (B, Seq, H, d_k) -&gt; (B, Seq, d_model)\n        # H * d_k equals d_model again\n        x = x.view(batch_size, seq_len, self.d_model)\n        return x\n\n    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor,\n                mask: torch.Tensor = None):\n        batch_size = Q.size(0)\n        \n        # 1. Linear projections (Apply the \"lenses\")\n        # Shape remains: (B, Seq, d_model)\n        Q = self.W_Q(Q)  \n        K = self.W_K(K)  \n        V = self.W_V(V)  \n        \n        # 2. Split into multiple heads\n        # Shape becomes: (B, H, Seq, d_k)\n        Q = self.split_heads(Q)  \n        K = self.split_heads(K)  \n        V = self.split_heads(V)  \n        \n        # 3. Apply attention (in parallel across heads)\n        # Output shape: (B, H, Seq, d_v)\n        attn_output, attention_weights = self.attention(Q, K, V, mask)\n        \n        # 4. Combine heads (Stitch them back together)\n        # Shape becomes: (B, Seq, d_model)\n        attn_output = self.combine_heads(attn_output)\n        \n        # 5. Final linear projection (Mix the experts' opinions)\n        # Shape remains: (B, Seq, d_model)\n        output = self.W_O(attn_output)\n        output = self.dropout(output)\n        \n        return output, attention_weights"
  },
  {
    "objectID": "posts/multi-head-attention/index.html#the-tensor-walkthrough",
    "href": "posts/multi-head-attention/index.html#the-tensor-walkthrough",
    "title": "Multi-Head Attention: A Tensor-First Walkthrough",
    "section": "The Tensor Walkthrough",
    "text": "The Tensor Walkthrough\nTo truly understand Multi-Head Attention, we need to trace the shapes.\n\nOur Scenario\n\nWe are processing 1 sentence (Batch Size \\(B=1\\)).\nThe sentence has 3 words (Sequence Length \\(L=3\\), e.g., ‚ÄúI‚Äù, ‚ÄúLove‚Äù, ‚ÄúAI‚Äù).\nEach word is represented by a tiny vector of size 4 (\\(d_{model}=4\\)).\nWe want 2 parallel heads (\\(H=2\\)).\nTherefore, each head will operate on vectors of size 2 (\\(d_k = 4 / 2 = 2\\)).\n\n\n\nStep 1: The Inputs and Projections\nThe inputs \\(Q\\), \\(K\\), and \\(V\\) all start with the same shape.\n\nInput Shape: (1, 3, 4) \\(\\rightarrow\\) (Batch, Seq, \\(d_{model}\\))\n\nWe pass them through linear layers (\\(W_Q\\), \\(W_K\\), and \\(W_V\\)). These are essentially filters that transform the raw word embeddings into ‚ÄúQuery space,‚Äù ‚ÄúKey space,‚Äù and ‚ÄúValue space.‚Äù\n# 1. Linear projections\nQ = self.W_Q(Q)\nK = self.W_K(K)\nV = self.W_V(V)\n\nOutput Shape: (1, 3, 4)\n\nCrucially, the shape hasn‚Äôt changed yet. We have just transformed the data inside the vectors.\n\n\nStep 2: Splitting the Heads (‚ÄúThe Magic Trick‚Äù)\nThis is the most confusing part for beginners. How do we get multiple heads out of one vector?\nWe take the final dimension (\\(d_{model}=4\\)) and physically chop it into \\(H\\) chunks of size \\(d_k\\).\n# 2. Split into multiple heads\nQ = self.split_heads(Q)\n# ... K and V do the same ...\n1. View (Reshape): We tell the system to interpret the (..., 4) dimension as (..., 2, 2).\n\nShape changes from (1, 3, 4) to (1, 3, 2, 2) \\(\\rightarrow\\) (Batch, Seq, Heads, \\(d_k\\)).\n\n2. Transpose (Swap): We swap the ‚ÄúSequence‚Äù dimension (dim 1) with the ‚ÄúHeads‚Äù dimension (dim 2).\n\nShape changes from (1, 3, 2, 2) to (1, 2, 3, 2) \\(\\rightarrow\\) (Batch, Heads, Seq, \\(d_k\\)).\n\nWhy Transpose? Matrix multiplication operations typically operate on the last two dimensions. By moving ‚ÄúHeads‚Äù to the second position, the system treats (1, 2) as a ‚Äúsuper-batch.‚Äù It effectively tricks the GPU into running the attention mechanism on Head 1 and Head 2 simultaneously in parallel.\n\n\nStep 3: Apply Attention\nWe now pass these 4D tensors to the attention function. Because of the shape (1, 2, 3, 2), the attention module calculates dot products on the last two dimensions (3, 2).\n# 3. Apply attention\nattn_output, attention_weights = self.attention(Q, K, V, mask)\n\nHead 1 performs attention on its (3, 2) data.\nHead 2 performs attention on its (3, 2) data.\n\nOutput Shape: (1, 2, 3, 2). The results are still separated by head.\n\n\nStep 4: Combining Heads\nWe have the results, but they are split. We need to stitch them back together into a single representation for each word. We essentially reverse Step 2.\n# 4. Combine heads\nattn_output = self.combine_heads(attn_output)\n1. Transpose Back: Swap Heads and Seq dimensions.\n\nShape changes from (1, 2, 3, 2) to (1, 3, 2, 2).\n\n2. View (Flatten): Merge the ‚ÄúHeads‚Äù dimension (size 2) and ‚Äú\\(d_k\\)‚Äù dimension (size 2) back into ‚Äú\\(d_{model}\\)‚Äù (size 4).\n\nShape changes from (1, 3, 2, 2) to (1, 3, 4).\n\nEffectively, the vector of size 2 from Head 1 and the vector of size 2 from Head 2 are now concatenated side-by-side to make a vector of size 4.\n\n\nStep 5: The Final Mixer (\\(W_O\\))\nWe have a vector of size 4, but the top half is purely from Head 1, and the bottom half is purely from Head 2. They haven‚Äôt interacted yet.\n# 5. Final linear projection\noutput = self.W_O(attn_output)\nWe pass this through a final linear layer, \\(W_O\\) (Output weights). This layer mixes these features together. It allows the model to synthesize the ‚Äúgrammar insights‚Äù from Head 1 with the ‚Äúsemantic insights‚Äù from Head 2 into a single, unified representation for the word.\n\nFinal Output Shape: (1, 3, 4)\n\nWe started with (1, 3, 4) and ended with (1, 3, 4), but the vectors now contain rich, contextual information gathered from multiple perspectives across the whole sentence."
  },
  {
    "objectID": "posts/multi-head-attention/index.html#deep-dive-why-one-big-matrix",
    "href": "posts/multi-head-attention/index.html#deep-dive-why-one-big-matrix",
    "title": "Multi-Head Attention: A Tensor-First Walkthrough",
    "section": "Deep Dive: Why ‚ÄúOne Big Matrix‚Äù?",
    "text": "Deep Dive: Why ‚ÄúOne Big Matrix‚Äù?\nYou might wonder why we typically use one large linear layer (size \\(d_{model}\\)) instead of creating a list of smaller separate layers (size \\(d_k\\)).\nMathematically, they are identical. Computationally, one big matrix is much faster.\nGPUs love crunching massive matrices. It is significantly more efficient to perform one massive matrix multiplication (\\(512 \\times 512\\)) and then slice the result in memory, rather than launching 8 separate, smaller kernel operations (\\(512 \\times 64\\)) in a loop. This implementation is a standard trick to maximize hardware utilization."
  },
  {
    "objectID": "posts/multi-head-attention/index.html#summary",
    "href": "posts/multi-head-attention/index.html#summary",
    "title": "Multi-Head Attention: A Tensor-First Walkthrough",
    "section": "Summary",
    "text": "Summary\nMulti-Head Attention is a sophisticated way of saying ‚Äúdo parallel processing and mix the results.‚Äù\nBy splitting the embedding dimension into distinct heads, we allow the Transformer to learn different types of relationships simultaneously. The intricate steps of view and transpose operations is simply the mechanism required to arrange the data efficiently so GPUs can perform this parallel computation and then stitch the diverse insights back together."
  }
]