[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "My First Blog Post",
    "section": "",
    "text": "Welcome to my new Data Science blog!"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Download Full CV (PDF)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ramu Nalla",
    "section": "",
    "text": "I am a Senior Data Scientist at HighLevel, where I focus on building and scaling Agentic AI systems using Large Language Models (LLMs). My work involves tackling complex NLP challenges at scale, specifically specializing in engineering production-grade RAG pipelines with tools like LangChain and LangGraph. My work also involves developing AI models to enhance SaaS business intelligence and establishing robust evaluation frameworks for LLM-based systems to drive measurable business impact.\nBefore HighLevel, I worked as a Senior ML Engineer at Enphase Energy for nearly five years, where I spearheaded chatbot initiatives and developed AI models to detect hardware-product failures.\nMy career has been primarily focused on applied machine learning, bridging the gap between innovation and real-world applications.\nI earned my B.Tech and M.Tech, along with an MBA minor, from IIT Kanpur.\nI enjoy blogging about Machine Learning, and I am currently working on a series of posts about NLP, MLOps, and Agentic AI architectures."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blogs",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nBuilding a Minimal RAG Pipeline from Scratch\n\n3 min\n\n\nNLP\n\nLLMs\n\nRAG\n\nPython\n\n\n\nA look under the hood of Retrieval-Augmented Generation. We build a semantic search engine using just NumPy to understand the core math behind the magic.\n\n\n\nJan 17, 2026\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#latest-thinking",
    "href": "index.html#latest-thinking",
    "title": "Ramu Nalla",
    "section": "Latest Thinking",
    "text": "Latest Thinking\n\n\n\n\n\nMy First Blog Post\n\n\n\n\n\n\n\n\nMay 20, 2024\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#updates",
    "href": "index.html#updates",
    "title": "Ramu Nalla",
    "section": "Updates",
    "text": "Updates\n\n\n\nDate\nEvent\n\n\n\n\nJan 17, 2026\nüöÄ Updated portfolio architecture\n\n\nJan 15, 2026\nüéâ Launched personal website\n\n\n\n\n\n\n‚úâÔ∏è üêô üíº ùïè\n\n\n¬© 2026 Ramu Nalla. Built with Quarto & Python."
  },
  {
    "objectID": "index.html#latest-posts",
    "href": "index.html#latest-posts",
    "title": "Ramu Nalla",
    "section": "Latest Posts",
    "text": "Latest Posts\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        date - Oldest\n      \n      \n        date - Newest\n      \n      \n        title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\ndate\n\n\n\ntitle\n\n\n\n\n\n\n\n\nJan 17, 2026\n\n\nBuilding a Minimal RAG Pipeline from Scratch\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#news",
    "href": "index.html#news",
    "title": "Ramu Nalla",
    "section": "News",
    "text": "News\n\n\n\nDate\nUpdate\n\n\n\n\nJan 17, 2026\nUpdated personal portfolio website\n\n\nJan 15, 2026\nLaunched personal portfolio website\n\n\n\n\n\n\n    \n\n\nBest to reach out to me over email or DM me on LinkedIn.\n\n\n¬© 2026 Ramu Nalla. All rights reserved."
  },
  {
    "objectID": "posts/2026-01-17-simple-rag-tutorial/index.html",
    "href": "posts/2026-01-17-simple-rag-tutorial/index.html",
    "title": "Building a Minimal RAG Pipeline from Scratch",
    "section": "",
    "text": "Published by Ramu Nalla - January 17, 2026\nIf you have worked with Large Language Models (LLMs) for more than five minutes, you have likely run into their biggest flaw: hallucinations. They sound confident, but they don‚Äôt know your private data, and their knowledge cutoff is always in the past.\nRetrieval-Augmented Generation (RAG) is the industry standard solution to this. It bridges the gap between a ‚Äúfrozen‚Äù LLM and your dynamic, private data.\nBefore I started using complex vector databases like Pinecone or Milvus, I wanted to understand exactly what was happening under the hood. So, I built a minimal RAG pipeline using nothing but Python and NumPy."
  },
  {
    "objectID": "posts/2026-01-17-simple-rag-tutorial/index.html#the-retrieval-mechanism",
    "href": "posts/2026-01-17-simple-rag-tutorial/index.html#the-retrieval-mechanism",
    "title": "Building a Minimal RAG Pipeline",
    "section": "",
    "text": "The most critical part of RAG is semantic search. We calculate the cosine similarity between the user‚Äôs query vector and our document vectors.\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# 1. Dummy Knowledge Base\ndocuments = [\n    \"RAG improves LLM accuracy by providing external context.\",\n    \"LangChain is a framework for building applications with LLMs.\",\n    \"Vector databases store embeddings for efficient semantic search.\"\n]\n\n# 2. Mock Embeddings (3-dimensional vectors for demo)\ndoc_vectors = np.array([\n    [0.9, 0.1, 0.1], \n    [0.2, 0.8, 0.2], \n    [0.1, 0.2, 0.9]\n])\n\n# 3. User Query Vector\nquery_vector = np.array([[0.85, 0.15, 0.1]]) \n\n# 4. Calculate Similarity\nscores = cosine_similarity(query_vector, doc_vectors).flatten()\n\n# 5. Retrieve Top Result\nbest_match_idx = np.argmax(scores)\nprint(f\"Query matched: '{documents[best_match_idx]}'\")\nprint(f\"Confidence Score: {scores[best_match_idx]:.4f}\")"
  },
  {
    "objectID": "posts/2026-01-17-simple-rag-tutorial/index.html#conclusion",
    "href": "posts/2026-01-17-simple-rag-tutorial/index.html#conclusion",
    "title": "Building a Minimal RAG Pipeline",
    "section": "",
    "text": "This is a simplified view of how retrieval works. In a production environment, we would scale this using tools like Pinecone or Milvus."
  },
  {
    "objectID": "posts/2026-01-17-simple-rag-tutorial/index.html#the-core-concept",
    "href": "posts/2026-01-17-simple-rag-tutorial/index.html#the-core-concept",
    "title": "Building a Minimal RAG Pipeline from Scratch",
    "section": "The Core Concept",
    "text": "The Core Concept\nRAG isn‚Äôt a single algorithm; it‚Äôs a workflow.\n\nRetrieval: Find the most relevant documents for a user‚Äôs question.\nAugmentation: Paste those documents into the LLM‚Äôs prompt context.\nGeneration: Ask the LLM to answer the question using only that context.\n\nThe ‚Äúmagic‚Äù happens in step 1. How does a computer know that ‚Äúneural network‚Äù and ‚Äúdeep learning‚Äù are related? The answer is Vector Embeddings."
  },
  {
    "objectID": "posts/2026-01-17-simple-rag-tutorial/index.html#the-code-vector-search-in-pure-python",
    "href": "posts/2026-01-17-simple-rag-tutorial/index.html#the-code-vector-search-in-pure-python",
    "title": "Building a Minimal RAG Pipeline from Scratch",
    "section": "The Code: ‚ÄúVector Search‚Äù in Pure Python",
    "text": "The Code: ‚ÄúVector Search‚Äù in Pure Python\nIn a real production environment, we use embedding models (like OpenAI‚Äôs text-embedding-3-small) to turn text into massive lists of numbers. For this demo, I‚Äôll manually create small 3-dimensional vectors to visualize the math.\nHere is the entire logic in one script:\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# 1. Our \"Knowledge Base\"\n# In reality, this would be thousands of PDF chunks stored in a Vector DB.\ndocuments = [\n    \"RAG bridges the gap between LLMs and private data.\",\n    \"LangChain is a popular framework for building AI agents.\",\n    \"Vector databases like Pinecone store embeddings for fast retrieval.\"\n]\n\n# 2. Mock Embeddings\n# Imagine these are the output of an embedding model.\n# Notice how Doc 1 (0.9, 0.1, ...) and Doc 3 (0.1, 0.2, ...) are mathematically different.\ndoc_vectors = np.array([\n    [0.9, 0.1, 0.1],  # Embedding for Document 1\n    [0.2, 0.8, 0.2],  # Embedding for Document 2\n    [0.1, 0.2, 0.9]   # Embedding for Document 3\n])\n\n# 3. The User's Query\n# \"How do I use private data with LLMs?\" \n# This query is semantically similar to Document 1.\nquery_vector = np.array([[0.85, 0.15, 0.1]]) \n\n# 4. The Retrieval Step (Cosine Similarity)\n# We calculate the angle between the Query and every Document.\n# Higher score = Closer match.\nscores = cosine_similarity(query_vector, doc_vectors).flatten()\n\n# 5. Get the Winner\nbest_match_idx = np.argmax(scores)\nretrieved_doc = documents[best_match_idx]\nconfidence = scores[best_match_idx]\n\nprint(f\"‚úÖ User Query Mapped to: '{retrieved_doc}'\")\nprint(f\"üìä Confidence Score: {confidence:.4f}\")\n\nWhat just happened?\nWhen we ran cosine_similarity, Python calculated the angle between our query vector and the document vectors.\n\nThe query vector [0.85, 0.15, 0.1] was heavily weighted towards the first dimension.\nDocument 1 [0.9, 0.1, 0.1] was also weighted towards the first dimension.\nTherefore, the math says: ‚ÄúThese two ideas are related.‚Äù"
  },
  {
    "objectID": "posts/2026-01-17-simple-rag-tutorial/index.html#moving-to-production",
    "href": "posts/2026-01-17-simple-rag-tutorial/index.html#moving-to-production",
    "title": "Building a Minimal RAG Pipeline from Scratch",
    "section": "Moving to Production",
    "text": "Moving to Production\nWhile this numpy example is great for intuition, it doesn‚Äôt scale to millions of rows. In my professional work, moving from this proof-of-concept to production involves:\n\nReal Embeddings: Replacing manual vectors with sentence-transformers or OpenAI embeddings.\nVector Store: Using a dedicated database (ChromaDB, Weaviate, or Snowflake) to index millions of vectors.\nReranking: Adding a second step to double-check the relevance of the retrieved documents.\n\nI‚Äôll be writing more about building Agentic workflows on top of this RAG architecture in future posts. Stay tuned!"
  }
]