---
title: "Building a Minimal RAG Pipeline"
description: "A step-by-step guide to implementing Retrieval-Augmented Generation (RAG) using Python and Vector Search concepts."
author: "Ramu Nalla"
date: "2026-01-17"
categories: [NLP, LLMs, RAG, Tutorial]
image: "image.jfif"
draft: false
---

# Introduction

Retrieval-Augmented Generation (RAG) has become the standard architecture for grounding Large Language Models (LLMs) on private data. In this post, I will demonstrate a minimal implementation of the retrieval step.

## The Retrieval Mechanism

The most critical part of RAG is semantic search. We calculate the cosine similarity between the user's query vector and our document vectors.

```python
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# 1. Dummy Knowledge Base
documents = [
    "RAG improves LLM accuracy by providing external context.",
    "LangChain is a framework for building applications with LLMs.",
    "Vector databases store embeddings for efficient semantic search."
]

# 2. Mock Embeddings (3-dimensional vectors for demo)
doc_vectors = np.array([
    [0.9, 0.1, 0.1], 
    [0.2, 0.8, 0.2], 
    [0.1, 0.2, 0.9]
])

# 3. User Query Vector
query_vector = np.array([[0.85, 0.15, 0.1]]) 

# 4. Calculate Similarity
scores = cosine_similarity(query_vector, doc_vectors).flatten()

# 5. Retrieve Top Result
best_match_idx = np.argmax(scores)
print(f"Query matched: '{documents[best_match_idx]}'")
print(f"Confidence Score: {scores[best_match_idx]:.4f}")
```

## Conclusion

This is a simplified view of how retrieval works. In a production environment, we would scale this using tools like **Pinecone** or **Milvus**.