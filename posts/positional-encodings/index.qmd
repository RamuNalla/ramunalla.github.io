---
title: "Positional Encoding in Transformers: The Geography of Language"
description: "How does a Transformer treat 'The cat ate the fish' different than 'The fish ate the cat'?  We explore the mathematics of Positional Encoding to see how models learn order in parallel processing."
categories: [Deep Learning, Transformers, NLP, PyTorch]
image: "positional-encoding-hero.png"
draft: false
author: "Ramu Nalla"
date: "2026-01-13"
format:
  html:
    toc: false
    toc-depth: 3
    toc-location: left
    toc-title: "Contents"
---

<div class="blog-manual-meta">Published by Ramu Nalla - January 13, 2026</div>

![Conceptual Illustration of Language Chaos.](image.png){width=70%}


---

Imagine you tossed the sentence "The cat ate the fish" into a blender. You know exactly which words are in the mix, but you have lost the story. "The fish ate the cat" would look identical in that blender.

This is the fundamental problem with the standard Transformer architecture.

Unlike Recurrent Neural Networks (RNNs), which process words sequentially ($t_1$, then $t_2$, then $t_3$), Transformers process all words in a sentence simultaneously in parallel. This parallelism is what makes them so fast, but it comes at a steep cost: the model is inherently invariant to word order. It sees a "bag of words," not a sentence. To fix this, we must inject a sense of order back into the data. We need to "tag" every word with its geographic location in the sentence before it enters the Transformer.

This technique is called **Positional Encoding (PE)**.

In this post, we are going to explore the two main strategies for doing this: the elegant, fixed mathematical approach (Sinusoidal) used in the original "Attention Is All You Need" paper, and the simpler, data-driven approach (Learnable).



## The Goal: A Unique Fingerprint

What properties should a good positional tag have?

1.  **Unique:** Position 5 must look different from Position 6.
2.  **Consistent distance:** The "mathematical distance" between Position 5 and 6 should be the same as between Position 10 and 11.
3.  **Extensible:** Ideally, the model should be able to handle sentences longer than ones it saw during training.
4.  **Deterministic:** The tag for position 5 should always be the same.

You might think, *"Why not just add the number 1 to the first word vector, 2 to the second, and so on?"*

The problem is **scale**. For a 1000-word document, the 1000th word would have massive values added to it, completely drowning out the actual semantic meaning of the word embedding (which usually consists of small numbers between -1 and 1).

We need a system that stays within a bounded range. **Enter trigonometry.**

## Strategy 1: Sinusoidal Positional Encoding (The Fixed Approach)

The authors of the original Transformer paper proposed a brilliant, non-learnable solution based on sine and cosine waves of different frequencies.

Here is the intuition: **Imagine a row of analog clocks.**

* The first clock has a hand that spins very fast (seconds).
* The next spins slower (minutes).
* The next spins even slower (hours).

If you take a snapshot of all the clock hands at any given moment, that specific combination of hand positions is a unique timestamp.

Sinusoidal encoding works similarly. Every position gets a unique combination of sine and cosine values generated by waves oscillating at different speeds across the embedding dimension.

### The Formula

This is the part that usually scares people away. Let's look at it and then decode it.

Given a position `pos` in the sentence, and a specific dimension index `i` in the embedding vector, the encoding is computed as:

$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$

$$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$

Don't panic. Let's break down the technical interpretation.

![Conceptual Illustration of Sine - Cosine Positional Encoding.](sine_cosine.png){width=70%}

### Technical Interpretation: Position vs. Dimension

The crucial thing to understand is that the **frequency** of the wave changes depending on where you are in the embedding vector.

**1. Interpreting Along the Dimensions (Fixing position `pos`)**

Imagine you are looking at the word at **Position 10**. Its positional encoding is a vector of size $d_{model}$ (e.g., 512).

* At the start of that vector (low `i` indices), the denominator is small. The frequency is **high**. The values change rapidly.
* At the end of that vector (high `i` indices), the denominator is huge. The frequency is **very low**. The wave is stretched out incredibly long.

This means early dimensions encode **local order**, while later dimensions encode **global position**.

**2. Interpreting Along the Positions (Fixing dimension `i`)**

Now, imagine you are looking at just **Dimension 0** across the entire sentence. You will see a perfect high-frequency sine wave as you read from word 1 to word 100. If you look at Dimension 256, you will see a much slower cosine wave.



### A Concrete Example (Tiny Math Trace)

Let's use tiny numbers to see it work.

* $d_{model} = 4$
* Sequence Length = 2 (Positions 0 and 1)

We need to build a $(2 \times 4)$ matrix.

**The Frequencies (The Denominator term):**

We have pairs of dimensions $2i$ and $2i+1$. The indices $i$ are $0$ and $1$. The denominator is $10000^{(2i/4)}$.

* For dimensions 0 and 1 ($i=0$): Denominator is $1$. Frequency term is $1$.
* For dimensions 2 and 3 ($i=1$): Denominator is $100$. Frequency term is $0.01$.

**Calculating Position 0:**

* Dim 0 (sin, freq 1): $\sin(0) = 0$
* Dim 1 (cos, freq 1): $\cos(0) = 1$
* Dim 2 (sin, freq 0.01): $\sin(0) = 0$
* Dim 3 (cos, freq 0.01): $\cos(0) = 1$

* **PE at Pos 0: [0, 1, 0, 1]**

**Calculating Position 1:**

* Dim 0 (sin, freq 1): $\sin(1) \approx 0.84$
* Dim 1 (cos, freq 1): $\cos(1) \approx 0.54$
* Dim 2 (sin, freq 0.01): $\sin(0.01) \approx 0.01$
* Dim 3 (cos, freq 0.01): $\cos(0.01) \approx 0.99$

* **PE at Pos 1: [0.84, 0.54, 0.01, 0.99]**

We now have unique vectors for position 0 and 1 that are bounded between -1 and 1. We literally add these to the semantic word embeddings.

### The Implementation (PyTorch)

Implementing this efficiently requires some clever broadcasting and a numerical stability trick.

```python
import torch
import torch.nn as nn
import math

class PositionalEncoding(nn.Module):
    """
    Sinusoidal positional encoding implementation.
    """
    def __init__(self, d_model: int, max_len: int = 5000, dropout_p: float = 0.1):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout_p)

        # 1. Create the Matrix Buffer
        # Initialize a matrix (max_len x d_model) with zeros.
        pe = torch.zeros(max_len, d_model) 

        # 2. Create Position Indices
        # A column vector [[0], [1], [2], ...]. Shape: (max_len, 1)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)

        # 3. Calculate the Frequencies (The "div_term")
        # The formula uses 10000^(2i/d_model).
        # To avoid huge numbers, we use the log trick: a^b = exp(b * ln(a))
        # This calculates the denominator term for the even indices (0, 2, 4...).
        # Shape: (d_model / 2)
        div_term = torch.exp(
            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)
        )

        # 4. Apply Sine and Cosine using Broadcasting
        # Multiply position (col vector) by frequencies (row vector) to get the
        # arguments for sin/cos. Fill even cols with sin, odd with cos.
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        # 5. Add Batch Dimension and Register
        # Shape becomes (1, max_len, d_model) for easy addition later.
        pe = pe.unsqueeze(0)

        # register_buffer means: "Save this tensor with the model weights, 
        # but do NOT update it during training with gradients."
        self.register_buffer('pe', pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: Input embeddings of shape (batch_size, actual_seq_len, d_model)
        """
        # 1. Slice the pre-computed matrix to match the actual sequence length.
        # If input is 10 words long, take the first 10 rows of 'pe'.
        # 2. ADD the encoding to the input embeddings.
        x = x + self.pe[:, :x.size(1), :]
        
        # Apply dropout for regularization
        return self.dropout(x)
```

The beauty of this code is that `pe` is computed once when the model is initialized. During training or inference, it's just a fast lookup and addition operation.

## Strategy 2: Learnable Positional Encoding (The Data-Driven Approach)

After the original Transformer paper, researchers asked a pragmatic question: *"Why work so hard with trigonometry? Why are we forcing a human-designed pattern onto the model? Why not just let the neural network learn the best position tags itself?"*

This is the **Learnable** approach. It is deceptively simple, yet it powers some of the most famous models in history, including BERT and the early GPT series.

### The Intuition: A Second Vocabulary

In this approach, we treat "Position" exactly the same way we treat "Words."

Just as the model has a lookup table (an embedding layer) for words—where ID 256 might map to the vector for "Apple"—it has a second lookup table for positions.

* Position ID 0 maps to a learned vector $v_0$.
* Position ID 1 maps to a learned vector $v_1$.
* ...
* Position ID 511 maps to a learned vector $v_{511}$.

These vectors are initialized with random noise. During the training process, while the model is learning to translate English to French, it is simultaneously adjusting these position vectors via backpropagation. It figures out exactly what numbers need to be in those vectors to help it distinguish *"The cat ate the mouse"* from *"The mouse ate the cat."*

### Technical Interpretation: The "Black Box" Dimensions

Unlike the Sinusoidal approach, where we know exactly what Dimension 0 represents (a high-frequency wave) and what Dimension 511 represents (a low-frequency wave), Learnable Encodings are opaque.

**1. No Fixed Mathematical Meaning**

If you look at Dimension $i$ in a learnable position vector, it does not necessarily correspond to a frequency or a clear pattern.

* Dimension 0 might learn to be a simple counter.
* Dimension 1 might learn to toggle on and off every 5 words.
* Dimension 2 might look like random noise to us, but contain a specific signal the model finds useful for attention heads.

We lose the interpretability of the sine waves, but we gain flexibility. The model is free to invent its own coordinate system.

**2. Neighbor Similarity**

Despite the lack of a fixed formula, a well-trained model usually converges on a logical structure. It tends to learn vectors such that **Position 5** and **Position 6** are very similar (they have a high cosine similarity).

**Why?** Because words next to each other usually share context. If the positional tags for neighbors were wildly different (orthogonal), it would be much harder for the Attention mechanism to learn local relationships like adjective-noun pairings. The model naturally drifts toward making neighbors "look" similar in vector space to minimize its loss.

### How It Works Overall: The "Summation" Step

A common point of confusion is where this positional information enters the network. Does it go into the Query? The Key?

In the standard architecture (like BERT or the original Transformer), the positional vector is added directly to the **Input Embedding** before the data ever touches the first Attention layer.

1.  **Step 1: Word Lookup.** We convert the word "Cat" into a 512-dimensional semantic vector.
2.  **Step 2: Position Lookup.** We look up the learned vector for "Position 5."
3.  **Step 3: Addition.** We element-wise add them together.

$$Input = Word\_Embedding + Position\_Embedding$$



**Wait, doesn't adding two vectors corrupt the information?** If "Cat" is `[0.5, 0.2]` and Position 5 is `[0.1, 0.9]`, the result `[0.6, 1.1]` is neither purely "Cat" nor purely "Position 5."

This is true. However, in high-dimensional space (e.g., 512, 1024, or 4096 dimensions), the model learns to segregate the information. It might use the first few dozen dimensions primarily for positional signaling and the rest for semantic meaning, or interleave them in complex ways.

Once this "summed" vector enters the first Attention layer:

* It is projected into Queries ($Q$), Keys ($K$), and Values ($V$).
* Because the position information was baked into the input, $Q$, $K$, and $V$ all contain traces of that positional signal.
* When $Q$ and $K$ interact (dot product), the positional traces help determine the attention score. (e.g., *"I am a noun at Pos 5, looking for an adjective near Pos 4"*).

### The Implementation (PyTorch)

It's almost too simple to show code for:

```python
class LearnablePositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        # Just a standard embedding layer!
        # Input ID 0 gives vector for pos 0, Input ID 5 gives vector for pos 5.
        self.pos_embedding = nn.Embedding(max_len, d_model)

    def forward(self, x):
        # x shape: (batch_size, seq_len, d_model)
        seq_len = x.size(1)
        
        # Create indices [0, 1, ... seq_len-1] on the right device
        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)
        
        # Lookup the learnable vectors
        pos_vectors = self.pos_embedding(positions)
        
        # Add them to the word embeddings
        return x + pos_vectors
```


## The Showdown: Sinusoidal vs. Learnable

Which one should you use? For a long time, this was a hot debate.

| Feature | Sinusoidal (Fixed) | Learnable |
| :--- | :--- | :--- |
| **How it works** | Deterministic math formula. | Learned weights during training. |
| **Parameters** | Zero trainable parameters. | `max_len * d_model` parameters. |
| **Extensibility** | **Excellent.** Can theoretically handle sequences longer than seen during training. | **Poor.** Cannot handle positions beyond `max_len` seen in training. |
| **Flexibility** | Rigid structure imposed by humans. | Flexible; model learns whatever structure the data demands. |
| **Implementation** | Slightly complex math to setup. | Very simple (just an Embedding layer). |

### The Verdict

In practice, for standard tasks like translation, both perform remarkably similarly. The model is usually smart enough to figure out ordering regardless of which method you use, as long as the tags are unique.

However, **Sinusoidal** is often preferred in research because it doesn't add parameters and has that theoretical ability to extrapolate to longer sequences. **Learnable** is preferred for its simplicity and is often the default in libraries like Hugging Face's BERT implementation.

## Summary

Without Positional Encoding, a Transformer is just a powerful bag-of-words model. By injecting these positional signals—whether through fixed trigonometry or learned embeddings—we provide the necessary geography for the model to understand the structure of language.

The sinusoidal approach, while mathematically dense at first glance, is an elegant solution that uses alternating frequencies to create unique, bounded time-stamps for every token in a sequence.